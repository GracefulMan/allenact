{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An open source framework for research in Embodied AI AllenAct is a modular and flexible learning framework designed with a focus on the unique requirements of Embodied-AI research. It provides first-class support for a growing collection of embodied environments, tasks and algorithms, provides reproductions of state-of-the-art models and includes extensive documentation, tutorials, start-up code, and pre-trained models. AllenAct is built and backed by the Allen Institute for AI (AI2) . AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. Quick Links # Website & Docs Github Install Tutorials AllenAct Paper Citation Features & Highlights # Support for multiple environments : Support for the iTHOR , RoboTHOR and Habitat embodied environments as well as for grid-worlds including MiniGrid . Task Abstraction : Tasks and environments are decoupled in AllenAct, enabling researchers to easily implement a large variety of tasks in the same environment. Algorithms : Support for a variety of on-policy algorithms including PPO , DD-PPO , A2C , Imitation Learning and DAgger as well as offline training such as offline IL. Sequential Algorithms : It is trivial to experiment with different sequences of training routines, which are often the key to successful policies. Simultaneous Losses : Easily combine various losses while training models (e.g. use an external self-supervised loss while optimizing a PPO loss). Multi-agent support : Support for multi-agent algorithms and tasks. Visualizations : Out of the box support to easily visualize first and third person views for agents as well as intermediate model tensors, integrated into Tensorboard. Pre-trained models : Code and models for a number of standard Embodied AI tasks. Tutorials : Start-up code and extensive tutorials to help ramp up to Embodied AI. First-class PyTorch support : One of the few RL frameworks to target PyTorch. Contributions # We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in this repository is subject to formatting, documentation, and type-annotation guidelines. For more details, please see the our contribution guidelines . Acknowledgments # This work builds upon the pytorch-a2c-ppo-acktr library of Ilya Kostrikov and uses some data structures from FAIR's habitat-lab . We would like to thank Dustin Schwenk for his help for the public release of the framework. License # AllenAct is MIT licensed, as found in the LICENSE file. Team # AllenAct is an open-source project built by members of the PRIOR research group at the Allen Institute for Artificial Intelligence (AI2). Citation # If you use this work, please cite our paper : @article { AllenAct , author = {Luca Weihs and Jordi Salvador and Klemen Kotar and Unnat Jain and Kuo-Hao Zeng and Roozbeh Mottaghi and Aniruddha Kembhavi} , title = {AllenAct: A Framework for Embodied AI Research} , year = {2020} , journal = {arXiv} , }","title":"Overview"},{"location":"#quick-links","text":"Website & Docs Github Install Tutorials AllenAct Paper Citation","title":"Quick Links"},{"location":"#features-highlights","text":"Support for multiple environments : Support for the iTHOR , RoboTHOR and Habitat embodied environments as well as for grid-worlds including MiniGrid . Task Abstraction : Tasks and environments are decoupled in AllenAct, enabling researchers to easily implement a large variety of tasks in the same environment. Algorithms : Support for a variety of on-policy algorithms including PPO , DD-PPO , A2C , Imitation Learning and DAgger as well as offline training such as offline IL. Sequential Algorithms : It is trivial to experiment with different sequences of training routines, which are often the key to successful policies. Simultaneous Losses : Easily combine various losses while training models (e.g. use an external self-supervised loss while optimizing a PPO loss). Multi-agent support : Support for multi-agent algorithms and tasks. Visualizations : Out of the box support to easily visualize first and third person views for agents as well as intermediate model tensors, integrated into Tensorboard. Pre-trained models : Code and models for a number of standard Embodied AI tasks. Tutorials : Start-up code and extensive tutorials to help ramp up to Embodied AI. First-class PyTorch support : One of the few RL frameworks to target PyTorch.","title":"Features &amp; Highlights"},{"location":"#contributions","text":"We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in this repository is subject to formatting, documentation, and type-annotation guidelines. For more details, please see the our contribution guidelines .","title":"Contributions"},{"location":"#acknowledgments","text":"This work builds upon the pytorch-a2c-ppo-acktr library of Ilya Kostrikov and uses some data structures from FAIR's habitat-lab . We would like to thank Dustin Schwenk for his help for the public release of the framework.","title":"Acknowledgments"},{"location":"#license","text":"AllenAct is MIT licensed, as found in the LICENSE file.","title":"License"},{"location":"#team","text":"AllenAct is an open-source project built by members of the PRIOR research group at the Allen Institute for Artificial Intelligence (AI2).","title":"Team"},{"location":"#citation","text":"If you use this work, please cite our paper : @article { AllenAct , author = {Luca Weihs and Jordi Salvador and Klemen Kotar and Unnat Jain and Kuo-Hao Zeng and Roozbeh Mottaghi and Aniruddha Kembhavi} , title = {AllenAct: A Framework for Embodied AI Research} , year = {2020} , journal = {arXiv} , }","title":"Citation"},{"location":"CONTRIBUTING/","text":"Contributing # We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in pull requests should adhere to the following guidelines. Found a bug or want to suggest an enhancement? # Please submit an issue in which you note the steps to reproduce the bug or in which you detail the enhancement. Making a pull request? # When making a pull request we require that any code respects several guidelines detailed below. Auto-formatting # All python code in this repository should be formatted using black . To use black auto-formatting across all files, simply run bash scripts/auto_format.sh which will run black auto-formatting as well as docformatter (used to auto-format documentation strings). Type-checking # Our code makes liberal use of type hints. If you have not had experience with type hinting in python we recommend reading the documentation of the typing python module or the simplified introduction to type hints found here . All methods should have typed arguments and output. Furthermore we use mypy to perform basic static type checking. Before making a pull request, there should be no warnings or errors when running dmypy run -- --follow-imports = skip . Explicitly ignoring type checking (for instance using # type: ignore ) should be only be done when it would otherwise be an extensive burden. Updating, adding, or removing packages? # We recommend using pipenv to keep track of dependencies, ensure reproducibility, and keep things synchronized. If you are doing so and have modified any installed packages please run: pipenv-setup sync --pipfile # Syncs packages to setup.py pip freeze > requirements.txt # Syncs packages to requirements.py before submitting a pull request. If you are not using pipenv , you are still required to update the file Pipfile with newly installed or modified packages. Moreover you must manually update the install_requires field of the setup.py file. Setting up pre-commit hooks (optional) # Pre-commit hooks check that, when you attempt to commit changes, your code adheres a number of formatting and type-checking guidelines. Pull requests containing code not adhering to these guidelines will not be accepted and thus we recommend installing these pre-commit hooks. Assuming you have installed all of the project requirements, you can install our recommended pre-commit hooks by running (from this project's root directory) pre-commit install After running the above, each time you run git commit ... a set of pre-commit checks will be run.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in pull requests should adhere to the following guidelines.","title":"Contributing"},{"location":"CONTRIBUTING/#found-a-bug-or-want-to-suggest-an-enhancement","text":"Please submit an issue in which you note the steps to reproduce the bug or in which you detail the enhancement.","title":"Found a bug or want to suggest an enhancement?"},{"location":"CONTRIBUTING/#making-a-pull-request","text":"When making a pull request we require that any code respects several guidelines detailed below.","title":"Making a pull request?"},{"location":"CONTRIBUTING/#auto-formatting","text":"All python code in this repository should be formatted using black . To use black auto-formatting across all files, simply run bash scripts/auto_format.sh which will run black auto-formatting as well as docformatter (used to auto-format documentation strings).","title":"Auto-formatting"},{"location":"CONTRIBUTING/#type-checking","text":"Our code makes liberal use of type hints. If you have not had experience with type hinting in python we recommend reading the documentation of the typing python module or the simplified introduction to type hints found here . All methods should have typed arguments and output. Furthermore we use mypy to perform basic static type checking. Before making a pull request, there should be no warnings or errors when running dmypy run -- --follow-imports = skip . Explicitly ignoring type checking (for instance using # type: ignore ) should be only be done when it would otherwise be an extensive burden.","title":"Type-checking"},{"location":"CONTRIBUTING/#updating-adding-or-removing-packages","text":"We recommend using pipenv to keep track of dependencies, ensure reproducibility, and keep things synchronized. If you are doing so and have modified any installed packages please run: pipenv-setup sync --pipfile # Syncs packages to setup.py pip freeze > requirements.txt # Syncs packages to requirements.py before submitting a pull request. If you are not using pipenv , you are still required to update the file Pipfile with newly installed or modified packages. Moreover you must manually update the install_requires field of the setup.py file.","title":"Updating, adding, or removing packages?"},{"location":"CONTRIBUTING/#setting-up-pre-commit-hooks-optional","text":"Pre-commit hooks check that, when you attempt to commit changes, your code adheres a number of formatting and type-checking guidelines. Pull requests containing code not adhering to these guidelines will not be accepted and thus we recommend installing these pre-commit hooks. Assuming you have installed all of the project requirements, you can install our recommended pre-commit hooks by running (from this project's root directory) pre-commit install After running the above, each time you run git commit ... a set of pre-commit checks will be run.","title":"Setting up pre-commit hooks (optional)"},{"location":"FAQ/","text":"FAQ # How do I file a bug regarding the code or documentation? # Please file bugs by submitting an issue . We also welcome contributions from the community, including new features and bugfixes on existing functionality. Please refer to our contribution guidelines . How do I generate documentation? # Documentation is generated using mkdoc and pydoc-markdown . Building documentation locally # The mkdocs command used to build our documentation relies on all documentation existing as subdirectories of the docs folder. To ensure that all relevant markdown files are placed into this directory, you should always run bash scripts/build_docs.sh from the top-level project directory before running any of the mkdocs commands below. If you have made no changes to the documentation and only wish to build documentation on your local machine, run the following from within the allenact root directory. Note: This will generate HTML documentation within the site folder mkdocs build Serving documentation locally # If you have made no changes to the documentation and only wish to serve documentation on your local machine (with live reloading of modified documentation), run the following from within the allenact root directory. mkdocs serve Then navigate to http://127.0.0.1:8000/ Modifying and serving documentation locally # If you have made changes to the documentation, you will need to run a documentation builder script before you serve it on your local machine. bash scripts/build_docs.sh mkdocs serve Then navigate to http://127.0.0.1:8000/ Alternatively, the site directory (once built) can be served as a static webpage on your local machine without installing any dependencies by running python -m http.server 8000 from within the site directory.","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#how-do-i-file-a-bug-regarding-the-code-or-documentation","text":"Please file bugs by submitting an issue . We also welcome contributions from the community, including new features and bugfixes on existing functionality. Please refer to our contribution guidelines .","title":"How do I file a bug regarding the code or documentation?"},{"location":"FAQ/#how-do-i-generate-documentation","text":"Documentation is generated using mkdoc and pydoc-markdown .","title":"How do I generate documentation?"},{"location":"FAQ/#building-documentation-locally","text":"The mkdocs command used to build our documentation relies on all documentation existing as subdirectories of the docs folder. To ensure that all relevant markdown files are placed into this directory, you should always run bash scripts/build_docs.sh from the top-level project directory before running any of the mkdocs commands below. If you have made no changes to the documentation and only wish to build documentation on your local machine, run the following from within the allenact root directory. Note: This will generate HTML documentation within the site folder mkdocs build","title":"Building documentation locally"},{"location":"FAQ/#serving-documentation-locally","text":"If you have made no changes to the documentation and only wish to serve documentation on your local machine (with live reloading of modified documentation), run the following from within the allenact root directory. mkdocs serve Then navigate to http://127.0.0.1:8000/","title":"Serving documentation locally"},{"location":"FAQ/#modifying-and-serving-documentation-locally","text":"If you have made changes to the documentation, you will need to run a documentation builder script before you serve it on your local machine. bash scripts/build_docs.sh mkdocs serve Then navigate to http://127.0.0.1:8000/ Alternatively, the site directory (once built) can be served as a static webpage on your local machine without installing any dependencies by running python -m http.server 8000 from within the site directory.","title":"Modifying and serving documentation locally"},{"location":"LICENSE/","text":"MIT License Original work Copyright (c) 2017 Ilya Kostrikov Original work Copyright (c) Facebook, Inc. and its affiliates. Modified work Copyright (c) 2020 Allen Institute for Artificial Intelligence Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Licence"},{"location":"api/constants/","text":"constants # [view_source]","title":"constants"},{"location":"api/constants/#constants","text":"[view_source]","title":"constants"},{"location":"api/core/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/","text":"core.algorithms.offpolicy_sync.losses.abstract_offpolicy_loss # [view_source] Defining abstract loss classes for actor critic models. AbstractOffPolicyLoss # class AbstractOffPolicyLoss ( typing . Generic [ ModelType ], Loss ) [view_source] Abstract class representing an off-policy loss function used to train a model. AbstractOffPolicyLoss.loss # | @abc . abstractmethod | loss ( model : ModelType , batch : ObservationType , memory : Memory , * args , ** kwargs , * , ,) -> Tuple [ torch . FloatTensor , Dict [ str , float ], Memory , int ] [view_source] Computes the loss. Loss after processing a batch of data with (part of) a model (possibly with memory). Parameters model : model to run on data batch (both assumed to be on the same device) batch : data to use as input for model (already on the same device as model) memory : model memory before processing current data batch Returns A tuple with : current_loss : total loss current_info : additional information about the current loss memory : model memory after processing current data batch bsize : batch size","title":"abstract_offpolicy_loss"},{"location":"api/core/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/#corealgorithmsoffpolicy_synclossesabstract_offpolicy_loss","text":"[view_source] Defining abstract loss classes for actor critic models.","title":"core.algorithms.offpolicy_sync.losses.abstract_offpolicy_loss"},{"location":"api/core/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/#abstractoffpolicyloss","text":"class AbstractOffPolicyLoss ( typing . Generic [ ModelType ], Loss ) [view_source] Abstract class representing an off-policy loss function used to train a model.","title":"AbstractOffPolicyLoss"},{"location":"api/core/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/#abstractoffpolicylossloss","text":"| @abc . abstractmethod | loss ( model : ModelType , batch : ObservationType , memory : Memory , * args , ** kwargs , * , ,) -> Tuple [ torch . FloatTensor , Dict [ str , float ], Memory , int ] [view_source] Computes the loss. Loss after processing a batch of data with (part of) a model (possibly with memory). Parameters model : model to run on data batch (both assumed to be on the same device) batch : data to use as input for model (already on the same device as model) memory : model memory before processing current data batch Returns A tuple with : current_loss : total loss current_info : additional information about the current loss memory : model memory after processing current data batch bsize : batch size","title":"AbstractOffPolicyLoss.loss"},{"location":"api/core/algorithms/onpolicy_sync/engine/","text":"core.algorithms.onpolicy_sync.engine # [view_source] Defines the reinforcement learning OnPolicyRLEngine . OnPolicyRLEngine # class OnPolicyRLEngine ( object ) [view_source] The reinforcement learning primary controller. This OnPolicyRLEngine class handles all training, validation, and testing as well as logging and checkpointing. You are not expected to instantiate this class yourself, instead you should define an experiment which will then be used to instantiate an OnPolicyRLEngine and perform any desired tasks. OnPolicyRLEngine.__init__ # | __init__ ( experiment_name : str , config : ExperimentConfig , results_queue : mp . Queue , checkpoints_queue : Optional [ | mp . Queue | ], checkpoints_dir : str , mode : str = \"train\" , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , mp_ctx : Optional [ BaseContext ] = None , worker_id : int = 0 , num_workers : int = 1 , device : Union [ str , torch . device , int ] = \"cpu\" , distributed_port : int = 0 , deterministic_agents : bool = False , max_sampler_processes_per_worker : Optional [ int ] = None , ** kwargs , ,) [view_source] Initializer. Parameters config : The ExperimentConfig defining the experiment to run. output_dir : Root directory at which checkpoints and logs should be saved. seed : Seed used to encourage deterministic behavior (it is difficult to ensure completely deterministic behavior due to CUDA issues and nondeterminism in environments). mode : \"train\", \"valid\", or \"test\". deterministic_cudnn : Whether or not to use deterministic cudnn. If True this may lower training performance this is necessary (but not sufficient) if you desire deterministic behavior. extra_tag : An additional label to add to the experiment when saving tensorboard logs. OnPolicyRLEngine.worker_seeds # | @staticmethod | worker_seeds ( nprocesses : int , initial_seed : Optional [ int ]) -> List [ int ] [view_source] Create a collection of seeds for workers without modifying the RNG state.","title":"engine"},{"location":"api/core/algorithms/onpolicy_sync/engine/#corealgorithmsonpolicy_syncengine","text":"[view_source] Defines the reinforcement learning OnPolicyRLEngine .","title":"core.algorithms.onpolicy_sync.engine"},{"location":"api/core/algorithms/onpolicy_sync/engine/#onpolicyrlengine","text":"class OnPolicyRLEngine ( object ) [view_source] The reinforcement learning primary controller. This OnPolicyRLEngine class handles all training, validation, and testing as well as logging and checkpointing. You are not expected to instantiate this class yourself, instead you should define an experiment which will then be used to instantiate an OnPolicyRLEngine and perform any desired tasks.","title":"OnPolicyRLEngine"},{"location":"api/core/algorithms/onpolicy_sync/engine/#onpolicyrlengine__init__","text":"| __init__ ( experiment_name : str , config : ExperimentConfig , results_queue : mp . Queue , checkpoints_queue : Optional [ | mp . Queue | ], checkpoints_dir : str , mode : str = \"train\" , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , mp_ctx : Optional [ BaseContext ] = None , worker_id : int = 0 , num_workers : int = 1 , device : Union [ str , torch . device , int ] = \"cpu\" , distributed_port : int = 0 , deterministic_agents : bool = False , max_sampler_processes_per_worker : Optional [ int ] = None , ** kwargs , ,) [view_source] Initializer. Parameters config : The ExperimentConfig defining the experiment to run. output_dir : Root directory at which checkpoints and logs should be saved. seed : Seed used to encourage deterministic behavior (it is difficult to ensure completely deterministic behavior due to CUDA issues and nondeterminism in environments). mode : \"train\", \"valid\", or \"test\". deterministic_cudnn : Whether or not to use deterministic cudnn. If True this may lower training performance this is necessary (but not sufficient) if you desire deterministic behavior. extra_tag : An additional label to add to the experiment when saving tensorboard logs.","title":"OnPolicyRLEngine.__init__"},{"location":"api/core/algorithms/onpolicy_sync/engine/#onpolicyrlengineworker_seeds","text":"| @staticmethod | worker_seeds ( nprocesses : int , initial_seed : Optional [ int ]) -> List [ int ] [view_source] Create a collection of seeds for workers without modifying the RNG state.","title":"OnPolicyRLEngine.worker_seeds"},{"location":"api/core/algorithms/onpolicy_sync/policy/","text":"core.algorithms.onpolicy_sync.policy # [view_source] ActorCriticModel # class ActorCriticModel ( Generic [ DistributionType ], nn . Module ) [view_source] Abstract class defining a deep (recurrent) actor critic agent. When defining a new agent, you should over subclass this class and implement the abstract methods. Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type gym.spaces.Discrete ). observation_space : The observation space expected by the agent. This is of type gym.spaces.dict . ActorCriticModel.__init__ # | __init__ ( action_space : gym . spaces . Discrete , observation_space : SpaceDict ) [view_source] Initializer. Parameters action_space : The space of actions available to the agent. observation_space : The observation space expected by the agent. ActorCriticModel.recurrent_memory_specification # | @property | recurrent_memory_specification () -> Optional [ FullMemorySpecType ] [view_source] The memory specification for the ActorCriticModel . See docs for _recurrent_memory_shape Returns The memory specification from _recurrent_memory_shape . ActorCriticModel.forward # | @abc . abstractmethod | forward ( observations : ObservationType , memory : Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]] [view_source] Transforms input observations (& previous hidden state) into action probabilities and the state value. Parameters observations : Multi-level map from key strings to tensors of shape [steps, samplers, (agents,) ...] with the current observations. memory : Memory object with recurrent memory. The shape of each tensor is determined by the corresponding entry in _recurrent_memory_specification . prev_actions : tensor of shape [steps, samplers, agents, ...] with the previous actions. masks : tensor of shape [steps, samplers, agents, 1] with zeros indicating steps where a new episode/task starts. Returns A tuple whose first element is an object of class ActorCriticOutput which stores the agent's probability distribution over possible actions (shape [steps, samplers, agents, num_actions]), the agent's value for the state (shape [steps, samplers, agents, 1]), and any extra information needed for loss computations. The second element is an optional Memory , which is only used in models with recurrent memory.","title":"policy"},{"location":"api/core/algorithms/onpolicy_sync/policy/#corealgorithmsonpolicy_syncpolicy","text":"[view_source]","title":"core.algorithms.onpolicy_sync.policy"},{"location":"api/core/algorithms/onpolicy_sync/policy/#actorcriticmodel","text":"class ActorCriticModel ( Generic [ DistributionType ], nn . Module ) [view_source] Abstract class defining a deep (recurrent) actor critic agent. When defining a new agent, you should over subclass this class and implement the abstract methods. Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type gym.spaces.Discrete ). observation_space : The observation space expected by the agent. This is of type gym.spaces.dict .","title":"ActorCriticModel"},{"location":"api/core/algorithms/onpolicy_sync/policy/#actorcriticmodel__init__","text":"| __init__ ( action_space : gym . spaces . Discrete , observation_space : SpaceDict ) [view_source] Initializer. Parameters action_space : The space of actions available to the agent. observation_space : The observation space expected by the agent.","title":"ActorCriticModel.__init__"},{"location":"api/core/algorithms/onpolicy_sync/policy/#actorcriticmodelrecurrent_memory_specification","text":"| @property | recurrent_memory_specification () -> Optional [ FullMemorySpecType ] [view_source] The memory specification for the ActorCriticModel . See docs for _recurrent_memory_shape Returns The memory specification from _recurrent_memory_shape .","title":"ActorCriticModel.recurrent_memory_specification"},{"location":"api/core/algorithms/onpolicy_sync/policy/#actorcriticmodelforward","text":"| @abc . abstractmethod | forward ( observations : ObservationType , memory : Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]] [view_source] Transforms input observations (& previous hidden state) into action probabilities and the state value. Parameters observations : Multi-level map from key strings to tensors of shape [steps, samplers, (agents,) ...] with the current observations. memory : Memory object with recurrent memory. The shape of each tensor is determined by the corresponding entry in _recurrent_memory_specification . prev_actions : tensor of shape [steps, samplers, agents, ...] with the previous actions. masks : tensor of shape [steps, samplers, agents, 1] with zeros indicating steps where a new episode/task starts. Returns A tuple whose first element is an object of class ActorCriticOutput which stores the agent's probability distribution over possible actions (shape [steps, samplers, agents, num_actions]), the agent's value for the state (shape [steps, samplers, agents, 1]), and any extra information needed for loss computations. The second element is an optional Memory , which is only used in models with recurrent memory.","title":"ActorCriticModel.forward"},{"location":"api/core/algorithms/onpolicy_sync/runner/","text":"core.algorithms.onpolicy_sync.runner # [view_source] Defines the reinforcement learning OnPolicyRunner .","title":"runner"},{"location":"api/core/algorithms/onpolicy_sync/runner/#corealgorithmsonpolicy_syncrunner","text":"[view_source] Defines the reinforcement learning OnPolicyRunner .","title":"core.algorithms.onpolicy_sync.runner"},{"location":"api/core/algorithms/onpolicy_sync/storage/","text":"core.algorithms.onpolicy_sync.storage # [view_source] RolloutStorage # class RolloutStorage ( object ) [view_source] Class for storing rollout information for RL trainers.","title":"storage"},{"location":"api/core/algorithms/onpolicy_sync/storage/#corealgorithmsonpolicy_syncstorage","text":"[view_source]","title":"core.algorithms.onpolicy_sync.storage"},{"location":"api/core/algorithms/onpolicy_sync/storage/#rolloutstorage","text":"class RolloutStorage ( object ) [view_source] Class for storing rollout information for RL trainers.","title":"RolloutStorage"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/","text":"core.algorithms.onpolicy_sync.vector_sampled_tasks # [view_source] VectorSampledTasks # class VectorSampledTasks ( object ) [view_source] Vectorized collection of tasks. Creates multiple processes where each process runs its own TaskSampler. Each process generates one Task from its TaskSampler at a time and this class allows for interacting with these tasks in a vectorized manner. When a task on a process completes, the process samples another task from its task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks. multiprocessing_start_method : the multiprocessing method used to spawn worker processes. Valid methods are {'spawn', 'forkserver', 'fork'} 'forkserver' is the recommended method as it works well with CUDA. If 'fork' is used, the subproccess must be started before any other GPU useage. VectorSampledTasks.is_closed # | @property | is_closed () -> bool [view_source] Has the vector task been closed. VectorSampledTasks.num_unpaused_tasks # | @property | num_unpaused_tasks () -> int [view_source] Number of unpaused processes. Returns Number of unpaused processes. VectorSampledTasks.mp_ctx # | @property | mp_ctx () [view_source] Get the multiprocessing process used by the vector task. Returns The multiprocessing context. VectorSampledTasks.next_task # | next_task ( ** kwargs ) [view_source] Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks. VectorSampledTasks.get_observations # | get_observations () [view_source] Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks. VectorSampledTasks.command_at # | command_at ( sampler_index : int , command : str , data : Optional [ Any ] = None ) -> Any [view_source] Runs the command on the selected task and returns the result. Parameters Returns Result of the command. VectorSampledTasks.call_at # | call_at ( sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function. VectorSampledTasks.next_task_at # | next_task_at ( sampler_index : int ) -> List [ RLStepResult ] [view_source] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the process to be reset. Returns List of length one containing the observations the newly sampled task. VectorSampledTasks.step_at # | step_at ( sampler_index : int , action : int ) -> List [ RLStepResult ] [view_source] Step in the index_process task in the vector. Parameters sampler_index : Index of the sampler to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process. VectorSampledTasks.async_step # | async_step ( actions : List [ List [ int ]]) -> None [view_source] Asynchronously step in the vectorized Tasks. Parameters actions : actions to be performed in the vectorized Tasks. VectorSampledTasks.wait_step # | wait_step () -> List [ Dict [ str , Any ]] [view_source] Wait until all the asynchronized processes have synchronized. VectorSampledTasks.step # | step ( actions : List [ List [ int ]]) [view_source] Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks. VectorSampledTasks.reset_all # | reset_all () [view_source] Reset all task samplers to their initial state (except for the RNG seed). VectorSampledTasks.set_seeds # | set_seeds ( seeds : List [ int ]) [view_source] Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds. VectorSampledTasks.pause_at # | pause_at ( sampler_index : int ) -> None [view_source] Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one. VectorSampledTasks.resume_all # | resume_all () -> None [view_source] Resumes any paused processes. VectorSampledTasks.call # | call ( function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None ) -> List [ Any ] [view_source] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions. VectorSampledTasks.attr_at # | attr_at ( sampler_index : int , attr_name : str ) -> Any [view_source] Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function. VectorSampledTasks.attr # | attr ( attr_names : Union [ List [ str ], str ]) -> List [ Any ] [view_source] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions. VectorSampledTasks.render # | render ( mode : str = \"human\" , * args , ** kwargs ) -> Union [ np . ndarray , None , List [ np . ndarray ]] [view_source] Render observations from all Tasks in a tiled image or list of images. SingleProcessVectorSampledTasks # class SingleProcessVectorSampledTasks ( object ) [view_source] Vectorized collection of tasks. Simultaneously handles the state of multiple TaskSamplers and their associated tasks. Allows for interacting with these tasks in a vectorized manner. When a task completes, another task is sampled from the appropriate task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks. SingleProcessVectorSampledTasks.is_closed # | @property | is_closed () -> bool [view_source] Has the vector task been closed. SingleProcessVectorSampledTasks.num_unpaused_tasks # | @property | num_unpaused_tasks () -> int [view_source] Number of unpaused processes. Returns Number of unpaused processes. SingleProcessVectorSampledTasks.next_task # | next_task ( ** kwargs ) [view_source] Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks. SingleProcessVectorSampledTasks.get_observations # | get_observations () [view_source] Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks. SingleProcessVectorSampledTasks.next_task_at # | next_task_at ( index_process : int ) -> List [ RLStepResult ] [view_source] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the generator to be reset. Returns List of length one containing the observations the newly sampled task. SingleProcessVectorSampledTasks.step_at # | step_at ( index_process : int , action : int ) -> List [ RLStepResult ] [view_source] Step in the index_process task in the vector. Parameters index_process : Index of the process to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process. SingleProcessVectorSampledTasks.step # | step ( actions : List [ List [ int ]]) [view_source] Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks. SingleProcessVectorSampledTasks.reset_all # | reset_all () [view_source] Reset all task samplers to their initial state (except for the RNG seed). SingleProcessVectorSampledTasks.set_seeds # | set_seeds ( seeds : List [ int ]) [view_source] Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds. SingleProcessVectorSampledTasks.pause_at # | pause_at ( sampler_index : int ) -> None [view_source] Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one. SingleProcessVectorSampledTasks.resume_all # | resume_all () -> None [view_source] Resumes any paused processes. SingleProcessVectorSampledTasks.command_at # | command_at ( sampler_index : int , command : str , data : Optional [ Any ] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function. SingleProcessVectorSampledTasks.call_at # | call_at ( sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function. SingleProcessVectorSampledTasks.call # | call ( function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None ) -> List [ Any ] [view_source] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions. SingleProcessVectorSampledTasks.attr_at # | attr_at ( sampler_index : int , attr_name : str ) -> Any [view_source] Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function. SingleProcessVectorSampledTasks.attr # | attr ( attr_names : Union [ List [ str ], str ]) -> List [ Any ] [view_source] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions. SingleProcessVectorSampledTasks.render # | render ( mode : str = \"human\" , * args , ** kwargs ) -> Union [ np . ndarray , None , List [ np . ndarray ]] [view_source] Render observations from all Tasks in a tiled image or a list of images.","title":"vector_sampled_tasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#corealgorithmsonpolicy_syncvector_sampled_tasks","text":"[view_source]","title":"core.algorithms.onpolicy_sync.vector_sampled_tasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasks","text":"class VectorSampledTasks ( object ) [view_source] Vectorized collection of tasks. Creates multiple processes where each process runs its own TaskSampler. Each process generates one Task from its TaskSampler at a time and this class allows for interacting with these tasks in a vectorized manner. When a task on a process completes, the process samples another task from its task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks. multiprocessing_start_method : the multiprocessing method used to spawn worker processes. Valid methods are {'spawn', 'forkserver', 'fork'} 'forkserver' is the recommended method as it works well with CUDA. If 'fork' is used, the subproccess must be started before any other GPU useage.","title":"VectorSampledTasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksis_closed","text":"| @property | is_closed () -> bool [view_source] Has the vector task been closed.","title":"VectorSampledTasks.is_closed"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksnum_unpaused_tasks","text":"| @property | num_unpaused_tasks () -> int [view_source] Number of unpaused processes. Returns Number of unpaused processes.","title":"VectorSampledTasks.num_unpaused_tasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksmp_ctx","text":"| @property | mp_ctx () [view_source] Get the multiprocessing process used by the vector task. Returns The multiprocessing context.","title":"VectorSampledTasks.mp_ctx"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksnext_task","text":"| next_task ( ** kwargs ) [view_source] Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks.","title":"VectorSampledTasks.next_task"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksget_observations","text":"| get_observations () [view_source] Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks.","title":"VectorSampledTasks.get_observations"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtaskscommand_at","text":"| command_at ( sampler_index : int , command : str , data : Optional [ Any ] = None ) -> Any [view_source] Runs the command on the selected task and returns the result. Parameters Returns Result of the command.","title":"VectorSampledTasks.command_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtaskscall_at","text":"| call_at ( sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function.","title":"VectorSampledTasks.call_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksnext_task_at","text":"| next_task_at ( sampler_index : int ) -> List [ RLStepResult ] [view_source] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the process to be reset. Returns List of length one containing the observations the newly sampled task.","title":"VectorSampledTasks.next_task_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksstep_at","text":"| step_at ( sampler_index : int , action : int ) -> List [ RLStepResult ] [view_source] Step in the index_process task in the vector. Parameters sampler_index : Index of the sampler to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process.","title":"VectorSampledTasks.step_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksasync_step","text":"| async_step ( actions : List [ List [ int ]]) -> None [view_source] Asynchronously step in the vectorized Tasks. Parameters actions : actions to be performed in the vectorized Tasks.","title":"VectorSampledTasks.async_step"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtaskswait_step","text":"| wait_step () -> List [ Dict [ str , Any ]] [view_source] Wait until all the asynchronized processes have synchronized.","title":"VectorSampledTasks.wait_step"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksstep","text":"| step ( actions : List [ List [ int ]]) [view_source] Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks.","title":"VectorSampledTasks.step"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksreset_all","text":"| reset_all () [view_source] Reset all task samplers to their initial state (except for the RNG seed).","title":"VectorSampledTasks.reset_all"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksset_seeds","text":"| set_seeds ( seeds : List [ int ]) [view_source] Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds.","title":"VectorSampledTasks.set_seeds"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtaskspause_at","text":"| pause_at ( sampler_index : int ) -> None [view_source] Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one.","title":"VectorSampledTasks.pause_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksresume_all","text":"| resume_all () -> None [view_source] Resumes any paused processes.","title":"VectorSampledTasks.resume_all"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtaskscall","text":"| call ( function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None ) -> List [ Any ] [view_source] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions.","title":"VectorSampledTasks.call"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksattr_at","text":"| attr_at ( sampler_index : int , attr_name : str ) -> Any [view_source] Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function.","title":"VectorSampledTasks.attr_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksattr","text":"| attr ( attr_names : Union [ List [ str ], str ]) -> List [ Any ] [view_source] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions.","title":"VectorSampledTasks.attr"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasksrender","text":"| render ( mode : str = \"human\" , * args , ** kwargs ) -> Union [ np . ndarray , None , List [ np . ndarray ]] [view_source] Render observations from all Tasks in a tiled image or list of images.","title":"VectorSampledTasks.render"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasks","text":"class SingleProcessVectorSampledTasks ( object ) [view_source] Vectorized collection of tasks. Simultaneously handles the state of multiple TaskSamplers and their associated tasks. Allows for interacting with these tasks in a vectorized manner. When a task completes, another task is sampled from the appropriate task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks.","title":"SingleProcessVectorSampledTasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksis_closed","text":"| @property | is_closed () -> bool [view_source] Has the vector task been closed.","title":"SingleProcessVectorSampledTasks.is_closed"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksnum_unpaused_tasks","text":"| @property | num_unpaused_tasks () -> int [view_source] Number of unpaused processes. Returns Number of unpaused processes.","title":"SingleProcessVectorSampledTasks.num_unpaused_tasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksnext_task","text":"| next_task ( ** kwargs ) [view_source] Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks.","title":"SingleProcessVectorSampledTasks.next_task"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksget_observations","text":"| get_observations () [view_source] Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks.","title":"SingleProcessVectorSampledTasks.get_observations"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksnext_task_at","text":"| next_task_at ( index_process : int ) -> List [ RLStepResult ] [view_source] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the generator to be reset. Returns List of length one containing the observations the newly sampled task.","title":"SingleProcessVectorSampledTasks.next_task_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksstep_at","text":"| step_at ( index_process : int , action : int ) -> List [ RLStepResult ] [view_source] Step in the index_process task in the vector. Parameters index_process : Index of the process to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process.","title":"SingleProcessVectorSampledTasks.step_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksstep","text":"| step ( actions : List [ List [ int ]]) [view_source] Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks.","title":"SingleProcessVectorSampledTasks.step"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksreset_all","text":"| reset_all () [view_source] Reset all task samplers to their initial state (except for the RNG seed).","title":"SingleProcessVectorSampledTasks.reset_all"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksset_seeds","text":"| set_seeds ( seeds : List [ int ]) [view_source] Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds.","title":"SingleProcessVectorSampledTasks.set_seeds"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtaskspause_at","text":"| pause_at ( sampler_index : int ) -> None [view_source] Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one.","title":"SingleProcessVectorSampledTasks.pause_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksresume_all","text":"| resume_all () -> None [view_source] Resumes any paused processes.","title":"SingleProcessVectorSampledTasks.resume_all"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtaskscommand_at","text":"| command_at ( sampler_index : int , command : str , data : Optional [ Any ] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function.","title":"SingleProcessVectorSampledTasks.command_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtaskscall_at","text":"| call_at ( sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None ) -> Any [view_source] Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function.","title":"SingleProcessVectorSampledTasks.call_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtaskscall","text":"| call ( function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None ) -> List [ Any ] [view_source] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions.","title":"SingleProcessVectorSampledTasks.call"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksattr_at","text":"| attr_at ( sampler_index : int , attr_name : str ) -> Any [view_source] Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function.","title":"SingleProcessVectorSampledTasks.attr_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksattr","text":"| attr ( attr_names : Union [ List [ str ], str ]) -> List [ Any ] [view_source] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions.","title":"SingleProcessVectorSampledTasks.attr"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasksrender","text":"| render ( mode : str = \"human\" , * args , ** kwargs ) -> Union [ np . ndarray , None , List [ np . ndarray ]] [view_source] Render observations from all Tasks in a tiled image or a list of images.","title":"SingleProcessVectorSampledTasks.render"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/","text":"core.algorithms.onpolicy_sync.losses.a2cacktr # [view_source] Implementation of A2C and ACKTR losses. A2CACKTR # class A2CACKTR ( AbstractActorCriticLoss ) [view_source] Class implementing A2C and ACKTR losses. Attributes acktr : True if should use ACKTR loss (currently not supported), otherwise uses A2C loss. value_loss_coef : Weight of value loss. entropy_coef : Weight of entropy (encouraging) loss. A2CACKTR.__init__ # | __init__ ( value_loss_coef , entropy_coef , acktr = False , * args , ** kwargs ) [view_source] Initializer. See class documentation for parameter definitions. A2C # class A2C ( A2CACKTR ) [view_source] A2C Loss. ACKTR # class ACKTR ( A2CACKTR ) [view_source] ACKTR Loss. This code is not supported as it currently lacks an implementation for recurrent models.","title":"a2cacktr"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/#corealgorithmsonpolicy_synclossesa2cacktr","text":"[view_source] Implementation of A2C and ACKTR losses.","title":"core.algorithms.onpolicy_sync.losses.a2cacktr"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/#a2cacktr","text":"class A2CACKTR ( AbstractActorCriticLoss ) [view_source] Class implementing A2C and ACKTR losses. Attributes acktr : True if should use ACKTR loss (currently not supported), otherwise uses A2C loss. value_loss_coef : Weight of value loss. entropy_coef : Weight of entropy (encouraging) loss.","title":"A2CACKTR"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/#a2cacktr__init__","text":"| __init__ ( value_loss_coef , entropy_coef , acktr = False , * args , ** kwargs ) [view_source] Initializer. See class documentation for parameter definitions.","title":"A2CACKTR.__init__"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/#a2c","text":"class A2C ( A2CACKTR ) [view_source] A2C Loss.","title":"A2C"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/#acktr","text":"class ACKTR ( A2CACKTR ) [view_source] ACKTR Loss. This code is not supported as it currently lacks an implementation for recurrent models.","title":"ACKTR"},{"location":"api/core/algorithms/onpolicy_sync/losses/abstract_loss/","text":"core.algorithms.onpolicy_sync.losses.abstract_loss # [view_source] Defining abstract loss classes for actor critic models. AbstractActorCriticLoss # class AbstractActorCriticLoss ( Loss ) [view_source] Abstract class representing a loss function used to train an ActorCriticModel. AbstractActorCriticLoss.loss # | @abc . abstractmethod | loss ( step_count : int , batch : ObservationType , actor_critic_output : ActorCriticOutput [ CategoricalDistr ], * args , ** kwargs , * , ,) -> Tuple [ torch . FloatTensor , Dict [ str , float ]] [view_source] Computes the loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. kwargs : Extra kwargs. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"abstract_loss"},{"location":"api/core/algorithms/onpolicy_sync/losses/abstract_loss/#corealgorithmsonpolicy_synclossesabstract_loss","text":"[view_source] Defining abstract loss classes for actor critic models.","title":"core.algorithms.onpolicy_sync.losses.abstract_loss"},{"location":"api/core/algorithms/onpolicy_sync/losses/abstract_loss/#abstractactorcriticloss","text":"class AbstractActorCriticLoss ( Loss ) [view_source] Abstract class representing a loss function used to train an ActorCriticModel.","title":"AbstractActorCriticLoss"},{"location":"api/core/algorithms/onpolicy_sync/losses/abstract_loss/#abstractactorcriticlossloss","text":"| @abc . abstractmethod | loss ( step_count : int , batch : ObservationType , actor_critic_output : ActorCriticOutput [ CategoricalDistr ], * args , ** kwargs , * , ,) -> Tuple [ torch . FloatTensor , Dict [ str , float ]] [view_source] Computes the loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. kwargs : Extra kwargs. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"AbstractActorCriticLoss.loss"},{"location":"api/core/algorithms/onpolicy_sync/losses/advisor/","text":"core.algorithms.onpolicy_sync.losses.advisor # [view_source] Defining the PPO loss for actor critic type models. AdvisorImitationStage # class AdvisorImitationStage ( AbstractActorCriticLoss ) [view_source] Implementation of the Advisor loss' stage 1 when main and auxiliary actors are equally weighted. AdvisorImitationStage.loss # | loss ( step_count : int , batch : Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]], actor_critic_output : ActorCriticOutput [ CategoricalDistr ], * args , ** kwargs ) [view_source] Compute imitation loss on main and auxiliary policies. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . Here batch[\"observations\"] must contain \"expert_action\" observations or \"expert_policy\" observations. See ExpertActionSensor (or ExpertPolicySensor ) for an example of a sensor producing such observations. actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. Ignored. kwargs : Extra kwargs. Ignored. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters. AdvisorWeightedStage # class AdvisorWeightedStage ( AbstractActorCriticLoss ) [view_source] Implementation of the Advisor loss' second stage (simplest variant). Attributes rl_loss : The RL loss to use, should be a loss object of type PPO or A2C (or a Builder that when called returns such a loss object). alpha : Exponent to use when reweighting the expert cross entropy loss. Larger alpha means an (exponentially) smaller weight assigned to the cross entropy loss. E.g. if a the weight with alpha=1 is 0.6 then with alpha=2 it is 0.6^2=0.36. bound : If the distance from the auxilary policy to expert policy is greater than this bound then the distance is set to 0. alpha_scheduler : An object of type AlphaScheduler which is before computing the loss in order to get a new value for alpha . smooth_expert_weight_decay : If not None, will redistribute (smooth) the weight assigned to the cross entropy loss at a particular step over the following smooth_expert_steps steps. Values of smooth_expert_weight_decay near 1 will increase how evenly weight is assigned to future steps. Values near 0 will decrease how evenly this weight is distributed with larger weight being given steps less far into the future . Here smooth_expert_steps is automatically defined from smooth_expert_weight_decay as detailed below. smooth_expert_steps : The number of \"future\" steps over which to distribute the current steps weight. This value is computed as math.ceil(-math.log(1 + ((1 - r) / r) / 0.05) / math.log(r)) - 1 where r=smooth_expert_weight_decay . This ensures that the weight is always distributed over at least one additional step and that it is never distributed more than 20 steps into the future. AdvisorWeightedStage.__init__ # | __init__ ( rl_loss : Optional [ Union [ Union [ PPO , A2C ], Builder [ Union [ PPO , A2C ]]]], fixed_alpha : Optional [ float ] = 1 , fixed_bound : Optional [ float ] = 0.1 , alpha_scheduler : AlphaScheduler = None , smooth_expert_weight_decay : Optional [ float ] = None , * args , ** kwargs ) [view_source] Initializer. See the class documentation for parameter definitions not included below. Parameters fixed_alpha : This fixed value of alpha to use. This value is IGNORED if alpha_scheduler is not None. fixed_bound : This fixed value of the bound to use.","title":"Advisor"},{"location":"api/core/algorithms/onpolicy_sync/losses/advisor/#corealgorithmsonpolicy_synclossesadvisor","text":"[view_source] Defining the PPO loss for actor critic type models.","title":"core.algorithms.onpolicy_sync.losses.advisor"},{"location":"api/core/algorithms/onpolicy_sync/losses/advisor/#advisorimitationstage","text":"class AdvisorImitationStage ( AbstractActorCriticLoss ) [view_source] Implementation of the Advisor loss' stage 1 when main and auxiliary actors are equally weighted.","title":"AdvisorImitationStage"},{"location":"api/core/algorithms/onpolicy_sync/losses/advisor/#advisorimitationstageloss","text":"| loss ( step_count : int , batch : Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]], actor_critic_output : ActorCriticOutput [ CategoricalDistr ], * args , ** kwargs ) [view_source] Compute imitation loss on main and auxiliary policies. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . Here batch[\"observations\"] must contain \"expert_action\" observations or \"expert_policy\" observations. See ExpertActionSensor (or ExpertPolicySensor ) for an example of a sensor producing such observations. actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. Ignored. kwargs : Extra kwargs. Ignored. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"AdvisorImitationStage.loss"},{"location":"api/core/algorithms/onpolicy_sync/losses/advisor/#advisorweightedstage","text":"class AdvisorWeightedStage ( AbstractActorCriticLoss ) [view_source] Implementation of the Advisor loss' second stage (simplest variant). Attributes rl_loss : The RL loss to use, should be a loss object of type PPO or A2C (or a Builder that when called returns such a loss object). alpha : Exponent to use when reweighting the expert cross entropy loss. Larger alpha means an (exponentially) smaller weight assigned to the cross entropy loss. E.g. if a the weight with alpha=1 is 0.6 then with alpha=2 it is 0.6^2=0.36. bound : If the distance from the auxilary policy to expert policy is greater than this bound then the distance is set to 0. alpha_scheduler : An object of type AlphaScheduler which is before computing the loss in order to get a new value for alpha . smooth_expert_weight_decay : If not None, will redistribute (smooth) the weight assigned to the cross entropy loss at a particular step over the following smooth_expert_steps steps. Values of smooth_expert_weight_decay near 1 will increase how evenly weight is assigned to future steps. Values near 0 will decrease how evenly this weight is distributed with larger weight being given steps less far into the future . Here smooth_expert_steps is automatically defined from smooth_expert_weight_decay as detailed below. smooth_expert_steps : The number of \"future\" steps over which to distribute the current steps weight. This value is computed as math.ceil(-math.log(1 + ((1 - r) / r) / 0.05) / math.log(r)) - 1 where r=smooth_expert_weight_decay . This ensures that the weight is always distributed over at least one additional step and that it is never distributed more than 20 steps into the future.","title":"AdvisorWeightedStage"},{"location":"api/core/algorithms/onpolicy_sync/losses/advisor/#advisorweightedstage__init__","text":"| __init__ ( rl_loss : Optional [ Union [ Union [ PPO , A2C ], Builder [ Union [ PPO , A2C ]]]], fixed_alpha : Optional [ float ] = 1 , fixed_bound : Optional [ float ] = 0.1 , alpha_scheduler : AlphaScheduler = None , smooth_expert_weight_decay : Optional [ float ] = None , * args , ** kwargs ) [view_source] Initializer. See the class documentation for parameter definitions not included below. Parameters fixed_alpha : This fixed value of alpha to use. This value is IGNORED if alpha_scheduler is not None. fixed_bound : This fixed value of the bound to use.","title":"AdvisorWeightedStage.__init__"},{"location":"api/core/algorithms/onpolicy_sync/losses/imitation/","text":"core.algorithms.onpolicy_sync.losses.imitation # [view_source] Defining imitation losses for actor critic type models. Imitation # class Imitation ( AbstractActorCriticLoss ) [view_source] Expert imitation loss. Imitation.loss # | loss ( step_count : int , batch : ObservationType , actor_critic_output : ActorCriticOutput [ CategoricalDistr ], * args , ** kwargs ) [view_source] Computes the imitation loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . Here batch[\"observations\"] must contain \"expert_action\" observations or \"expert_policy\" observations. See ExpertActionSensor (or ExpertPolicySensor ) for an example of a sensor producing such observations. actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. Ignored. kwargs : Extra kwargs. Ignored. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"imitation"},{"location":"api/core/algorithms/onpolicy_sync/losses/imitation/#corealgorithmsonpolicy_synclossesimitation","text":"[view_source] Defining imitation losses for actor critic type models.","title":"core.algorithms.onpolicy_sync.losses.imitation"},{"location":"api/core/algorithms/onpolicy_sync/losses/imitation/#imitation","text":"class Imitation ( AbstractActorCriticLoss ) [view_source] Expert imitation loss.","title":"Imitation"},{"location":"api/core/algorithms/onpolicy_sync/losses/imitation/#imitationloss","text":"| loss ( step_count : int , batch : ObservationType , actor_critic_output : ActorCriticOutput [ CategoricalDistr ], * args , ** kwargs ) [view_source] Computes the imitation loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . Here batch[\"observations\"] must contain \"expert_action\" observations or \"expert_policy\" observations. See ExpertActionSensor (or ExpertPolicySensor ) for an example of a sensor producing such observations. actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. Ignored. kwargs : Extra kwargs. Ignored. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"Imitation.loss"},{"location":"api/core/algorithms/onpolicy_sync/losses/kfac/","text":"core.algorithms.onpolicy_sync.losses.kfac # [view_source] Implementation of the KFAC optimizer. TODO: this code is not supported as it currently lacks an implementation for recurrent models.","title":"kfac"},{"location":"api/core/algorithms/onpolicy_sync/losses/kfac/#corealgorithmsonpolicy_synclosseskfac","text":"[view_source] Implementation of the KFAC optimizer. TODO: this code is not supported as it currently lacks an implementation for recurrent models.","title":"core.algorithms.onpolicy_sync.losses.kfac"},{"location":"api/core/algorithms/onpolicy_sync/losses/ppo/","text":"core.algorithms.onpolicy_sync.losses.ppo # [view_source] Defining the PPO loss for actor critic type models. PPO # class PPO ( AbstractActorCriticLoss ) [view_source] Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. value_loss_coef : Weight of the value loss. entropy_coef : Weight of the entropy (encouraging) loss. use_clipped_value_loss : Whether or not to also clip the value loss. PPO.__init__ # | __init__ ( clip_param : float , value_loss_coef : float , entropy_coef : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , * args , ** kwargs ) [view_source] Initializer. See the class documentation for parameter definitions. PPOValue # class PPOValue ( AbstractActorCriticLoss ) [view_source] Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. use_clipped_value_loss : Whether or not to also clip the value loss. PPOValue.__init__ # | __init__ ( clip_param : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , * args , ** kwargs ) [view_source] Initializer. See the class documentation for parameter definitions.","title":"ppo"},{"location":"api/core/algorithms/onpolicy_sync/losses/ppo/#corealgorithmsonpolicy_synclossesppo","text":"[view_source] Defining the PPO loss for actor critic type models.","title":"core.algorithms.onpolicy_sync.losses.ppo"},{"location":"api/core/algorithms/onpolicy_sync/losses/ppo/#ppo","text":"class PPO ( AbstractActorCriticLoss ) [view_source] Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. value_loss_coef : Weight of the value loss. entropy_coef : Weight of the entropy (encouraging) loss. use_clipped_value_loss : Whether or not to also clip the value loss.","title":"PPO"},{"location":"api/core/algorithms/onpolicy_sync/losses/ppo/#ppo__init__","text":"| __init__ ( clip_param : float , value_loss_coef : float , entropy_coef : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , * args , ** kwargs ) [view_source] Initializer. See the class documentation for parameter definitions.","title":"PPO.__init__"},{"location":"api/core/algorithms/onpolicy_sync/losses/ppo/#ppovalue","text":"class PPOValue ( AbstractActorCriticLoss ) [view_source] Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. use_clipped_value_loss : Whether or not to also clip the value loss.","title":"PPOValue"},{"location":"api/core/algorithms/onpolicy_sync/losses/ppo/#ppovalue__init__","text":"| __init__ ( clip_param : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , * args , ** kwargs ) [view_source] Initializer. See the class documentation for parameter definitions.","title":"PPOValue.__init__"},{"location":"api/core/base_abstractions/distributions/","text":"core.base_abstractions.distributions # [view_source] CategoricalDistr # class CategoricalDistr ( Distr ) [view_source] A categorical distribution extending PyTorch's Categorical. AddBias # class AddBias ( nn . Module ) [view_source] Adding bias parameters to input values. AddBias.__init__ # | __init__ ( bias : torch . FloatTensor ) [view_source] Initializer. Parameters bias : data to use as the initial values of the bias. AddBias.forward # | forward ( x : torch . FloatTensor ) -> torch . FloatTensor [view_source] Adds the stored bias parameters to x .","title":"distributions"},{"location":"api/core/base_abstractions/distributions/#corebase_abstractionsdistributions","text":"[view_source]","title":"core.base_abstractions.distributions"},{"location":"api/core/base_abstractions/distributions/#categoricaldistr","text":"class CategoricalDistr ( Distr ) [view_source] A categorical distribution extending PyTorch's Categorical.","title":"CategoricalDistr"},{"location":"api/core/base_abstractions/distributions/#addbias","text":"class AddBias ( nn . Module ) [view_source] Adding bias parameters to input values.","title":"AddBias"},{"location":"api/core/base_abstractions/distributions/#addbias__init__","text":"| __init__ ( bias : torch . FloatTensor ) [view_source] Initializer. Parameters bias : data to use as the initial values of the bias.","title":"AddBias.__init__"},{"location":"api/core/base_abstractions/distributions/#addbiasforward","text":"| forward ( x : torch . FloatTensor ) -> torch . FloatTensor [view_source] Adds the stored bias parameters to x .","title":"AddBias.forward"},{"location":"api/core/base_abstractions/experiment_config/","text":"core.base_abstractions.experiment_config # [view_source] Defines the ExperimentConfig abstract class used as the basis of all experiments. FrozenClassVariables # class FrozenClassVariables ( abc . ABCMeta ) [view_source] Metaclass for ExperimentConfig. Ensures ExperimentConfig class-level attributes cannot be modified. ExperimentConfig attributes can still be modified at the object level. ExperimentConfig # class ExperimentConfig (, metaclass = FrozenClassVariables ) [view_source] Abstract class used to define experiments. Instead of using yaml or text files, experiments in our framework are defined as a class. In particular, to define an experiment one must define a new class inheriting from this class which implements all of the below methods. The below methods will then be called when running the experiment. ExperimentConfig.tag # | @classmethod | @abc . abstractmethod | tag ( cls ) -> str [view_source] A string describing the experiment. ExperimentConfig.training_pipeline # | @classmethod | @abc . abstractmethod | training_pipeline ( cls , ** kwargs ) -> TrainingPipeline [view_source] Creates the training pipeline. Parameters kwargs : Extra kwargs. Currently unused. Returns An instantiate TrainingPipeline object. ExperimentConfig.machine_params # | @classmethod | @abc . abstractmethod | machine_params ( cls , mode = \"train\" , ** kwargs ) -> Union [ MachineParams , Dict [ str , Any ]] [view_source] Parameters used to specify machine information. Machine information includes at least (1) the number of processes to train with and (2) the gpu devices indices to use. mode : Whether or not the machine parameters should be those for \"train\", \"valid\", or \"test\". kwargs : Extra kwargs. Returns A dictionary of the form {\"nprocesses\" : ..., \"gpu_ids\": ..., ...} . Here nprocesses must be a non-negative integer, gpu_ids must be a sequence of non-negative integers (if empty, then everything will be run on the cpu). ExperimentConfig.create_model # | @classmethod | @abc . abstractmethod | create_model ( cls , ** kwargs ) -> nn . Module [view_source] Create the neural model. ExperimentConfig.make_sampler_fn # | @classmethod | @abc . abstractmethod | make_sampler_fn ( cls , ** kwargs ) -> TaskSampler [view_source] Create the TaskSampler given keyword arguments. These kwargs will be generated by one of ExperimentConfig.train_task_sampler_args , ExperimentConfig.valid_task_sampler_args , or ExperimentConfig.test_task_sampler_args depending on whether the user has chosen to train, validate, or test. ExperimentConfig.train_task_sampler_args # | train_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the training parameters for the process_ind th training process. These parameters are meant be passed as keyword arguments to ExperimentConfig.make_sampler_fn to generate a task sampler. Parameters process_ind : The unique index of the training process ( 0 \u2264 process_ind < total_processes ). total_processes : The total number of training processes. devices : Gpu devices (if any) to use. seeds : The seeds to use, if any. deterministic_cudnn : Whether or not to use deterministic cudnn. Returns The parameters for make_sampler_fn ExperimentConfig.valid_task_sampler_args # | valid_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the validation parameters for the process_ind th validation process. See ExperimentConfig.train_task_sampler_args for parameter definitions. ExperimentConfig.test_task_sampler_args # | test_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the test parameters for the process_ind th test process. See ExperimentConfig.train_task_sampler_args for parameter definitions.","title":"experiment_config"},{"location":"api/core/base_abstractions/experiment_config/#corebase_abstractionsexperiment_config","text":"[view_source] Defines the ExperimentConfig abstract class used as the basis of all experiments.","title":"core.base_abstractions.experiment_config"},{"location":"api/core/base_abstractions/experiment_config/#frozenclassvariables","text":"class FrozenClassVariables ( abc . ABCMeta ) [view_source] Metaclass for ExperimentConfig. Ensures ExperimentConfig class-level attributes cannot be modified. ExperimentConfig attributes can still be modified at the object level.","title":"FrozenClassVariables"},{"location":"api/core/base_abstractions/experiment_config/#experimentconfig","text":"class ExperimentConfig (, metaclass = FrozenClassVariables ) [view_source] Abstract class used to define experiments. Instead of using yaml or text files, experiments in our framework are defined as a class. In particular, to define an experiment one must define a new class inheriting from this class which implements all of the below methods. The below methods will then be called when running the experiment.","title":"ExperimentConfig"},{"location":"api/core/base_abstractions/experiment_config/#experimentconfigtag","text":"| @classmethod | @abc . abstractmethod | tag ( cls ) -> str [view_source] A string describing the experiment.","title":"ExperimentConfig.tag"},{"location":"api/core/base_abstractions/experiment_config/#experimentconfigtraining_pipeline","text":"| @classmethod | @abc . abstractmethod | training_pipeline ( cls , ** kwargs ) -> TrainingPipeline [view_source] Creates the training pipeline. Parameters kwargs : Extra kwargs. Currently unused. Returns An instantiate TrainingPipeline object.","title":"ExperimentConfig.training_pipeline"},{"location":"api/core/base_abstractions/experiment_config/#experimentconfigmachine_params","text":"| @classmethod | @abc . abstractmethod | machine_params ( cls , mode = \"train\" , ** kwargs ) -> Union [ MachineParams , Dict [ str , Any ]] [view_source] Parameters used to specify machine information. Machine information includes at least (1) the number of processes to train with and (2) the gpu devices indices to use. mode : Whether or not the machine parameters should be those for \"train\", \"valid\", or \"test\". kwargs : Extra kwargs. Returns A dictionary of the form {\"nprocesses\" : ..., \"gpu_ids\": ..., ...} . Here nprocesses must be a non-negative integer, gpu_ids must be a sequence of non-negative integers (if empty, then everything will be run on the cpu).","title":"ExperimentConfig.machine_params"},{"location":"api/core/base_abstractions/experiment_config/#experimentconfigcreate_model","text":"| @classmethod | @abc . abstractmethod | create_model ( cls , ** kwargs ) -> nn . Module [view_source] Create the neural model.","title":"ExperimentConfig.create_model"},{"location":"api/core/base_abstractions/experiment_config/#experimentconfigmake_sampler_fn","text":"| @classmethod | @abc . abstractmethod | make_sampler_fn ( cls , ** kwargs ) -> TaskSampler [view_source] Create the TaskSampler given keyword arguments. These kwargs will be generated by one of ExperimentConfig.train_task_sampler_args , ExperimentConfig.valid_task_sampler_args , or ExperimentConfig.test_task_sampler_args depending on whether the user has chosen to train, validate, or test.","title":"ExperimentConfig.make_sampler_fn"},{"location":"api/core/base_abstractions/experiment_config/#experimentconfigtrain_task_sampler_args","text":"| train_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the training parameters for the process_ind th training process. These parameters are meant be passed as keyword arguments to ExperimentConfig.make_sampler_fn to generate a task sampler. Parameters process_ind : The unique index of the training process ( 0 \u2264 process_ind < total_processes ). total_processes : The total number of training processes. devices : Gpu devices (if any) to use. seeds : The seeds to use, if any. deterministic_cudnn : Whether or not to use deterministic cudnn. Returns The parameters for make_sampler_fn","title":"ExperimentConfig.train_task_sampler_args"},{"location":"api/core/base_abstractions/experiment_config/#experimentconfigvalid_task_sampler_args","text":"| valid_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the validation parameters for the process_ind th validation process. See ExperimentConfig.train_task_sampler_args for parameter definitions.","title":"ExperimentConfig.valid_task_sampler_args"},{"location":"api/core/base_abstractions/experiment_config/#experimentconfigtest_task_sampler_args","text":"| test_task_sampler_args ( process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False ) -> Dict [ str , Any ] [view_source] Specifies the test parameters for the process_ind th test process. See ExperimentConfig.train_task_sampler_args for parameter definitions.","title":"ExperimentConfig.test_task_sampler_args"},{"location":"api/core/base_abstractions/misc/","text":"core.base_abstractions.misc # [view_source] Memory # class Memory ( Dict ) [view_source] Memory.check_append # | check_append ( key : str , tensor : torch . Tensor , sampler_dim : int ) -> \"Memory\" [view_source] Appends a new memory type given its identifier, its memory tensor and its sampler dim. Parameters key : string identifier of the memory type tensor : memory tensor sampler_dim : sampler dimension Returns Updated Memory Memory.tensor # | tensor ( key : str ) -> torch . Tensor [view_source] Returns the memory tensor for a given memory type. Parameters key : string identifier of the memory type Returns Memory tensor for type key Memory.sampler_dim # | sampler_dim ( key : str ) -> int [view_source] Returns the sampler dimension for the given memory type. Parameters key : string identifier of the memory type Returns The sampler dim Memory.sampler_select # | sampler_select ( keep : Sequence [ int ]) -> \"Memory\" [view_source] Equivalent to PyTorch index_select along the sampler_dim of each memory type. Parameters keep : a list of sampler indices to keep Returns Selected memory Memory.set_tensor # | set_tensor ( key : str , tensor : torch . Tensor ) -> \"Memory\" [view_source] Replaces tensor for given key with an updated version. Parameters key : memory type identifier to update tensor : updated tensor Returns Updated memory Memory.step_select # | step_select ( step : int ) -> \"Memory\" [view_source] Equivalent to slicing with length 1 for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step Memory.step_squeeze # | step_squeeze ( step : int ) -> \"Memory\" [view_source] Equivalent to simple indexing for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step (and squeezed step dimension) Memory.slice # | slice ( dim : int , start : Optional [ int ] = None , stop : Optional [ int ] = None , step : int = 1 ) -> \"Memory\" [view_source] Slicing for dimensions that have same extents in all memory types. It also accepts negative indices. Parameters dim : the dimension to slice start : the index of the first item to keep if given (default 0 if None) stop : the index of the first item to discard if given (default tensor size along dim if None) step : the increment between consecutive indices (default 1) Returns Sliced memory","title":"misc"},{"location":"api/core/base_abstractions/misc/#corebase_abstractionsmisc","text":"[view_source]","title":"core.base_abstractions.misc"},{"location":"api/core/base_abstractions/misc/#memory","text":"class Memory ( Dict ) [view_source]","title":"Memory"},{"location":"api/core/base_abstractions/misc/#memorycheck_append","text":"| check_append ( key : str , tensor : torch . Tensor , sampler_dim : int ) -> \"Memory\" [view_source] Appends a new memory type given its identifier, its memory tensor and its sampler dim. Parameters key : string identifier of the memory type tensor : memory tensor sampler_dim : sampler dimension Returns Updated Memory","title":"Memory.check_append"},{"location":"api/core/base_abstractions/misc/#memorytensor","text":"| tensor ( key : str ) -> torch . Tensor [view_source] Returns the memory tensor for a given memory type. Parameters key : string identifier of the memory type Returns Memory tensor for type key","title":"Memory.tensor"},{"location":"api/core/base_abstractions/misc/#memorysampler_dim","text":"| sampler_dim ( key : str ) -> int [view_source] Returns the sampler dimension for the given memory type. Parameters key : string identifier of the memory type Returns The sampler dim","title":"Memory.sampler_dim"},{"location":"api/core/base_abstractions/misc/#memorysampler_select","text":"| sampler_select ( keep : Sequence [ int ]) -> \"Memory\" [view_source] Equivalent to PyTorch index_select along the sampler_dim of each memory type. Parameters keep : a list of sampler indices to keep Returns Selected memory","title":"Memory.sampler_select"},{"location":"api/core/base_abstractions/misc/#memoryset_tensor","text":"| set_tensor ( key : str , tensor : torch . Tensor ) -> \"Memory\" [view_source] Replaces tensor for given key with an updated version. Parameters key : memory type identifier to update tensor : updated tensor Returns Updated memory","title":"Memory.set_tensor"},{"location":"api/core/base_abstractions/misc/#memorystep_select","text":"| step_select ( step : int ) -> \"Memory\" [view_source] Equivalent to slicing with length 1 for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step","title":"Memory.step_select"},{"location":"api/core/base_abstractions/misc/#memorystep_squeeze","text":"| step_squeeze ( step : int ) -> \"Memory\" [view_source] Equivalent to simple indexing for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step (and squeezed step dimension)","title":"Memory.step_squeeze"},{"location":"api/core/base_abstractions/misc/#memoryslice","text":"| slice ( dim : int , start : Optional [ int ] = None , stop : Optional [ int ] = None , step : int = 1 ) -> \"Memory\" [view_source] Slicing for dimensions that have same extents in all memory types. It also accepts negative indices. Parameters dim : the dimension to slice start : the index of the first item to keep if given (default 0 if None) stop : the index of the first item to discard if given (default tensor size along dim if None) step : the increment between consecutive indices (default 1) Returns Sliced memory","title":"Memory.slice"},{"location":"api/core/base_abstractions/preprocessor/","text":"core.base_abstractions.preprocessor # [view_source] Preprocessor # class Preprocessor ( abc . ABC ) [view_source] Represents a preprocessor that transforms data from a sensor or another preprocessor to the input of agents or other preprocessors. The user of this class needs to implement the process method and the user is also required to set the below attributes: Attributes: # input_uuids : List of input universally unique ids. uuid : Universally unique id. observation_space : gym.Space object corresponding to processed observation spaces. Preprocessor.process # | @abc . abstractmethod | process ( obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Any [view_source] Returns processed observations from sensors or other preprocessors. Parameters obs : Dict with available observations and processed observations. Returns Processed observation. PreprocessorGraph # class PreprocessorGraph () [view_source] Represents a graph of preprocessors, with each preprocessor being identified through a universally unique id. Attributes preprocessors : List containing preprocessors with required input uuids, output uuid of each sensor must be unique. PreprocessorGraph.__init__ # | __init__ ( preprocessors : List [ Union [ Preprocessor , Builder [ Preprocessor ]]]) -> None [view_source] Initializer. Parameters preprocessors : The preprocessors that will be included in the graph. PreprocessorGraph.get # | get ( uuid : str ) -> Preprocessor [view_source] Return preprocessor with the given uuid . Parameters uuid : The unique id of the preprocessor. Returns The preprocessor with unique id uuid . PreprocessorGraph.get_observations # | get_observations ( obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Dict [ str , Any ] [view_source] Get processed observations. Returns Collect observations processed from all sensors and return them packaged inside a Dict. ObservationSet # class ObservationSet () [view_source] Represents a list of source_ids, corresponding to sensors and preprocessors, with each source being identified through a unique id. Attributes source_ids : List containing sensor and preprocessor ids to be consumed by agents. Each source uuid must be unique. graph : Computation graph for all preprocessors. observation_spaces : Observation spaces of all output sources. device : Device where the PreprocessorGraph is executed. ObservationSet.__init__ # | __init__ ( source_ids : List [ str ], all_preprocessors : List [ Union [ Preprocessor , Builder [ Preprocessor ]]], all_sensors : List [ Sensor ]) -> None [view_source] Initializer. Parameters source_ids : The sensors and preprocessors that will be included in the set. all_preprocessors : The entire list of preprocessors to be executed. all_sensors : The entire list of sensors. ObservationSet.get # | get ( uuid : str ) -> Preprocessor [view_source] Return preprocessor with the given uuid . Parameters uuid : The unique id of the preprocessor. Returns The preprocessor with unique id uuid . ObservationSet.get_observations # | get_observations ( obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Dict [ str , Any ] [view_source] Get all observations within a dictionary. Returns Collect observations from all sources and return them packaged inside a Dict.","title":"preprocessor"},{"location":"api/core/base_abstractions/preprocessor/#corebase_abstractionspreprocessor","text":"[view_source]","title":"core.base_abstractions.preprocessor"},{"location":"api/core/base_abstractions/preprocessor/#preprocessor","text":"class Preprocessor ( abc . ABC ) [view_source] Represents a preprocessor that transforms data from a sensor or another preprocessor to the input of agents or other preprocessors. The user of this class needs to implement the process method and the user is also required to set the below attributes:","title":"Preprocessor"},{"location":"api/core/base_abstractions/preprocessor/#attributes","text":"input_uuids : List of input universally unique ids. uuid : Universally unique id. observation_space : gym.Space object corresponding to processed observation spaces.","title":"Attributes:"},{"location":"api/core/base_abstractions/preprocessor/#preprocessorprocess","text":"| @abc . abstractmethod | process ( obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Any [view_source] Returns processed observations from sensors or other preprocessors. Parameters obs : Dict with available observations and processed observations. Returns Processed observation.","title":"Preprocessor.process"},{"location":"api/core/base_abstractions/preprocessor/#preprocessorgraph","text":"class PreprocessorGraph () [view_source] Represents a graph of preprocessors, with each preprocessor being identified through a universally unique id. Attributes preprocessors : List containing preprocessors with required input uuids, output uuid of each sensor must be unique.","title":"PreprocessorGraph"},{"location":"api/core/base_abstractions/preprocessor/#preprocessorgraph__init__","text":"| __init__ ( preprocessors : List [ Union [ Preprocessor , Builder [ Preprocessor ]]]) -> None [view_source] Initializer. Parameters preprocessors : The preprocessors that will be included in the graph.","title":"PreprocessorGraph.__init__"},{"location":"api/core/base_abstractions/preprocessor/#preprocessorgraphget","text":"| get ( uuid : str ) -> Preprocessor [view_source] Return preprocessor with the given uuid . Parameters uuid : The unique id of the preprocessor. Returns The preprocessor with unique id uuid .","title":"PreprocessorGraph.get"},{"location":"api/core/base_abstractions/preprocessor/#preprocessorgraphget_observations","text":"| get_observations ( obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Dict [ str , Any ] [view_source] Get processed observations. Returns Collect observations processed from all sensors and return them packaged inside a Dict.","title":"PreprocessorGraph.get_observations"},{"location":"api/core/base_abstractions/preprocessor/#observationset","text":"class ObservationSet () [view_source] Represents a list of source_ids, corresponding to sensors and preprocessors, with each source being identified through a unique id. Attributes source_ids : List containing sensor and preprocessor ids to be consumed by agents. Each source uuid must be unique. graph : Computation graph for all preprocessors. observation_spaces : Observation spaces of all output sources. device : Device where the PreprocessorGraph is executed.","title":"ObservationSet"},{"location":"api/core/base_abstractions/preprocessor/#observationset__init__","text":"| __init__ ( source_ids : List [ str ], all_preprocessors : List [ Union [ Preprocessor , Builder [ Preprocessor ]]], all_sensors : List [ Sensor ]) -> None [view_source] Initializer. Parameters source_ids : The sensors and preprocessors that will be included in the set. all_preprocessors : The entire list of preprocessors to be executed. all_sensors : The entire list of sensors.","title":"ObservationSet.__init__"},{"location":"api/core/base_abstractions/preprocessor/#observationsetget","text":"| get ( uuid : str ) -> Preprocessor [view_source] Return preprocessor with the given uuid . Parameters uuid : The unique id of the preprocessor. Returns The preprocessor with unique id uuid .","title":"ObservationSet.get"},{"location":"api/core/base_abstractions/preprocessor/#observationsetget_observations","text":"| get_observations ( obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Dict [ str , Any ] [view_source] Get all observations within a dictionary. Returns Collect observations from all sources and return them packaged inside a Dict.","title":"ObservationSet.get_observations"},{"location":"api/core/base_abstractions/sensor/","text":"core.base_abstractions.sensor # [view_source] Sensor # class Sensor ( Generic [ EnvType , SubTaskType ]) [view_source] Represents a sensor that provides data from the environment to agent. The user of this class needs to implement the get_observation method and the user is also required to set the below attributes: Attributes uuid : universally unique id. observation_space : gym.Space object corresponding to observation of sensor. Sensor.get_observation # | get_observation ( env : EnvType , task : Optional [ SubTaskType ], * args : Any , ** kwargs : Any ) -> Any [view_source] Returns observations from the environment (or task). Parameters env : The environment the sensor is used upon. task : (Optionally) a Task from which the sensor should get data. Returns Current observation for Sensor. SensorSuite # class SensorSuite ( Generic [ EnvType ]) [view_source] Represents a set of sensors, with each sensor being identified through a unique id. Attributes sensors : list containing sensors for the environment, uuid of each sensor must be unique. SensorSuite.__init__ # | __init__ ( sensors : Sequence [ Sensor ]) -> None [view_source] Initializer. Parameters param sensors : the sensors that will be included in the suite. SensorSuite.get # | get ( uuid : str ) -> Sensor [view_source] Return sensor with the given uuid . Parameters uuid : The unique id of the sensor Returns The sensor with unique id uuid . SensorSuite.get_observations # | get_observations ( env : EnvType , task : Optional [ SubTaskType ], ** kwargs : Any ) -> Dict [ str , Any ] [view_source] Get all observations corresponding to the sensors in the suite. Parameters env : The environment from which to get the observation. task : (Optionally) the task from which to get the observation. Returns Data from all sensors packaged inside a Dict. VisionSensor # class VisionSensor ( Sensor [ EnvType , SubTaskType ]) [view_source] VisionSensor.__init__ # | __init__ ( mean : Optional [ np . ndarray ] = None , stdev : Optional [ np . ndarray ] = None , height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"vision\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : Optional [ int ] = None , unnormalized_infimum : float = - np . inf , unnormalized_supremum : float = np . inf , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : The images will be normalized with means config[\"mean\"] and standard deviations config[\"stdev\"] . If both config[\"height\"] and config[\"width\"] are non-negative integers then the image returned from the environment will be rescaled to have config[\"height\"] rows and config[\"width\"] columns using bilinear sampling. The universally unique identifier will be set as config[\"uuid\"] . args : Extra args. Currently unused. kwargs : Extra kwargs. Currently unused. VisionSensor.height # | @property | height () -> Optional [ int ] [view_source] Height that input image will be rescale to have. Returns The height as a non-negative integer or None if no rescaling is done. VisionSensor.width # | @property | width () -> Optional [ int ] [view_source] Width that input image will be rescale to have. Returns The width as a non-negative integer or None if no rescaling is done. RGBSensor # class RGBSensor ( VisionSensor [ EnvType , SubTaskType ], ABC ) [view_source] RGBSensor.__init__ # | __init__ ( use_resnet_normalization : bool = False , mean : Optional [ np . ndarray ] = np . array ( | [[[ 0.485 , 0.456 , 0.406 ]]], dtype = np . float32 | ), stdev : Optional [ np . ndarray ] = np . array ( | [[[ 0.229 , 0.224 , 0.225 ]]], dtype = np . float32 | ), height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"rgb\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 3 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 1.0 , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : If config[\"use_resnet_normalization\"] is True then the RGB images will be normalized with means [0.485, 0.456, 0.406] and standard deviations [0.229, 0.224, 0.225] (i.e. using the standard resnet normalization). If both config[\"height\"] and config[\"width\"] are non-negative integers then the RGB image returned from the environment will be rescaled to have shape (config[\"height\"], config[\"width\"], 3) using bilinear sampling. args : Extra args. Currently unused. kwargs : Extra kwargs. Currently unused. DepthSensor # class DepthSensor ( VisionSensor [ EnvType , SubTaskType ], ABC ) [view_source] DepthSensor.__init__ # | __init__ ( use_normalization : bool = False , mean : Optional [ np . ndarray ] = np . array ([[ 0.5 ]], dtype = np . float32 ), stdev : Optional [ np . ndarray ] = np . array ([[ 0.25 ]], dtype = np . float32 ), height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"depth\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 1 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 5.0 , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : If config[\"use_normalization\"] is True then the depth images will be normalized with mean 0.5 and standard deviation 0.25. If both config[\"height\"] and config[\"width\"] are non-negative integers then the depth image returned from the environment will be rescaled to have shape (config[\"height\"], config[\"width\"]) using bilinear sampling. args : Extra args. Currently unused. kwargs : Extra kwargs. Currently unused. ResNetSensor # class ResNetSensor ( VisionSensor [ EnvType , SubTaskType ], ABC ) [view_source] ResNetSensor.to # | to ( device : torch . device ) -> \"ResNetSensor\" [view_source] Moves sensor to specified device. Parameters device : The device for the sensor. RGBResNetSensor # class RGBResNetSensor ( ResNetSensor [ EnvType , SubTaskType ], ABC ) [view_source] RGBResNetSensor.__init__ # | __init__ ( use_resnet_normalization : bool = True , mean : Optional [ np . ndarray ] = np . array ( | [[[ 0.485 , 0.456 , 0.406 ]]], dtype = np . float32 | ), stdev : Optional [ np . ndarray ] = np . array ( | [[[ 0.229 , 0.224 , 0.225 ]]], dtype = np . float32 | ), height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"rgbresnet\" , output_shape : Optional [ Tuple [ int , ... ]] = ( 2048 ,), output_channels : Optional [ int ] = None , unnormalized_infimum : float = - np . inf , unnormalized_supremum : float = np . inf , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : If config[\"use_resnet_normalization\"] is True then the RGB images will be normalized with means [0.485, 0.456, 0.406] and standard deviations [0.229, 0.224, 0.225] (i.e. using the standard resnet normalization). If both config[\"height\"] and config[\"width\"] are non-negative integers then the RGB image returned from the environment will be rescaled to have shape (config[\"height\"], config[\"width\"], 3) using bilinear sampling before being fed to a ResNet-50 and extracting the flattened 2048-dimensional output embedding. args : Extra args. Currently unused. kwargs : Extra kwargs. Currently unused. DepthResNetSensor # class DepthResNetSensor ( ResNetSensor [ EnvType , SubTaskType ], ABC ) [view_source] DepthResNetSensor.__init__ # | __init__ ( use_normalization : bool = False , mean : Optional [ np . ndarray ] = np . array ([[ 0.5 ]], dtype = np . float32 ), stdev : Optional [ np . ndarray ] = np . array ([[ 0.25 ]], dtype = np . float32 ), height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"depthresnet\" , output_shape : Optional [ Tuple [ int , ... ]] = ( 2048 ,), output_channels : Optional [ int ] = None , unnormalized_infimum : float = - np . inf , unnormalized_supremum : float = np . inf , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : If config[\"use_normalization\"] is True then the depth images will be normalized with mean 0.5 and standard deviation 0.25. If both config[\"height\"] and config[\"width\"] are non-negative integers then the depth image returned from the environment will be rescaled to have shape (config[\"height\"], config[\"width\"], 1) using bilinear sampling before being replicated to fill in three channels to feed a ResNet-50 and finally extract the flattened 2048-dimensional output embedding. args : Extra args. Currently unused. kwargs : Extra kwargs. Currently unused.","title":"sensor"},{"location":"api/core/base_abstractions/sensor/#corebase_abstractionssensor","text":"[view_source]","title":"core.base_abstractions.sensor"},{"location":"api/core/base_abstractions/sensor/#sensor","text":"class Sensor ( Generic [ EnvType , SubTaskType ]) [view_source] Represents a sensor that provides data from the environment to agent. The user of this class needs to implement the get_observation method and the user is also required to set the below attributes: Attributes uuid : universally unique id. observation_space : gym.Space object corresponding to observation of sensor.","title":"Sensor"},{"location":"api/core/base_abstractions/sensor/#sensorget_observation","text":"| get_observation ( env : EnvType , task : Optional [ SubTaskType ], * args : Any , ** kwargs : Any ) -> Any [view_source] Returns observations from the environment (or task). Parameters env : The environment the sensor is used upon. task : (Optionally) a Task from which the sensor should get data. Returns Current observation for Sensor.","title":"Sensor.get_observation"},{"location":"api/core/base_abstractions/sensor/#sensorsuite","text":"class SensorSuite ( Generic [ EnvType ]) [view_source] Represents a set of sensors, with each sensor being identified through a unique id. Attributes sensors : list containing sensors for the environment, uuid of each sensor must be unique.","title":"SensorSuite"},{"location":"api/core/base_abstractions/sensor/#sensorsuite__init__","text":"| __init__ ( sensors : Sequence [ Sensor ]) -> None [view_source] Initializer. Parameters param sensors : the sensors that will be included in the suite.","title":"SensorSuite.__init__"},{"location":"api/core/base_abstractions/sensor/#sensorsuiteget","text":"| get ( uuid : str ) -> Sensor [view_source] Return sensor with the given uuid . Parameters uuid : The unique id of the sensor Returns The sensor with unique id uuid .","title":"SensorSuite.get"},{"location":"api/core/base_abstractions/sensor/#sensorsuiteget_observations","text":"| get_observations ( env : EnvType , task : Optional [ SubTaskType ], ** kwargs : Any ) -> Dict [ str , Any ] [view_source] Get all observations corresponding to the sensors in the suite. Parameters env : The environment from which to get the observation. task : (Optionally) the task from which to get the observation. Returns Data from all sensors packaged inside a Dict.","title":"SensorSuite.get_observations"},{"location":"api/core/base_abstractions/sensor/#visionsensor","text":"class VisionSensor ( Sensor [ EnvType , SubTaskType ]) [view_source]","title":"VisionSensor"},{"location":"api/core/base_abstractions/sensor/#visionsensor__init__","text":"| __init__ ( mean : Optional [ np . ndarray ] = None , stdev : Optional [ np . ndarray ] = None , height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"vision\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : Optional [ int ] = None , unnormalized_infimum : float = - np . inf , unnormalized_supremum : float = np . inf , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : The images will be normalized with means config[\"mean\"] and standard deviations config[\"stdev\"] . If both config[\"height\"] and config[\"width\"] are non-negative integers then the image returned from the environment will be rescaled to have config[\"height\"] rows and config[\"width\"] columns using bilinear sampling. The universally unique identifier will be set as config[\"uuid\"] . args : Extra args. Currently unused. kwargs : Extra kwargs. Currently unused.","title":"VisionSensor.__init__"},{"location":"api/core/base_abstractions/sensor/#visionsensorheight","text":"| @property | height () -> Optional [ int ] [view_source] Height that input image will be rescale to have. Returns The height as a non-negative integer or None if no rescaling is done.","title":"VisionSensor.height"},{"location":"api/core/base_abstractions/sensor/#visionsensorwidth","text":"| @property | width () -> Optional [ int ] [view_source] Width that input image will be rescale to have. Returns The width as a non-negative integer or None if no rescaling is done.","title":"VisionSensor.width"},{"location":"api/core/base_abstractions/sensor/#rgbsensor","text":"class RGBSensor ( VisionSensor [ EnvType , SubTaskType ], ABC ) [view_source]","title":"RGBSensor"},{"location":"api/core/base_abstractions/sensor/#rgbsensor__init__","text":"| __init__ ( use_resnet_normalization : bool = False , mean : Optional [ np . ndarray ] = np . array ( | [[[ 0.485 , 0.456 , 0.406 ]]], dtype = np . float32 | ), stdev : Optional [ np . ndarray ] = np . array ( | [[[ 0.229 , 0.224 , 0.225 ]]], dtype = np . float32 | ), height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"rgb\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 3 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 1.0 , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : If config[\"use_resnet_normalization\"] is True then the RGB images will be normalized with means [0.485, 0.456, 0.406] and standard deviations [0.229, 0.224, 0.225] (i.e. using the standard resnet normalization). If both config[\"height\"] and config[\"width\"] are non-negative integers then the RGB image returned from the environment will be rescaled to have shape (config[\"height\"], config[\"width\"], 3) using bilinear sampling. args : Extra args. Currently unused. kwargs : Extra kwargs. Currently unused.","title":"RGBSensor.__init__"},{"location":"api/core/base_abstractions/sensor/#depthsensor","text":"class DepthSensor ( VisionSensor [ EnvType , SubTaskType ], ABC ) [view_source]","title":"DepthSensor"},{"location":"api/core/base_abstractions/sensor/#depthsensor__init__","text":"| __init__ ( use_normalization : bool = False , mean : Optional [ np . ndarray ] = np . array ([[ 0.5 ]], dtype = np . float32 ), stdev : Optional [ np . ndarray ] = np . array ([[ 0.25 ]], dtype = np . float32 ), height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"depth\" , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 1 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 5.0 , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : If config[\"use_normalization\"] is True then the depth images will be normalized with mean 0.5 and standard deviation 0.25. If both config[\"height\"] and config[\"width\"] are non-negative integers then the depth image returned from the environment will be rescaled to have shape (config[\"height\"], config[\"width\"]) using bilinear sampling. args : Extra args. Currently unused. kwargs : Extra kwargs. Currently unused.","title":"DepthSensor.__init__"},{"location":"api/core/base_abstractions/sensor/#resnetsensor","text":"class ResNetSensor ( VisionSensor [ EnvType , SubTaskType ], ABC ) [view_source]","title":"ResNetSensor"},{"location":"api/core/base_abstractions/sensor/#resnetsensorto","text":"| to ( device : torch . device ) -> \"ResNetSensor\" [view_source] Moves sensor to specified device. Parameters device : The device for the sensor.","title":"ResNetSensor.to"},{"location":"api/core/base_abstractions/sensor/#rgbresnetsensor","text":"class RGBResNetSensor ( ResNetSensor [ EnvType , SubTaskType ], ABC ) [view_source]","title":"RGBResNetSensor"},{"location":"api/core/base_abstractions/sensor/#rgbresnetsensor__init__","text":"| __init__ ( use_resnet_normalization : bool = True , mean : Optional [ np . ndarray ] = np . array ( | [[[ 0.485 , 0.456 , 0.406 ]]], dtype = np . float32 | ), stdev : Optional [ np . ndarray ] = np . array ( | [[[ 0.229 , 0.224 , 0.225 ]]], dtype = np . float32 | ), height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"rgbresnet\" , output_shape : Optional [ Tuple [ int , ... ]] = ( 2048 ,), output_channels : Optional [ int ] = None , unnormalized_infimum : float = - np . inf , unnormalized_supremum : float = np . inf , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : If config[\"use_resnet_normalization\"] is True then the RGB images will be normalized with means [0.485, 0.456, 0.406] and standard deviations [0.229, 0.224, 0.225] (i.e. using the standard resnet normalization). If both config[\"height\"] and config[\"width\"] are non-negative integers then the RGB image returned from the environment will be rescaled to have shape (config[\"height\"], config[\"width\"], 3) using bilinear sampling before being fed to a ResNet-50 and extracting the flattened 2048-dimensional output embedding. args : Extra args. Currently unused. kwargs : Extra kwargs. Currently unused.","title":"RGBResNetSensor.__init__"},{"location":"api/core/base_abstractions/sensor/#depthresnetsensor","text":"class DepthResNetSensor ( ResNetSensor [ EnvType , SubTaskType ], ABC ) [view_source]","title":"DepthResNetSensor"},{"location":"api/core/base_abstractions/sensor/#depthresnetsensor__init__","text":"| __init__ ( use_normalization : bool = False , mean : Optional [ np . ndarray ] = np . array ([[ 0.5 ]], dtype = np . float32 ), stdev : Optional [ np . ndarray ] = np . array ([[ 0.25 ]], dtype = np . float32 ), height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = \"depthresnet\" , output_shape : Optional [ Tuple [ int , ... ]] = ( 2048 ,), output_channels : Optional [ int ] = None , unnormalized_infimum : float = - np . inf , unnormalized_supremum : float = np . inf , scale_first : bool = True , ** kwargs : Any ) [view_source] Initializer. Parameters config : If config[\"use_normalization\"] is True then the depth images will be normalized with mean 0.5 and standard deviation 0.25. If both config[\"height\"] and config[\"width\"] are non-negative integers then the depth image returned from the environment will be rescaled to have shape (config[\"height\"], config[\"width\"], 1) using bilinear sampling before being replicated to fill in three channels to feed a ResNet-50 and finally extract the flattened 2048-dimensional output embedding. args : Extra args. Currently unused. kwargs : Extra kwargs. Currently unused.","title":"DepthResNetSensor.__init__"},{"location":"api/core/base_abstractions/task/","text":"core.base_abstractions.task # [view_source] Defines the primary data structures by which agents interact with their environment. Task # class Task ( Generic [ EnvType ]) [view_source] An abstract class defining a, goal directed, 'task.' Agents interact with their environment through a task by taking a step after which they receive new observations, rewards, and (potentially) other useful information. A Task is a helpful generalization of the OpenAI gym's Env class and allows for multiple tasks (e.g. point and object navigation) to be defined on a single environment (e.g. AI2-THOR). Attributes env : The environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : Dictionary of (k, v) pairs defining task goals and other task information. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors. Task.action_space # | @property | @abstractmethod | action_space () -> gym . Space [view_source] Task's action space. Returns The action space for the task. Task.render # | @abstractmethod | render ( mode : str = \"rgb\" , * args , ** kwargs ) -> np . ndarray [view_source] Render the current task state. Rendered task state can come in any supported modes. Parameters mode : The mode in which to render. For example, you might have a 'rgb' mode that renders the agent's egocentric viewpoint or a 'dev' mode returning additional information. args : Extra args. kwargs : Extra kwargs. Returns An numpy array corresponding to the requested render. Task.step # | step ( action : Union [ int , Sequence [ int ]]) -> RLStepResult [view_source] Take an action in the environment (one per agent). Takes the action in the environment corresponding to self.class_action_names()[action] for each action if it's a Sequence and returns observations (& rewards and any additional information) corresponding to the agent's new state. Note that this function should not be overwritten without care (instead implement the _step function). Parameters action : The action to take. Returns A RLStepResult object encoding the new observations, reward, and (possibly) additional information. Task.reached_max_steps # | reached_max_steps () -> bool [view_source] Has the agent reached the maximum number of steps. Task.reached_terminal_state # | @abstractmethod | reached_terminal_state () -> bool [view_source] Has the agent reached a terminal state (excluding reaching the maximum number of steps). Task.is_done # | is_done () -> bool [view_source] Did the agent reach a terminal state or performed the maximum number of steps. Task.num_steps_taken # | num_steps_taken () -> int [view_source] Number of steps taken by the agent in the task so far. Task.class_action_names # | @classmethod | @abstractmethod | class_action_names ( cls , ** kwargs ) -> Tuple [ str , ... ] [view_source] A tuple of action names. Parameters kwargs : Keyword arguments. Returns Tuple of (ordered) action names so that taking action running task.step(i) corresponds to taking action task.class_action_names()[i]. Task.action_names # | action_names () -> Tuple [ str , ... ] [view_source] Action names of the Task instance. This method should be overwritten if class_action_names requires key word arguments to determine the number of actions. Task.total_actions # | @property | total_actions () -> int [view_source] Total number of actions available to an agent in this Task. Task.index_to_action # | index_to_action ( index : int ) -> str [view_source] Returns the action name correspond to index . Task.close # | @abstractmethod | close () -> None [view_source] Closes the environment and any other files opened by the Task (if applicable). Task.metrics # | metrics () -> Dict [ str , Any ] [view_source] Computes metrics related to the task after the task's completion. By default this function is automatically called during training and the reported metrics logged to tensorboard. Returns A dictionary where every key is a string (the metric's name) and the value is the value of the metric. Task.query_expert # | query_expert ( ** kwargs ) -> Tuple [ Any , bool ] [view_source] Query the expert policy for this task. Returns A tuple (x, y) where x is the expert action (or policy) and y is False \\ if the expert could not determine the optimal action (otherwise True). Here y \\ is used for masking. Even when y is False, x should still lie in the space of \\ possible values (e.g. if x is the expert policy then x should be the correct length, \\ sum to 1, and have non-negative entries). Task.cumulative_reward # | @property | cumulative_reward () -> float [view_source] Mean per-agent total cumulative in the task so far. Returns Mean per-agent cumulative reward as a float. TaskSampler # class TaskSampler ( abc . ABC ) [view_source] Abstract class defining a how new tasks are sampled. TaskSampler.length # | @property | @abstractmethod | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). TaskSampler.total_unique # | @property | @abstractmethod | total_unique () -> Optional [ Union [ int , float ]] [view_source] Total unique tasks. Returns Total number of unique tasks that can be sampled. Can be float('inf') or, if the total unique is not known, None. TaskSampler.last_sampled_task # | @property | @abstractmethod | last_sampled_task () -> Optional [ Task ] [view_source] Get the most recently sampled Task. Returns The most recently sampled Task. TaskSampler.next_task # | @abstractmethod | next_task ( force_advance_scene : bool = False ) -> Optional [ Task ] [view_source] Get the next task in the sampler's stream. Parameters force_advance_scene : Used to (if applicable) force the task sampler to use a new scene for the next task. This is useful if, during training, you would like to train with one scene for some number of steps and then explicitly control when you begin training with the next scene. Returns The next Task in the sampler's stream if a next task exists. Otherwise None. TaskSampler.close # | @abstractmethod | close () -> None [view_source] Closes any open environments or streams. Should be run when done sampling. TaskSampler.all_observation_spaces_equal # | @property | @abstractmethod | all_observation_spaces_equal () -> bool [view_source] Checks if all observation spaces of tasks that can be sampled are equal. This will almost always simply return True . A case in which it should return False includes, for example, a setting where you design a TaskSampler that can generate different types of tasks, i.e. point navigation tasks and object navigation tasks. In this case, these different tasks may output different types of observations. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. TaskSampler.reset # | @abstractmethod | reset () -> None [view_source] Resets task sampler to its original state (except for any seed). TaskSampler.set_seed # | @abstractmethod | set_seed ( seed : int ) -> None [view_source] Sets new RNG seed. Parameters seed : New seed.","title":"task"},{"location":"api/core/base_abstractions/task/#corebase_abstractionstask","text":"[view_source] Defines the primary data structures by which agents interact with their environment.","title":"core.base_abstractions.task"},{"location":"api/core/base_abstractions/task/#task","text":"class Task ( Generic [ EnvType ]) [view_source] An abstract class defining a, goal directed, 'task.' Agents interact with their environment through a task by taking a step after which they receive new observations, rewards, and (potentially) other useful information. A Task is a helpful generalization of the OpenAI gym's Env class and allows for multiple tasks (e.g. point and object navigation) to be defined on a single environment (e.g. AI2-THOR). Attributes env : The environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : Dictionary of (k, v) pairs defining task goals and other task information. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors.","title":"Task"},{"location":"api/core/base_abstractions/task/#taskaction_space","text":"| @property | @abstractmethod | action_space () -> gym . Space [view_source] Task's action space. Returns The action space for the task.","title":"Task.action_space"},{"location":"api/core/base_abstractions/task/#taskrender","text":"| @abstractmethod | render ( mode : str = \"rgb\" , * args , ** kwargs ) -> np . ndarray [view_source] Render the current task state. Rendered task state can come in any supported modes. Parameters mode : The mode in which to render. For example, you might have a 'rgb' mode that renders the agent's egocentric viewpoint or a 'dev' mode returning additional information. args : Extra args. kwargs : Extra kwargs. Returns An numpy array corresponding to the requested render.","title":"Task.render"},{"location":"api/core/base_abstractions/task/#taskstep","text":"| step ( action : Union [ int , Sequence [ int ]]) -> RLStepResult [view_source] Take an action in the environment (one per agent). Takes the action in the environment corresponding to self.class_action_names()[action] for each action if it's a Sequence and returns observations (& rewards and any additional information) corresponding to the agent's new state. Note that this function should not be overwritten without care (instead implement the _step function). Parameters action : The action to take. Returns A RLStepResult object encoding the new observations, reward, and (possibly) additional information.","title":"Task.step"},{"location":"api/core/base_abstractions/task/#taskreached_max_steps","text":"| reached_max_steps () -> bool [view_source] Has the agent reached the maximum number of steps.","title":"Task.reached_max_steps"},{"location":"api/core/base_abstractions/task/#taskreached_terminal_state","text":"| @abstractmethod | reached_terminal_state () -> bool [view_source] Has the agent reached a terminal state (excluding reaching the maximum number of steps).","title":"Task.reached_terminal_state"},{"location":"api/core/base_abstractions/task/#taskis_done","text":"| is_done () -> bool [view_source] Did the agent reach a terminal state or performed the maximum number of steps.","title":"Task.is_done"},{"location":"api/core/base_abstractions/task/#tasknum_steps_taken","text":"| num_steps_taken () -> int [view_source] Number of steps taken by the agent in the task so far.","title":"Task.num_steps_taken"},{"location":"api/core/base_abstractions/task/#taskclass_action_names","text":"| @classmethod | @abstractmethod | class_action_names ( cls , ** kwargs ) -> Tuple [ str , ... ] [view_source] A tuple of action names. Parameters kwargs : Keyword arguments. Returns Tuple of (ordered) action names so that taking action running task.step(i) corresponds to taking action task.class_action_names()[i].","title":"Task.class_action_names"},{"location":"api/core/base_abstractions/task/#taskaction_names","text":"| action_names () -> Tuple [ str , ... ] [view_source] Action names of the Task instance. This method should be overwritten if class_action_names requires key word arguments to determine the number of actions.","title":"Task.action_names"},{"location":"api/core/base_abstractions/task/#tasktotal_actions","text":"| @property | total_actions () -> int [view_source] Total number of actions available to an agent in this Task.","title":"Task.total_actions"},{"location":"api/core/base_abstractions/task/#taskindex_to_action","text":"| index_to_action ( index : int ) -> str [view_source] Returns the action name correspond to index .","title":"Task.index_to_action"},{"location":"api/core/base_abstractions/task/#taskclose","text":"| @abstractmethod | close () -> None [view_source] Closes the environment and any other files opened by the Task (if applicable).","title":"Task.close"},{"location":"api/core/base_abstractions/task/#taskmetrics","text":"| metrics () -> Dict [ str , Any ] [view_source] Computes metrics related to the task after the task's completion. By default this function is automatically called during training and the reported metrics logged to tensorboard. Returns A dictionary where every key is a string (the metric's name) and the value is the value of the metric.","title":"Task.metrics"},{"location":"api/core/base_abstractions/task/#taskquery_expert","text":"| query_expert ( ** kwargs ) -> Tuple [ Any , bool ] [view_source] Query the expert policy for this task. Returns A tuple (x, y) where x is the expert action (or policy) and y is False \\ if the expert could not determine the optimal action (otherwise True). Here y \\ is used for masking. Even when y is False, x should still lie in the space of \\ possible values (e.g. if x is the expert policy then x should be the correct length, \\ sum to 1, and have non-negative entries).","title":"Task.query_expert"},{"location":"api/core/base_abstractions/task/#taskcumulative_reward","text":"| @property | cumulative_reward () -> float [view_source] Mean per-agent total cumulative in the task so far. Returns Mean per-agent cumulative reward as a float.","title":"Task.cumulative_reward"},{"location":"api/core/base_abstractions/task/#tasksampler","text":"class TaskSampler ( abc . ABC ) [view_source] Abstract class defining a how new tasks are sampled.","title":"TaskSampler"},{"location":"api/core/base_abstractions/task/#tasksamplerlength","text":"| @property | @abstractmethod | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"TaskSampler.length"},{"location":"api/core/base_abstractions/task/#tasksamplertotal_unique","text":"| @property | @abstractmethod | total_unique () -> Optional [ Union [ int , float ]] [view_source] Total unique tasks. Returns Total number of unique tasks that can be sampled. Can be float('inf') or, if the total unique is not known, None.","title":"TaskSampler.total_unique"},{"location":"api/core/base_abstractions/task/#tasksamplerlast_sampled_task","text":"| @property | @abstractmethod | last_sampled_task () -> Optional [ Task ] [view_source] Get the most recently sampled Task. Returns The most recently sampled Task.","title":"TaskSampler.last_sampled_task"},{"location":"api/core/base_abstractions/task/#tasksamplernext_task","text":"| @abstractmethod | next_task ( force_advance_scene : bool = False ) -> Optional [ Task ] [view_source] Get the next task in the sampler's stream. Parameters force_advance_scene : Used to (if applicable) force the task sampler to use a new scene for the next task. This is useful if, during training, you would like to train with one scene for some number of steps and then explicitly control when you begin training with the next scene. Returns The next Task in the sampler's stream if a next task exists. Otherwise None.","title":"TaskSampler.next_task"},{"location":"api/core/base_abstractions/task/#tasksamplerclose","text":"| @abstractmethod | close () -> None [view_source] Closes any open environments or streams. Should be run when done sampling.","title":"TaskSampler.close"},{"location":"api/core/base_abstractions/task/#tasksamplerall_observation_spaces_equal","text":"| @property | @abstractmethod | all_observation_spaces_equal () -> bool [view_source] Checks if all observation spaces of tasks that can be sampled are equal. This will almost always simply return True . A case in which it should return False includes, for example, a setting where you design a TaskSampler that can generate different types of tasks, i.e. point navigation tasks and object navigation tasks. In this case, these different tasks may output different types of observations. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"TaskSampler.all_observation_spaces_equal"},{"location":"api/core/base_abstractions/task/#tasksamplerreset","text":"| @abstractmethod | reset () -> None [view_source] Resets task sampler to its original state (except for any seed).","title":"TaskSampler.reset"},{"location":"api/core/base_abstractions/task/#tasksamplerset_seed","text":"| @abstractmethod | set_seed ( seed : int ) -> None [view_source] Sets new RNG seed. Parameters seed : New seed.","title":"TaskSampler.set_seed"},{"location":"api/core/models/basic_models/","text":"core.models.basic_models # [view_source] Basic building block torch networks that can be used across a variety of tasks. SimpleCNN # class SimpleCNN ( nn . Module ) [view_source] A Simple N-Conv CNN followed by a fully connected layer. Takes in observations (of type gym.spaces.dict) and produces an embedding of the rgb_uuid and/or depth_uuid components. Attributes observation_space : The observation_space of the agent, should have rgb_uuid or depth_uuid as a component (otherwise it is a blind model). output_size : The size of the embedding vector to produce. SimpleCNN.__init__ # | __init__ ( observation_space : SpaceDict , output_size : int , layer_channels : Sequence [ int ] = ( 32 , 64 , 32 ), kernel_sizes : Sequence [ Tuple [ int , int ]] = (( 8 , 8 ), ( 4 , 4 ), ( 3 , 3 )), layers_stride : Sequence [ Tuple [ int , int ]] = (( 4 , 4 ), ( 2 , 2 ), ( 1 , 1 )), paddings : Sequence [ Tuple [ int , int ]] = (( 0 , 0 ), ( 0 , 0 ), ( 0 , 0 )), dilations : Sequence [ Tuple [ int , int ]] = (( 1 , 1 ), ( 1 , 1 ), ( 1 , 1 )), rgb_uuid : str = \"rgb\" , depth_uuid : str = \"depth\" , flatten : bool = True , output_relu : bool = True ) [view_source] Initializer. Parameters observation_space : See class attributes documentation. output_size : See class attributes documentation. SimpleCNN.layer_init # | @staticmethod | layer_init ( cnn ) -> None [view_source] Initialize layer parameters using Kaiming normal. SimpleCNN.is_blind # | @property | is_blind () [view_source] True if the observation space doesn't include self.rgb_uuid or self.depth_uuid . RNNStateEncoder # class RNNStateEncoder ( nn . Module ) [view_source] A simple RNN-based model playing a role in many baseline embodied- navigation agents. See seq_forward for more details of how this model is used. RNNStateEncoder.__init__ # | __init__ ( input_size : int , hidden_size : int , num_layers : int = 1 , rnn_type : str = \"GRU\" , trainable_masked_hidden_state : bool = False ) [view_source] An RNN for encoding the state in RL. Supports masking the hidden state during various timesteps in the forward lass. Parameters input_size : The input size of the RNN. hidden_size : The hidden size. num_layers : The number of recurrent layers. rnn_type : The RNN cell type. Must be GRU or LSTM. trainable_masked_hidden_state : If True the initial hidden state (used at the start of a Task) is trainable (as opposed to being a vector of zeros). RNNStateEncoder.layer_init # | layer_init () [view_source] Initialize the RNN parameters in the model. RNNStateEncoder.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] The number of recurrent layers in the network. RNNStateEncoder.single_forward # | single_forward ( x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor ) -> Tuple [ | torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]] | ] [view_source] Forward for a single-step input. RNNStateEncoder.seq_forward # | seq_forward ( x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor ) -> Tuple [ | torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]] | ] [view_source] Forward for a sequence of length T. Parameters x : (Steps, Samplers, Agents, -1) tensor. hidden_states : The starting hidden states. masks : A (Steps, Samplers, Agents) tensor. The masks to be applied to hidden state at every timestep, equal to 0 whenever the previous step finalized the task, 1 elsewhere.","title":"basic_models"},{"location":"api/core/models/basic_models/#coremodelsbasic_models","text":"[view_source] Basic building block torch networks that can be used across a variety of tasks.","title":"core.models.basic_models"},{"location":"api/core/models/basic_models/#simplecnn","text":"class SimpleCNN ( nn . Module ) [view_source] A Simple N-Conv CNN followed by a fully connected layer. Takes in observations (of type gym.spaces.dict) and produces an embedding of the rgb_uuid and/or depth_uuid components. Attributes observation_space : The observation_space of the agent, should have rgb_uuid or depth_uuid as a component (otherwise it is a blind model). output_size : The size of the embedding vector to produce.","title":"SimpleCNN"},{"location":"api/core/models/basic_models/#simplecnn__init__","text":"| __init__ ( observation_space : SpaceDict , output_size : int , layer_channels : Sequence [ int ] = ( 32 , 64 , 32 ), kernel_sizes : Sequence [ Tuple [ int , int ]] = (( 8 , 8 ), ( 4 , 4 ), ( 3 , 3 )), layers_stride : Sequence [ Tuple [ int , int ]] = (( 4 , 4 ), ( 2 , 2 ), ( 1 , 1 )), paddings : Sequence [ Tuple [ int , int ]] = (( 0 , 0 ), ( 0 , 0 ), ( 0 , 0 )), dilations : Sequence [ Tuple [ int , int ]] = (( 1 , 1 ), ( 1 , 1 ), ( 1 , 1 )), rgb_uuid : str = \"rgb\" , depth_uuid : str = \"depth\" , flatten : bool = True , output_relu : bool = True ) [view_source] Initializer. Parameters observation_space : See class attributes documentation. output_size : See class attributes documentation.","title":"SimpleCNN.__init__"},{"location":"api/core/models/basic_models/#simplecnnlayer_init","text":"| @staticmethod | layer_init ( cnn ) -> None [view_source] Initialize layer parameters using Kaiming normal.","title":"SimpleCNN.layer_init"},{"location":"api/core/models/basic_models/#simplecnnis_blind","text":"| @property | is_blind () [view_source] True if the observation space doesn't include self.rgb_uuid or self.depth_uuid .","title":"SimpleCNN.is_blind"},{"location":"api/core/models/basic_models/#rnnstateencoder","text":"class RNNStateEncoder ( nn . Module ) [view_source] A simple RNN-based model playing a role in many baseline embodied- navigation agents. See seq_forward for more details of how this model is used.","title":"RNNStateEncoder"},{"location":"api/core/models/basic_models/#rnnstateencoder__init__","text":"| __init__ ( input_size : int , hidden_size : int , num_layers : int = 1 , rnn_type : str = \"GRU\" , trainable_masked_hidden_state : bool = False ) [view_source] An RNN for encoding the state in RL. Supports masking the hidden state during various timesteps in the forward lass. Parameters input_size : The input size of the RNN. hidden_size : The hidden size. num_layers : The number of recurrent layers. rnn_type : The RNN cell type. Must be GRU or LSTM. trainable_masked_hidden_state : If True the initial hidden state (used at the start of a Task) is trainable (as opposed to being a vector of zeros).","title":"RNNStateEncoder.__init__"},{"location":"api/core/models/basic_models/#rnnstateencoderlayer_init","text":"| layer_init () [view_source] Initialize the RNN parameters in the model.","title":"RNNStateEncoder.layer_init"},{"location":"api/core/models/basic_models/#rnnstateencodernum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] The number of recurrent layers in the network.","title":"RNNStateEncoder.num_recurrent_layers"},{"location":"api/core/models/basic_models/#rnnstateencodersingle_forward","text":"| single_forward ( x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor ) -> Tuple [ | torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]] | ] [view_source] Forward for a single-step input.","title":"RNNStateEncoder.single_forward"},{"location":"api/core/models/basic_models/#rnnstateencoderseq_forward","text":"| seq_forward ( x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor ) -> Tuple [ | torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]] | ] [view_source] Forward for a sequence of length T. Parameters x : (Steps, Samplers, Agents, -1) tensor. hidden_states : The starting hidden states. masks : A (Steps, Samplers, Agents) tensor. The masks to be applied to hidden state at every timestep, equal to 0 whenever the previous step finalized the task, 1 elsewhere.","title":"RNNStateEncoder.seq_forward"},{"location":"api/plugins/babyai_plugin/babyai_constants/","text":"plugins.babyai_plugin.babyai_constants # [view_source]","title":"babyai_constants"},{"location":"api/plugins/babyai_plugin/babyai_constants/#pluginsbabyai_pluginbabyai_constants","text":"[view_source]","title":"plugins.babyai_plugin.babyai_constants"},{"location":"api/plugins/babyai_plugin/babyai_models/","text":"plugins.babyai_plugin.babyai_models # [view_source] BabyAIACModelWrapped # class BabyAIACModelWrapped ( babyai . model . ACModel ) [view_source] BabyAIACModelWrapped.forward_once # | forward_once ( obs , memory , instr_embedding = None ) [view_source] Copied (with minor modifications) from babyai.model.ACModel.forward(...) .","title":"babyai_models"},{"location":"api/plugins/babyai_plugin/babyai_models/#pluginsbabyai_pluginbabyai_models","text":"[view_source]","title":"plugins.babyai_plugin.babyai_models"},{"location":"api/plugins/babyai_plugin/babyai_models/#babyaiacmodelwrapped","text":"class BabyAIACModelWrapped ( babyai . model . ACModel ) [view_source]","title":"BabyAIACModelWrapped"},{"location":"api/plugins/babyai_plugin/babyai_models/#babyaiacmodelwrappedforward_once","text":"| forward_once ( obs , memory , instr_embedding = None ) [view_source] Copied (with minor modifications) from babyai.model.ACModel.forward(...) .","title":"BabyAIACModelWrapped.forward_once"},{"location":"api/plugins/babyai_plugin/babyai_tasks/","text":"plugins.babyai_plugin.babyai_tasks # [view_source]","title":"babyai_tasks"},{"location":"api/plugins/babyai_plugin/babyai_tasks/#pluginsbabyai_pluginbabyai_tasks","text":"[view_source]","title":"plugins.babyai_plugin.babyai_tasks"},{"location":"api/plugins/babyai_plugin/scripts/download_babyai_expert_demos/","text":"plugins.babyai_plugin.scripts.download_babyai_expert_demos # [view_source] get_args # get_args () [view_source] Creates the argument parser and parses input arguments.","title":"download_babyai_expert_demos"},{"location":"api/plugins/babyai_plugin/scripts/download_babyai_expert_demos/#pluginsbabyai_pluginscriptsdownload_babyai_expert_demos","text":"[view_source]","title":"plugins.babyai_plugin.scripts.download_babyai_expert_demos"},{"location":"api/plugins/babyai_plugin/scripts/download_babyai_expert_demos/#get_args","text":"get_args () [view_source] Creates the argument parser and parses input arguments.","title":"get_args"},{"location":"api/plugins/babyai_plugin/scripts/get_instr_length_percentiles/","text":"plugins.babyai_plugin.scripts.get_instr_length_percentiles # [view_source]","title":"get_instr_length_percentiles"},{"location":"api/plugins/babyai_plugin/scripts/get_instr_length_percentiles/#pluginsbabyai_pluginscriptsget_instr_length_percentiles","text":"[view_source]","title":"plugins.babyai_plugin.scripts.get_instr_length_percentiles"},{"location":"api/plugins/babyai_plugin/scripts/truncate_expert_demos/","text":"plugins.babyai_plugin.scripts.truncate_expert_demos # [view_source]","title":"truncate_expert_demos"},{"location":"api/plugins/babyai_plugin/scripts/truncate_expert_demos/#pluginsbabyai_pluginscriptstruncate_expert_demos","text":"[view_source]","title":"plugins.babyai_plugin.scripts.truncate_expert_demos"},{"location":"api/plugins/habitat_plugin/habitat_constants/","text":"plugins.habitat_plugin.habitat_constants # [view_source]","title":"habitat_constants"},{"location":"api/plugins/habitat_plugin/habitat_constants/#pluginshabitat_pluginhabitat_constants","text":"[view_source]","title":"plugins.habitat_plugin.habitat_constants"},{"location":"api/plugins/habitat_plugin/habitat_environment/","text":"plugins.habitat_plugin.habitat_environment # [view_source] A wrapper for interacting with the Habitat environment.","title":"habitat_environment"},{"location":"api/plugins/habitat_plugin/habitat_environment/#pluginshabitat_pluginhabitat_environment","text":"[view_source] A wrapper for interacting with the Habitat environment.","title":"plugins.habitat_plugin.habitat_environment"},{"location":"api/plugins/habitat_plugin/habitat_preprocessors/","text":"plugins.habitat_plugin.habitat_preprocessors # [view_source] ResnetPreProcessorHabitat # class ResnetPreProcessorHabitat ( Preprocessor ) [view_source] Preprocess RGB or depth image using a ResNet model.","title":"habitat_preprocessors"},{"location":"api/plugins/habitat_plugin/habitat_preprocessors/#pluginshabitat_pluginhabitat_preprocessors","text":"[view_source]","title":"plugins.habitat_plugin.habitat_preprocessors"},{"location":"api/plugins/habitat_plugin/habitat_preprocessors/#resnetpreprocessorhabitat","text":"class ResnetPreProcessorHabitat ( Preprocessor ) [view_source] Preprocess RGB or depth image using a ResNet model.","title":"ResnetPreProcessorHabitat"},{"location":"api/plugins/habitat_plugin/habitat_sensors/","text":"plugins.habitat_plugin.habitat_sensors # [view_source]","title":"habitat_sensors"},{"location":"api/plugins/habitat_plugin/habitat_sensors/#pluginshabitat_pluginhabitat_sensors","text":"[view_source]","title":"plugins.habitat_plugin.habitat_sensors"},{"location":"api/plugins/habitat_plugin/habitat_task_samplers/","text":"plugins.habitat_plugin.habitat_task_samplers # [view_source] PointNavTaskSampler # class PointNavTaskSampler ( TaskSampler ) [view_source] PointNavTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] @return: Number of total tasks remaining that can be sampled. Can be float('inf'). PointNavTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] @return: True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. ObjectNavTaskSampler # class ObjectNavTaskSampler ( TaskSampler ) [view_source] ObjectNavTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] @return: Number of total tasks remaining that can be sampled. Can be float('inf'). ObjectNavTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] @return: True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"habitat_task_samplers"},{"location":"api/plugins/habitat_plugin/habitat_task_samplers/#pluginshabitat_pluginhabitat_task_samplers","text":"[view_source]","title":"plugins.habitat_plugin.habitat_task_samplers"},{"location":"api/plugins/habitat_plugin/habitat_task_samplers/#pointnavtasksampler","text":"class PointNavTaskSampler ( TaskSampler ) [view_source]","title":"PointNavTaskSampler"},{"location":"api/plugins/habitat_plugin/habitat_task_samplers/#pointnavtasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] @return: Number of total tasks remaining that can be sampled. Can be float('inf').","title":"PointNavTaskSampler.length"},{"location":"api/plugins/habitat_plugin/habitat_task_samplers/#pointnavtasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] @return: True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"PointNavTaskSampler.all_observation_spaces_equal"},{"location":"api/plugins/habitat_plugin/habitat_task_samplers/#objectnavtasksampler","text":"class ObjectNavTaskSampler ( TaskSampler ) [view_source]","title":"ObjectNavTaskSampler"},{"location":"api/plugins/habitat_plugin/habitat_task_samplers/#objectnavtasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] @return: Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ObjectNavTaskSampler.length"},{"location":"api/plugins/habitat_plugin/habitat_task_samplers/#objectnavtasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] @return: True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"ObjectNavTaskSampler.all_observation_spaces_equal"},{"location":"api/plugins/habitat_plugin/habitat_tasks/","text":"plugins.habitat_plugin.habitat_tasks # [view_source]","title":"habitat_tasks"},{"location":"api/plugins/habitat_plugin/habitat_tasks/#pluginshabitat_pluginhabitat_tasks","text":"[view_source]","title":"plugins.habitat_plugin.habitat_tasks"},{"location":"api/plugins/habitat_plugin/habitat_utils/","text":"plugins.habitat_plugin.habitat_utils # [view_source] construct_env_configs # construct_env_configs ( config : Config , allow_scene_repeat : bool = False ) -> List [ Config ] [view_source] Create list of Habitat Configs for training on multiple processes To allow better performance, dataset are split into small ones for each individual env, grouped by scenes. Parameters config : configs that contain num_processes as well as information necessary to create individual environments. allow_scene_repeat : if True and the number of distinct scenes in the dataset is less than the total number of processes this will result in scenes being repeated across processes. If False , then if the total number of processes is greater than the number of scenes, this will result in a RuntimeError exception being raised. Returns List of Configs, one for each process.","title":"habitat_utils"},{"location":"api/plugins/habitat_plugin/habitat_utils/#pluginshabitat_pluginhabitat_utils","text":"[view_source]","title":"plugins.habitat_plugin.habitat_utils"},{"location":"api/plugins/habitat_plugin/habitat_utils/#construct_env_configs","text":"construct_env_configs ( config : Config , allow_scene_repeat : bool = False ) -> List [ Config ] [view_source] Create list of Habitat Configs for training on multiple processes To allow better performance, dataset are split into small ones for each individual env, grouped by scenes. Parameters config : configs that contain num_processes as well as information necessary to create individual environments. allow_scene_repeat : if True and the number of distinct scenes in the dataset is less than the total number of processes this will result in scenes being repeated across processes. If False , then if the total number of processes is greater than the number of scenes, this will result in a RuntimeError exception being raised. Returns List of Configs, one for each process.","title":"construct_env_configs"},{"location":"api/plugins/habitat_plugin/scripts/agent_demo/","text":"plugins.habitat_plugin.scripts.agent_demo # [view_source]","title":"agent_demo"},{"location":"api/plugins/habitat_plugin/scripts/agent_demo/#pluginshabitat_pluginscriptsagent_demo","text":"[view_source]","title":"plugins.habitat_plugin.scripts.agent_demo"},{"location":"api/plugins/habitat_plugin/scripts/make_map/","text":"plugins.habitat_plugin.scripts.make_map # [view_source]","title":"make_map"},{"location":"api/plugins/habitat_plugin/scripts/make_map/#pluginshabitat_pluginscriptsmake_map","text":"[view_source]","title":"plugins.habitat_plugin.scripts.make_map"},{"location":"api/plugins/ithor_plugin/ithor_constants/","text":"plugins.ithor_plugin.ithor_constants # [view_source] Common constants used when training agents to complete tasks in iTHOR, the interactive version of AI2-THOR.","title":"ithor_constants"},{"location":"api/plugins/ithor_plugin/ithor_constants/#pluginsithor_pluginithor_constants","text":"[view_source] Common constants used when training agents to complete tasks in iTHOR, the interactive version of AI2-THOR.","title":"plugins.ithor_plugin.ithor_constants"},{"location":"api/plugins/ithor_plugin/ithor_environment/","text":"plugins.ithor_plugin.ithor_environment # [view_source] A wrapper for engaging with the THOR environment. IThorEnvironment # class IThorEnvironment ( object ) [view_source] Wrapper for the ai2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on AI2-THOR. Attributes controller : The ai2thor controller. IThorEnvironment.__init__ # | __init__ ( x_display : Optional [ str ] = None , docker_enabled : bool = False , local_thor_build : Optional [ str ] = None , visibility_distance : float = VISIBILITY_DISTANCE , fov : float = FOV , player_screen_width : int = 300 , player_screen_height : int = 300 , quality : str = \"Very Low\" , restrict_to_initially_reachable_points : bool = False , make_agents_visible : bool = True , object_open_speed : float = 1.0 , simplify_physics : bool = False ) -> None [view_source] Initializer. Parameters x_display : The x display into which to launch ai2thor (possibly necessarily if you are running on a server without an attached display). docker_enabled : Whether or not to run thor in a docker container (useful on a server without an attached display so that you don't have to start an x display). local_thor_build : The path to a local build of ai2thor. This is probably not necessary for your use case and can be safely ignored. visibility_distance : The distance (in meters) at which objects, in the viewport of the agent, are considered visible by ai2thor and will have their \"visible\" flag be set to True in the metadata. fov : The agent's camera's field of view. player_screen_width : The width resolution (in pixels) of the images returned by ai2thor. player_screen_height : The height resolution (in pixels) of the images returned by ai2thor. quality : The quality at which to render. Possible quality settings can be found in ai2thor._quality_settings.QUALITY_SETTINGS . restrict_to_initially_reachable_points : Whether or not to restrict the agent to locations in ai2thor that were found to be (initially) reachable by the agent (i.e. reachable by the agent after resetting the scene). This can be useful if you want to ensure there are only a fixed set of locations where the agent can go. make_agents_visible : Whether or not the agent should be visible. Most noticable when there are multiple agents or when quality settings are high so that the agent casts a shadow. object_open_speed : How quickly objects should be opened. High speeds mean faster simulation but also mean that opening objects have a lot of kinetic energy and can, possibly, knock other objects away. simplify_physics : Whether or not to simplify physics when applicable. Currently this only simplies object interactions when opening drawers (when simplified, objects within a drawer do not slide around on their own when the drawer is opened or closed, instead they are effectively glued down). IThorEnvironment.scene_name # | @property | scene_name () -> str [view_source] Current ai2thor scene. IThorEnvironment.current_frame # | @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view. IThorEnvironment.last_event # | @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller. IThorEnvironment.started # | @property | started () -> bool [view_source] Has the ai2thor controller been started. IThorEnvironment.last_action # | @property | last_action () -> str [view_source] Last action, as a string, taken by the agent. IThorEnvironment.last_action # | @last_action . setter | last_action ( value : str ) -> None [view_source] Set the last action taken by the agent. Doing this is rewriting history, be careful. IThorEnvironment.last_action_success # | @property | last_action_success () -> bool [view_source] Was the last action taken by the agent a success? IThorEnvironment.last_action_success # | @last_action_success . setter | last_action_success ( value : bool ) -> None [view_source] Set whether or not the last action taken by the agent was a success. Doing this is rewriting history, be careful. IThorEnvironment.last_action_return # | @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" . IThorEnvironment.last_action_return # | @last_action_return . setter | last_action_return ( value : Any ) -> None [view_source] Set the value returned by the last action. Doing this is rewriting history, be careful. IThorEnvironment.start # | start ( scene_name : Optional [ str ], move_mag : float = 0.25 , ** kwargs , ,) -> None [view_source] Starts the ai2thor controller if it was previously stopped. After starting, reset will be called with the scene name and move magnitude. Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to reset. IThorEnvironment.stop # | stop () -> None [view_source] Stops the ai2thor controller. IThorEnvironment.reset # | reset ( scene_name : Optional [ str ], move_mag : float = 0.25 , ** kwargs , ,) [view_source] Resets the ai2thor in a new scene. Resets ai2thor into a new scene and initializes the scene/agents with prespecified settings (e.g. move magnitude). Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to the controller \"Initialize\" action. IThorEnvironment.teleport_agent_to # | teleport_agent_to ( x : float , y : float , z : float , rotation : float , horizon : float , standing : Optional [ bool ] = None , force_action : bool = False , only_initially_reachable : Optional [ bool ] = None , verbose = True , ignore_y_diffs = False ) -> None [view_source] Helper function teleporting the agent to a given location. IThorEnvironment.random_reachable_state # | random_reachable_state ( seed : int = None ) -> Dict [view_source] Returns a random reachable location in the scene. IThorEnvironment.randomize_agent_location # | randomize_agent_location ( seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None ) -> Dict [view_source] Teleports the agent to a random reachable location in the scene. IThorEnvironment.object_pixels_in_frame # | object_pixels_in_frame ( object_id : str , hide_all : bool = True , hide_transparent : bool = False ) -> np . ndarray [view_source] Return an mask for a given object in the agent's current view. Parameters object_id : The id of the object. hide_all : Whether or not to hide all other objects in the scene before getting the mask. hide_transparent : Whether or not partially transparent objects are considered to occlude the object. Returns A numpy array of the mask. IThorEnvironment.object_pixels_on_grid # | object_pixels_on_grid ( object_id : str , grid_shape : Tuple [ int , int ], hide_all : bool = True , hide_transparent : bool = False ) -> np . ndarray [view_source] Like object_pixels_in_frame but counts object pixels in a partitioning of the image. IThorEnvironment.object_in_hand # | object_in_hand () [view_source] Object metadata for the object in the agent's hand. IThorEnvironment.initially_reachable_points # | @property | initially_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that were reachable after initially resetting. IThorEnvironment.initially_reachable_points_set # | @property | initially_reachable_points_set () -> Set [ Tuple [ float , float ]] [view_source] Set of (x,z) locations in the scene that were reachable after initially resetting. IThorEnvironment.currently_reachable_points # | @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable. IThorEnvironment.get_agent_location # | get_agent_location () -> Dict [ str , Union [ float , bool ]] [view_source] Gets agent's location. IThorEnvironment.step # | step ( action_dict : Dict [ str , Union [ str , int , float ]]) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment. IThorEnvironment.position_dist # | @staticmethod | position_dist ( p0 : Mapping [ str , Any ], p1 : Mapping [ str , Any ], ignore_y : bool = False ) -> float [view_source] Distance between two points of the form {\"x\": x, \"y\":y, \"z\":z\"}. IThorEnvironment.rotation_dist # | @staticmethod | rotation_dist ( a : Dict [ str , float ], b : Dict [ str , float ]) [view_source] Distance between rotations. IThorEnvironment.closest_object_with_properties # | closest_object_with_properties ( properties : Dict [ str , Any ]) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that has the given properties. IThorEnvironment.closest_visible_object_of_type # | closest_visible_object_of_type ( object_type : str ) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that is visible and has the given type. IThorEnvironment.closest_object_of_type # | closest_object_of_type ( object_type : str ) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that has the given type. IThorEnvironment.closest_reachable_point_to_position # | closest_reachable_point_to_position ( position : Dict [ str , float ]) -> Tuple [ Dict [ str , float ], float ] [view_source] Of all reachable positions, find the one that is closest to the given location. IThorEnvironment.all_objects # | all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata. IThorEnvironment.all_objects_with_properties # | all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties. IThorEnvironment.visible_objects # | visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects.","title":"ithor_environment"},{"location":"api/plugins/ithor_plugin/ithor_environment/#pluginsithor_pluginithor_environment","text":"[view_source] A wrapper for engaging with the THOR environment.","title":"plugins.ithor_plugin.ithor_environment"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironment","text":"class IThorEnvironment ( object ) [view_source] Wrapper for the ai2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on AI2-THOR. Attributes controller : The ai2thor controller.","title":"IThorEnvironment"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironment__init__","text":"| __init__ ( x_display : Optional [ str ] = None , docker_enabled : bool = False , local_thor_build : Optional [ str ] = None , visibility_distance : float = VISIBILITY_DISTANCE , fov : float = FOV , player_screen_width : int = 300 , player_screen_height : int = 300 , quality : str = \"Very Low\" , restrict_to_initially_reachable_points : bool = False , make_agents_visible : bool = True , object_open_speed : float = 1.0 , simplify_physics : bool = False ) -> None [view_source] Initializer. Parameters x_display : The x display into which to launch ai2thor (possibly necessarily if you are running on a server without an attached display). docker_enabled : Whether or not to run thor in a docker container (useful on a server without an attached display so that you don't have to start an x display). local_thor_build : The path to a local build of ai2thor. This is probably not necessary for your use case and can be safely ignored. visibility_distance : The distance (in meters) at which objects, in the viewport of the agent, are considered visible by ai2thor and will have their \"visible\" flag be set to True in the metadata. fov : The agent's camera's field of view. player_screen_width : The width resolution (in pixels) of the images returned by ai2thor. player_screen_height : The height resolution (in pixels) of the images returned by ai2thor. quality : The quality at which to render. Possible quality settings can be found in ai2thor._quality_settings.QUALITY_SETTINGS . restrict_to_initially_reachable_points : Whether or not to restrict the agent to locations in ai2thor that were found to be (initially) reachable by the agent (i.e. reachable by the agent after resetting the scene). This can be useful if you want to ensure there are only a fixed set of locations where the agent can go. make_agents_visible : Whether or not the agent should be visible. Most noticable when there are multiple agents or when quality settings are high so that the agent casts a shadow. object_open_speed : How quickly objects should be opened. High speeds mean faster simulation but also mean that opening objects have a lot of kinetic energy and can, possibly, knock other objects away. simplify_physics : Whether or not to simplify physics when applicable. Currently this only simplies object interactions when opening drawers (when simplified, objects within a drawer do not slide around on their own when the drawer is opened or closed, instead they are effectively glued down).","title":"IThorEnvironment.__init__"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentscene_name","text":"| @property | scene_name () -> str [view_source] Current ai2thor scene.","title":"IThorEnvironment.scene_name"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentcurrent_frame","text":"| @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view.","title":"IThorEnvironment.current_frame"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_event","text":"| @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller.","title":"IThorEnvironment.last_event"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentstarted","text":"| @property | started () -> bool [view_source] Has the ai2thor controller been started.","title":"IThorEnvironment.started"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action","text":"| @property | last_action () -> str [view_source] Last action, as a string, taken by the agent.","title":"IThorEnvironment.last_action"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action_1","text":"| @last_action . setter | last_action ( value : str ) -> None [view_source] Set the last action taken by the agent. Doing this is rewriting history, be careful.","title":"IThorEnvironment.last_action"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action_success","text":"| @property | last_action_success () -> bool [view_source] Was the last action taken by the agent a success?","title":"IThorEnvironment.last_action_success"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action_success_1","text":"| @last_action_success . setter | last_action_success ( value : bool ) -> None [view_source] Set whether or not the last action taken by the agent was a success. Doing this is rewriting history, be careful.","title":"IThorEnvironment.last_action_success"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action_return","text":"| @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" .","title":"IThorEnvironment.last_action_return"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentlast_action_return_1","text":"| @last_action_return . setter | last_action_return ( value : Any ) -> None [view_source] Set the value returned by the last action. Doing this is rewriting history, be careful.","title":"IThorEnvironment.last_action_return"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentstart","text":"| start ( scene_name : Optional [ str ], move_mag : float = 0.25 , ** kwargs , ,) -> None [view_source] Starts the ai2thor controller if it was previously stopped. After starting, reset will be called with the scene name and move magnitude. Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to reset.","title":"IThorEnvironment.start"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentstop","text":"| stop () -> None [view_source] Stops the ai2thor controller.","title":"IThorEnvironment.stop"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentreset","text":"| reset ( scene_name : Optional [ str ], move_mag : float = 0.25 , ** kwargs , ,) [view_source] Resets the ai2thor in a new scene. Resets ai2thor into a new scene and initializes the scene/agents with prespecified settings (e.g. move magnitude). Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to the controller \"Initialize\" action.","title":"IThorEnvironment.reset"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentteleport_agent_to","text":"| teleport_agent_to ( x : float , y : float , z : float , rotation : float , horizon : float , standing : Optional [ bool ] = None , force_action : bool = False , only_initially_reachable : Optional [ bool ] = None , verbose = True , ignore_y_diffs = False ) -> None [view_source] Helper function teleporting the agent to a given location.","title":"IThorEnvironment.teleport_agent_to"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentrandom_reachable_state","text":"| random_reachable_state ( seed : int = None ) -> Dict [view_source] Returns a random reachable location in the scene.","title":"IThorEnvironment.random_reachable_state"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentrandomize_agent_location","text":"| randomize_agent_location ( seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None ) -> Dict [view_source] Teleports the agent to a random reachable location in the scene.","title":"IThorEnvironment.randomize_agent_location"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentobject_pixels_in_frame","text":"| object_pixels_in_frame ( object_id : str , hide_all : bool = True , hide_transparent : bool = False ) -> np . ndarray [view_source] Return an mask for a given object in the agent's current view. Parameters object_id : The id of the object. hide_all : Whether or not to hide all other objects in the scene before getting the mask. hide_transparent : Whether or not partially transparent objects are considered to occlude the object. Returns A numpy array of the mask.","title":"IThorEnvironment.object_pixels_in_frame"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentobject_pixels_on_grid","text":"| object_pixels_on_grid ( object_id : str , grid_shape : Tuple [ int , int ], hide_all : bool = True , hide_transparent : bool = False ) -> np . ndarray [view_source] Like object_pixels_in_frame but counts object pixels in a partitioning of the image.","title":"IThorEnvironment.object_pixels_on_grid"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentobject_in_hand","text":"| object_in_hand () [view_source] Object metadata for the object in the agent's hand.","title":"IThorEnvironment.object_in_hand"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentinitially_reachable_points","text":"| @property | initially_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that were reachable after initially resetting.","title":"IThorEnvironment.initially_reachable_points"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentinitially_reachable_points_set","text":"| @property | initially_reachable_points_set () -> Set [ Tuple [ float , float ]] [view_source] Set of (x,z) locations in the scene that were reachable after initially resetting.","title":"IThorEnvironment.initially_reachable_points_set"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentcurrently_reachable_points","text":"| @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable.","title":"IThorEnvironment.currently_reachable_points"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentget_agent_location","text":"| get_agent_location () -> Dict [ str , Union [ float , bool ]] [view_source] Gets agent's location.","title":"IThorEnvironment.get_agent_location"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentstep","text":"| step ( action_dict : Dict [ str , Union [ str , int , float ]]) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment.","title":"IThorEnvironment.step"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentposition_dist","text":"| @staticmethod | position_dist ( p0 : Mapping [ str , Any ], p1 : Mapping [ str , Any ], ignore_y : bool = False ) -> float [view_source] Distance between two points of the form {\"x\": x, \"y\":y, \"z\":z\"}.","title":"IThorEnvironment.position_dist"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentrotation_dist","text":"| @staticmethod | rotation_dist ( a : Dict [ str , float ], b : Dict [ str , float ]) [view_source] Distance between rotations.","title":"IThorEnvironment.rotation_dist"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentclosest_object_with_properties","text":"| closest_object_with_properties ( properties : Dict [ str , Any ]) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that has the given properties.","title":"IThorEnvironment.closest_object_with_properties"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentclosest_visible_object_of_type","text":"| closest_visible_object_of_type ( object_type : str ) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that is visible and has the given type.","title":"IThorEnvironment.closest_visible_object_of_type"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentclosest_object_of_type","text":"| closest_object_of_type ( object_type : str ) -> Optional [ Dict [ str , Any ]] [view_source] Find the object closest to the agent that has the given type.","title":"IThorEnvironment.closest_object_of_type"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentclosest_reachable_point_to_position","text":"| closest_reachable_point_to_position ( position : Dict [ str , float ]) -> Tuple [ Dict [ str , float ], float ] [view_source] Of all reachable positions, find the one that is closest to the given location.","title":"IThorEnvironment.closest_reachable_point_to_position"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentall_objects","text":"| all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata.","title":"IThorEnvironment.all_objects"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentall_objects_with_properties","text":"| all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties.","title":"IThorEnvironment.all_objects_with_properties"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironmentvisible_objects","text":"| visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects.","title":"IThorEnvironment.visible_objects"},{"location":"api/plugins/ithor_plugin/ithor_sensors/","text":"plugins.ithor_plugin.ithor_sensors # [view_source] RGBSensorThor # class RGBSensorThor ( RGBSensor [ IThorEnvironment , Task [ IThorEnvironment ]]) [view_source] Sensor for RGB images in iTHOR. Returns from a running IThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"ithor_sensors"},{"location":"api/plugins/ithor_plugin/ithor_sensors/#pluginsithor_pluginithor_sensors","text":"[view_source]","title":"plugins.ithor_plugin.ithor_sensors"},{"location":"api/plugins/ithor_plugin/ithor_sensors/#rgbsensorthor","text":"class RGBSensorThor ( RGBSensor [ IThorEnvironment , Task [ IThorEnvironment ]]) [view_source] Sensor for RGB images in iTHOR. Returns from a running IThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"RGBSensorThor"},{"location":"api/plugins/ithor_plugin/ithor_task_samplers/","text":"plugins.ithor_plugin.ithor_task_samplers # [view_source] ObjectNavTaskSampler # class ObjectNavTaskSampler ( TaskSampler ) [view_source] ObjectNavTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). ObjectNavTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"ithor_task_samplers"},{"location":"api/plugins/ithor_plugin/ithor_task_samplers/#pluginsithor_pluginithor_task_samplers","text":"[view_source]","title":"plugins.ithor_plugin.ithor_task_samplers"},{"location":"api/plugins/ithor_plugin/ithor_task_samplers/#objectnavtasksampler","text":"class ObjectNavTaskSampler ( TaskSampler ) [view_source]","title":"ObjectNavTaskSampler"},{"location":"api/plugins/ithor_plugin/ithor_task_samplers/#objectnavtasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ObjectNavTaskSampler.length"},{"location":"api/plugins/ithor_plugin/ithor_task_samplers/#objectnavtasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"ObjectNavTaskSampler.all_observation_spaces_equal"},{"location":"api/plugins/ithor_plugin/ithor_tasks/","text":"plugins.ithor_plugin.ithor_tasks # [view_source] ObjectNavTask # class ObjectNavTask ( Task [ IThorEnvironment ]) [view_source] Defines the object navigation task in AI2-THOR. In object navigation an agent is randomly initialized into an AI2-THOR scene and must find an object of a given type (e.g. tomato, television, etc). An object is considered found if the agent takes an End action and the object is visible to the agent (see here for a definition of visibiliy in AI2-THOR). The actions available to an agent in this task are: Move ahead Moves agent ahead by 0.25 meters. Rotate left / rotate right Rotates the agent by 90 degrees counter-clockwise / clockwise. Look down / look up Changes agent view angle by 30 degrees up or down. An agent cannot look more than 30 degrees above horizontal or less than 60 degrees below horizontal. End Ends the task and the agent receives a positive reward if the object type is visible to the agent, otherwise it receives a negative reward. Attributes env : The ai2thor environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : The task info. Must contain a field \"object_type\" that specifies, as a string, the goal object type. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors. ObjectNavTask.__init__ # | __init__ ( env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) -> None [view_source] Initializer. See class documentation for parameter definitions. ObjectNavTask.judge # | judge () -> float [view_source] Compute the reward after having taken a step.","title":"ithor_tasks"},{"location":"api/plugins/ithor_plugin/ithor_tasks/#pluginsithor_pluginithor_tasks","text":"[view_source]","title":"plugins.ithor_plugin.ithor_tasks"},{"location":"api/plugins/ithor_plugin/ithor_tasks/#objectnavtask","text":"class ObjectNavTask ( Task [ IThorEnvironment ]) [view_source] Defines the object navigation task in AI2-THOR. In object navigation an agent is randomly initialized into an AI2-THOR scene and must find an object of a given type (e.g. tomato, television, etc). An object is considered found if the agent takes an End action and the object is visible to the agent (see here for a definition of visibiliy in AI2-THOR). The actions available to an agent in this task are: Move ahead Moves agent ahead by 0.25 meters. Rotate left / rotate right Rotates the agent by 90 degrees counter-clockwise / clockwise. Look down / look up Changes agent view angle by 30 degrees up or down. An agent cannot look more than 30 degrees above horizontal or less than 60 degrees below horizontal. End Ends the task and the agent receives a positive reward if the object type is visible to the agent, otherwise it receives a negative reward. Attributes env : The ai2thor environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : The task info. Must contain a field \"object_type\" that specifies, as a string, the goal object type. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors.","title":"ObjectNavTask"},{"location":"api/plugins/ithor_plugin/ithor_tasks/#objectnavtask__init__","text":"| __init__ ( env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) -> None [view_source] Initializer. See class documentation for parameter definitions.","title":"ObjectNavTask.__init__"},{"location":"api/plugins/ithor_plugin/ithor_tasks/#objectnavtaskjudge","text":"| judge () -> float [view_source] Compute the reward after having taken a step.","title":"ObjectNavTask.judge"},{"location":"api/plugins/ithor_plugin/ithor_util/","text":"plugins.ithor_plugin.ithor_util # [view_source] round_to_factor # round_to_factor ( num : float , base : int ) -> int [view_source] Rounds floating point number to the nearest integer multiple of the given base. E.g., for floating number 90.1 and integer base 45, the result is 90. Attributes num : floating point number to be rounded. base : integer base","title":"ithor_util"},{"location":"api/plugins/ithor_plugin/ithor_util/#pluginsithor_pluginithor_util","text":"[view_source]","title":"plugins.ithor_plugin.ithor_util"},{"location":"api/plugins/ithor_plugin/ithor_util/#round_to_factor","text":"round_to_factor ( num : float , base : int ) -> int [view_source] Rounds floating point number to the nearest integer multiple of the given base. E.g., for floating number 90.1 and integer base 45, the result is 90. Attributes num : floating point number to be rounded. base : integer base","title":"round_to_factor"},{"location":"api/plugins/ithor_plugin/scripts/make_objectnav_debug_dataset/","text":"plugins.ithor_plugin.scripts.make_objectnav_debug_dataset # [view_source]","title":"make_objectnav_debug_dataset"},{"location":"api/plugins/ithor_plugin/scripts/make_objectnav_debug_dataset/#pluginsithor_pluginscriptsmake_objectnav_debug_dataset","text":"[view_source]","title":"plugins.ithor_plugin.scripts.make_objectnav_debug_dataset"},{"location":"api/plugins/lighthouse_plugin/lighthouse_environment/","text":"plugins.lighthouse_plugin.lighthouse_environment # [view_source]","title":"lighthouse_environment"},{"location":"api/plugins/lighthouse_plugin/lighthouse_environment/#pluginslighthouse_pluginlighthouse_environment","text":"[view_source]","title":"plugins.lighthouse_plugin.lighthouse_environment"},{"location":"api/plugins/lighthouse_plugin/lighthouse_models/","text":"plugins.lighthouse_plugin.lighthouse_models # [view_source]","title":"lighthouse_models"},{"location":"api/plugins/lighthouse_plugin/lighthouse_models/#pluginslighthouse_pluginlighthouse_models","text":"[view_source]","title":"plugins.lighthouse_plugin.lighthouse_models"},{"location":"api/plugins/lighthouse_plugin/lighthouse_sensors/","text":"plugins.lighthouse_plugin.lighthouse_sensors # [view_source]","title":"lighthouse_sensors"},{"location":"api/plugins/lighthouse_plugin/lighthouse_sensors/#pluginslighthouse_pluginlighthouse_sensors","text":"[view_source]","title":"plugins.lighthouse_plugin.lighthouse_sensors"},{"location":"api/plugins/lighthouse_plugin/lighthouse_tasks/","text":"plugins.lighthouse_plugin.lighthouse_tasks # [view_source] LightHouseTask # class LightHouseTask ( Task [ LightHouseEnvironment ], abc . ABC ) [view_source] Defines an abstract embodied task in the light house gridworld. Attributes env : The light house environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : Dictionary of (k, v) pairs defining task goals and other task information. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors. LightHouseTask.__init__ # | __init__ ( env : LightHouseEnvironment , sensors : Union [ SensorSuite , List [ Sensor ]], task_info : Dict [ str , Any ], max_steps : int , ** kwargs , ,) -> None [view_source] Initializer. See class documentation for parameter definitions.","title":"lighthouse_tasks"},{"location":"api/plugins/lighthouse_plugin/lighthouse_tasks/#pluginslighthouse_pluginlighthouse_tasks","text":"[view_source]","title":"plugins.lighthouse_plugin.lighthouse_tasks"},{"location":"api/plugins/lighthouse_plugin/lighthouse_tasks/#lighthousetask","text":"class LightHouseTask ( Task [ LightHouseEnvironment ], abc . ABC ) [view_source] Defines an abstract embodied task in the light house gridworld. Attributes env : The light house environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : Dictionary of (k, v) pairs defining task goals and other task information. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors.","title":"LightHouseTask"},{"location":"api/plugins/lighthouse_plugin/lighthouse_tasks/#lighthousetask__init__","text":"| __init__ ( env : LightHouseEnvironment , sensors : Union [ SensorSuite , List [ Sensor ]], task_info : Dict [ str , Any ], max_steps : int , ** kwargs , ,) -> None [view_source] Initializer. See class documentation for parameter definitions.","title":"LightHouseTask.__init__"},{"location":"api/plugins/lighthouse_plugin/lighthouse_util/","text":"plugins.lighthouse_plugin.lighthouse_util # [view_source]","title":"lighthouse_util"},{"location":"api/plugins/lighthouse_plugin/lighthouse_util/#pluginslighthouse_pluginlighthouse_util","text":"[view_source]","title":"plugins.lighthouse_plugin.lighthouse_util"},{"location":"api/plugins/minigrid_plugin/minigrid_environments/","text":"plugins.minigrid_plugin.minigrid_environments # [view_source] FastCrossing # class FastCrossing ( CrossingEnv ) [view_source] Similar to CrossingEnv , but to support faster task sampling as per repeat_failed_task_for_min_steps flag in MiniGridTaskSampler. AskForHelpSimpleCrossing # class AskForHelpSimpleCrossing ( CrossingEnv ) [view_source] Corresponds to WC FAULTY SWITCH environment. AskForHelpSimpleCrossing.step # | step ( action : int ) [view_source] Reveal the observation only if the toggle action is executed.","title":"minigrid_environments"},{"location":"api/plugins/minigrid_plugin/minigrid_environments/#pluginsminigrid_pluginminigrid_environments","text":"[view_source]","title":"plugins.minigrid_plugin.minigrid_environments"},{"location":"api/plugins/minigrid_plugin/minigrid_environments/#fastcrossing","text":"class FastCrossing ( CrossingEnv ) [view_source] Similar to CrossingEnv , but to support faster task sampling as per repeat_failed_task_for_min_steps flag in MiniGridTaskSampler.","title":"FastCrossing"},{"location":"api/plugins/minigrid_plugin/minigrid_environments/#askforhelpsimplecrossing","text":"class AskForHelpSimpleCrossing ( CrossingEnv ) [view_source] Corresponds to WC FAULTY SWITCH environment.","title":"AskForHelpSimpleCrossing"},{"location":"api/plugins/minigrid_plugin/minigrid_environments/#askforhelpsimplecrossingstep","text":"| step ( action : int ) [view_source] Reveal the observation only if the toggle action is executed.","title":"AskForHelpSimpleCrossing.step"},{"location":"api/plugins/minigrid_plugin/minigrid_models/","text":"plugins.minigrid_plugin.minigrid_models # [view_source]","title":"minigrid_models"},{"location":"api/plugins/minigrid_plugin/minigrid_models/#pluginsminigrid_pluginminigrid_models","text":"[view_source]","title":"plugins.minigrid_plugin.minigrid_models"},{"location":"api/plugins/minigrid_plugin/minigrid_offpolicy/","text":"plugins.minigrid_plugin.minigrid_offpolicy # [view_source]","title":"minigrid_offpolicy"},{"location":"api/plugins/minigrid_plugin/minigrid_offpolicy/#pluginsminigrid_pluginminigrid_offpolicy","text":"[view_source]","title":"plugins.minigrid_plugin.minigrid_offpolicy"},{"location":"api/plugins/minigrid_plugin/minigrid_sensors/","text":"plugins.minigrid_plugin.minigrid_sensors # [view_source]","title":"minigrid_sensors"},{"location":"api/plugins/minigrid_plugin/minigrid_sensors/#pluginsminigrid_pluginminigrid_sensors","text":"[view_source]","title":"plugins.minigrid_plugin.minigrid_sensors"},{"location":"api/plugins/minigrid_plugin/minigrid_tasks/","text":"plugins.minigrid_plugin.minigrid_tasks # [view_source] MiniGridTask # class MiniGridTask ( Task [ CrossingEnv ]) [view_source] MiniGridTask.generate_graph # | generate_graph () -> nx . DiGraph [view_source] The generated graph is based on the fully observable grid (as the expert sees it all). env: environment to generate the graph over","title":"minigrid_tasks"},{"location":"api/plugins/minigrid_plugin/minigrid_tasks/#pluginsminigrid_pluginminigrid_tasks","text":"[view_source]","title":"plugins.minigrid_plugin.minigrid_tasks"},{"location":"api/plugins/minigrid_plugin/minigrid_tasks/#minigridtask","text":"class MiniGridTask ( Task [ CrossingEnv ]) [view_source]","title":"MiniGridTask"},{"location":"api/plugins/minigrid_plugin/minigrid_tasks/#minigridtaskgenerate_graph","text":"| generate_graph () -> nx . DiGraph [view_source] The generated graph is based on the fully observable grid (as the expert sees it all). env: environment to generate the graph over","title":"MiniGridTask.generate_graph"},{"location":"api/plugins/minigrid_plugin/configs/minigrid_nomemory/","text":"plugins.minigrid_plugin.configs.minigrid_nomemory # [view_source] Experiment Config for MiniGrid tutorial.","title":"minigrid_nomemory"},{"location":"api/plugins/minigrid_plugin/configs/minigrid_nomemory/#pluginsminigrid_pluginconfigsminigrid_nomemory","text":"[view_source] Experiment Config for MiniGrid tutorial.","title":"plugins.minigrid_plugin.configs.minigrid_nomemory"},{"location":"api/plugins/robothor_plugin/robothor_constants/","text":"plugins.robothor_plugin.robothor_constants # [view_source]","title":"robothor_constants"},{"location":"api/plugins/robothor_plugin/robothor_constants/#pluginsrobothor_pluginrobothor_constants","text":"[view_source]","title":"plugins.robothor_plugin.robothor_constants"},{"location":"api/plugins/robothor_plugin/robothor_environment/","text":"plugins.robothor_plugin.robothor_environment # [view_source] RoboThorEnvironment # class RoboThorEnvironment () [view_source] Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration RoboThorEnvironment.initialize_grid_dimensions # | initialize_grid_dimensions ( reachable_points : Collection [ Dict [ str , float ]]) -> Tuple [ int , int , int , int ] [view_source] Computes bounding box for reachable points quantized with the current gridSize. RoboThorEnvironment.distance_from_point_to_object_type # | distance_from_point_to_object_type ( point : Dict [ str , float ], object_type : str ) -> float [view_source] Minimal geodesic distance from a point to an object of the given type. It might return -1.0 for unreachable targets. RoboThorEnvironment.distance_to_object_type # | distance_to_object_type ( object_type : str ) -> float [view_source] Minimal geodesic distance to object of given type from agent's current location. It might return -1.0 for unreachable targets. RoboThorEnvironment.distance_to_point # | distance_to_point ( target : Dict [ str , float ]) -> float [view_source] Minimal geodesic distance to end point from agent's current location. It might return -1.0 for unreachable targets. RoboThorEnvironment.agent_state # | agent_state () -> Dict [view_source] Return agent position, rotation and horizon. RoboThorEnvironment.reset # | reset ( scene_name : str = None , filtered_objects : Optional [ List [ str ]] = None ) -> None [view_source] Resets scene to a known initial state. RoboThorEnvironment.random_reachable_state # | random_reachable_state ( seed : Optional [ int ] = None ) -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Returns a random reachable location in the scene. RoboThorEnvironment.randomize_agent_location # | randomize_agent_location ( seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None ) -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Teleports the agent to a random reachable location in the scene. RoboThorEnvironment.currently_reachable_points # | @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable. RoboThorEnvironment.scene_name # | @property | scene_name () -> str [view_source] Current ai2thor scene. RoboThorEnvironment.current_frame # | @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view. RoboThorEnvironment.current_depth # | @property | current_depth () -> np . ndarray [view_source] Returns depth image corresponding to the agent's egocentric view. RoboThorEnvironment.last_event # | @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller. RoboThorEnvironment.last_action # | @property | last_action () -> str [view_source] Last action, as a string, taken by the agent. RoboThorEnvironment.last_action_success # | @property | last_action_success () -> bool [view_source] Was the last action taken by the agent a success? RoboThorEnvironment.last_action_return # | @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" . RoboThorEnvironment.step # | step ( action_dict : Dict ) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment. RoboThorEnvironment.stop # | stop () [view_source] Stops the ai2thor controller. RoboThorEnvironment.all_objects # | all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata. RoboThorEnvironment.all_objects_with_properties # | all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties. RoboThorEnvironment.visible_objects # | visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects. RoboThorCachedEnvironment # class RoboThorCachedEnvironment () [view_source] Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration RoboThorCachedEnvironment.agent_state # | agent_state () -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Return agent position, rotation and horizon. RoboThorCachedEnvironment.reset # | reset ( scene_name : str = None ) -> None [view_source] Resets scene to a known initial state. RoboThorCachedEnvironment.currently_reachable_points # | @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable. RoboThorCachedEnvironment.scene_name # | @property | scene_name () -> str [view_source] Current ai2thor scene. RoboThorCachedEnvironment.current_frame # | @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view. RoboThorCachedEnvironment.current_depth # | @property | current_depth () -> np . ndarray [view_source] Returns depth image corresponding to the agent's egocentric view. RoboThorCachedEnvironment.last_event # | @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller. RoboThorCachedEnvironment.last_action # | @property | last_action () -> str [view_source] Last action, as a string, taken by the agent. RoboThorCachedEnvironment.last_action_success # | @property | last_action_success () -> bool [view_source] In the cached environment, all actions succeed. RoboThorCachedEnvironment.last_action_return # | @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" . RoboThorCachedEnvironment.step # | step ( action_dict : Dict [ str , Union [ str , int , float ]]) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment. RoboThorCachedEnvironment.stop # | stop () [view_source] Stops the ai2thor controller. RoboThorCachedEnvironment.all_objects # | all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata. RoboThorCachedEnvironment.all_objects_with_properties # | all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties. RoboThorCachedEnvironment.visible_objects # | visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects.","title":"robothor_environment"},{"location":"api/plugins/robothor_plugin/robothor_environment/#pluginsrobothor_pluginrobothor_environment","text":"[view_source]","title":"plugins.robothor_plugin.robothor_environment"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironment","text":"class RoboThorEnvironment () [view_source] Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration","title":"RoboThorEnvironment"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentinitialize_grid_dimensions","text":"| initialize_grid_dimensions ( reachable_points : Collection [ Dict [ str , float ]]) -> Tuple [ int , int , int , int ] [view_source] Computes bounding box for reachable points quantized with the current gridSize.","title":"RoboThorEnvironment.initialize_grid_dimensions"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentdistance_from_point_to_object_type","text":"| distance_from_point_to_object_type ( point : Dict [ str , float ], object_type : str ) -> float [view_source] Minimal geodesic distance from a point to an object of the given type. It might return -1.0 for unreachable targets.","title":"RoboThorEnvironment.distance_from_point_to_object_type"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentdistance_to_object_type","text":"| distance_to_object_type ( object_type : str ) -> float [view_source] Minimal geodesic distance to object of given type from agent's current location. It might return -1.0 for unreachable targets.","title":"RoboThorEnvironment.distance_to_object_type"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentdistance_to_point","text":"| distance_to_point ( target : Dict [ str , float ]) -> float [view_source] Minimal geodesic distance to end point from agent's current location. It might return -1.0 for unreachable targets.","title":"RoboThorEnvironment.distance_to_point"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentagent_state","text":"| agent_state () -> Dict [view_source] Return agent position, rotation and horizon.","title":"RoboThorEnvironment.agent_state"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentreset","text":"| reset ( scene_name : str = None , filtered_objects : Optional [ List [ str ]] = None ) -> None [view_source] Resets scene to a known initial state.","title":"RoboThorEnvironment.reset"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentrandom_reachable_state","text":"| random_reachable_state ( seed : Optional [ int ] = None ) -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Returns a random reachable location in the scene.","title":"RoboThorEnvironment.random_reachable_state"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentrandomize_agent_location","text":"| randomize_agent_location ( seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None ) -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Teleports the agent to a random reachable location in the scene.","title":"RoboThorEnvironment.randomize_agent_location"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentcurrently_reachable_points","text":"| @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable.","title":"RoboThorEnvironment.currently_reachable_points"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentscene_name","text":"| @property | scene_name () -> str [view_source] Current ai2thor scene.","title":"RoboThorEnvironment.scene_name"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentcurrent_frame","text":"| @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view.","title":"RoboThorEnvironment.current_frame"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentcurrent_depth","text":"| @property | current_depth () -> np . ndarray [view_source] Returns depth image corresponding to the agent's egocentric view.","title":"RoboThorEnvironment.current_depth"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentlast_event","text":"| @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller.","title":"RoboThorEnvironment.last_event"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentlast_action","text":"| @property | last_action () -> str [view_source] Last action, as a string, taken by the agent.","title":"RoboThorEnvironment.last_action"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentlast_action_success","text":"| @property | last_action_success () -> bool [view_source] Was the last action taken by the agent a success?","title":"RoboThorEnvironment.last_action_success"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentlast_action_return","text":"| @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" .","title":"RoboThorEnvironment.last_action_return"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentstep","text":"| step ( action_dict : Dict ) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment.","title":"RoboThorEnvironment.step"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentstop","text":"| stop () [view_source] Stops the ai2thor controller.","title":"RoboThorEnvironment.stop"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentall_objects","text":"| all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata.","title":"RoboThorEnvironment.all_objects"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentall_objects_with_properties","text":"| all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties.","title":"RoboThorEnvironment.all_objects_with_properties"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironmentvisible_objects","text":"| visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects.","title":"RoboThorEnvironment.visible_objects"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironment","text":"class RoboThorCachedEnvironment () [view_source] Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration","title":"RoboThorCachedEnvironment"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentagent_state","text":"| agent_state () -> Dict [ str , Union [ Dict [ str , float ], float ]] [view_source] Return agent position, rotation and horizon.","title":"RoboThorCachedEnvironment.agent_state"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentreset","text":"| reset ( scene_name : str = None ) -> None [view_source] Resets scene to a known initial state.","title":"RoboThorCachedEnvironment.reset"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentcurrently_reachable_points","text":"| @property | currently_reachable_points () -> List [ Dict [ str , float ]] [view_source] List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable.","title":"RoboThorCachedEnvironment.currently_reachable_points"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentscene_name","text":"| @property | scene_name () -> str [view_source] Current ai2thor scene.","title":"RoboThorCachedEnvironment.scene_name"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentcurrent_frame","text":"| @property | current_frame () -> np . ndarray [view_source] Returns rgb image corresponding to the agent's egocentric view.","title":"RoboThorCachedEnvironment.current_frame"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentcurrent_depth","text":"| @property | current_depth () -> np . ndarray [view_source] Returns depth image corresponding to the agent's egocentric view.","title":"RoboThorCachedEnvironment.current_depth"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentlast_event","text":"| @property | last_event () -> ai2thor . server . Event [view_source] Last event returned by the controller.","title":"RoboThorCachedEnvironment.last_event"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentlast_action","text":"| @property | last_action () -> str [view_source] Last action, as a string, taken by the agent.","title":"RoboThorCachedEnvironment.last_action"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentlast_action_success","text":"| @property | last_action_success () -> bool [view_source] In the cached environment, all actions succeed.","title":"RoboThorCachedEnvironment.last_action_success"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentlast_action_return","text":"| @property | last_action_return () -> Any [view_source] Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" .","title":"RoboThorCachedEnvironment.last_action_return"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentstep","text":"| step ( action_dict : Dict [ str , Union [ str , int , float ]]) -> ai2thor . server . Event [view_source] Take a step in the ai2thor environment.","title":"RoboThorCachedEnvironment.step"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentstop","text":"| stop () [view_source] Stops the ai2thor controller.","title":"RoboThorCachedEnvironment.stop"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentall_objects","text":"| all_objects () -> List [ Dict [ str , Any ]] [view_source] Return all object metadata.","title":"RoboThorCachedEnvironment.all_objects"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentall_objects_with_properties","text":"| all_objects_with_properties ( properties : Dict [ str , Any ]) -> List [ Dict [ str , Any ]] [view_source] Find all objects with the given properties.","title":"RoboThorCachedEnvironment.all_objects_with_properties"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironmentvisible_objects","text":"| visible_objects () -> List [ Dict [ str , Any ]] [view_source] Return all visible objects.","title":"RoboThorCachedEnvironment.visible_objects"},{"location":"api/plugins/robothor_plugin/robothor_models/","text":"plugins.robothor_plugin.robothor_models # [view_source] ResnetTensorGoalEncoder # class ResnetTensorGoalEncoder ( nn . Module ) [view_source] ResnetTensorGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetTensorObjectNavActorCritic # class ResnetTensorObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] ResnetTensorObjectNavActorCritic.recurrent_hidden_state_size # | @property | recurrent_hidden_state_size () -> Union [ int , Dict [ str , Tuple [ Sequence [ Tuple [ str , Optional [ int ]]], torch . dtype ]]] [view_source] The recurrent hidden state size of the model. ResnetTensorObjectNavActorCritic.is_blind # | @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). ResnetTensorObjectNavActorCritic.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers. ResnetTensorObjectNavActorCritic.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetFasterRCNNTensorsGoalEncoder # class ResnetFasterRCNNTensorsGoalEncoder ( nn . Module ) [view_source] ResnetFasterRCNNTensorsGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetFasterRCNNTensorsObjectNavActorCritic # class ResnetFasterRCNNTensorsObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] ResnetFasterRCNNTensorsObjectNavActorCritic.recurrent_hidden_state_size # | @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model. ResnetFasterRCNNTensorsObjectNavActorCritic.is_blind # | @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). ResnetFasterRCNNTensorsObjectNavActorCritic.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers. ResnetFasterRCNNTensorsObjectNavActorCritic.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"robothor_models"},{"location":"api/plugins/robothor_plugin/robothor_models/#pluginsrobothor_pluginrobothor_models","text":"[view_source]","title":"plugins.robothor_plugin.robothor_models"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnettensorgoalencoder","text":"class ResnetTensorGoalEncoder ( nn . Module ) [view_source]","title":"ResnetTensorGoalEncoder"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnettensorgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetTensorGoalEncoder.get_object_type_encoding"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcritic","text":"class ResnetTensorObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source]","title":"ResnetTensorObjectNavActorCritic"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcriticrecurrent_hidden_state_size","text":"| @property | recurrent_hidden_state_size () -> Union [ int , Dict [ str , Tuple [ Sequence [ Tuple [ str , Optional [ int ]]], torch . dtype ]]] [view_source] The recurrent hidden state size of the model.","title":"ResnetTensorObjectNavActorCritic.recurrent_hidden_state_size"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcriticis_blind","text":"| @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"ResnetTensorObjectNavActorCritic.is_blind"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcriticnum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers.","title":"ResnetTensorObjectNavActorCritic.num_recurrent_layers"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcriticget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetTensorObjectNavActorCritic.get_object_type_encoding"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsgoalencoder","text":"class ResnetFasterRCNNTensorsGoalEncoder ( nn . Module ) [view_source]","title":"ResnetFasterRCNNTensorsGoalEncoder"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetFasterRCNNTensorsGoalEncoder.get_object_type_encoding"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcritic","text":"class ResnetFasterRCNNTensorsObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source]","title":"ResnetFasterRCNNTensorsObjectNavActorCritic"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcriticrecurrent_hidden_state_size","text":"| @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model.","title":"ResnetFasterRCNNTensorsObjectNavActorCritic.recurrent_hidden_state_size"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcriticis_blind","text":"| @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"ResnetFasterRCNNTensorsObjectNavActorCritic.is_blind"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcriticnum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers.","title":"ResnetFasterRCNNTensorsObjectNavActorCritic.num_recurrent_layers"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcriticget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetFasterRCNNTensorsObjectNavActorCritic.get_object_type_encoding"},{"location":"api/plugins/robothor_plugin/robothor_preprocessors/","text":"plugins.robothor_plugin.robothor_preprocessors # [view_source] FasterRCNNPreProcessorRoboThor # class FasterRCNNPreProcessorRoboThor ( Preprocessor ) [view_source] Preprocess RGB image using a ResNet model.","title":"robothor_preprocessors"},{"location":"api/plugins/robothor_plugin/robothor_preprocessors/#pluginsrobothor_pluginrobothor_preprocessors","text":"[view_source]","title":"plugins.robothor_plugin.robothor_preprocessors"},{"location":"api/plugins/robothor_plugin/robothor_preprocessors/#fasterrcnnpreprocessorrobothor","text":"class FasterRCNNPreProcessorRoboThor ( Preprocessor ) [view_source] Preprocess RGB image using a ResNet model.","title":"FasterRCNNPreProcessorRoboThor"},{"location":"api/plugins/robothor_plugin/robothor_sensors/","text":"plugins.robothor_plugin.robothor_sensors # [view_source] RGBSensorRoboThor # class RGBSensorRoboThor ( RGBSensor [ RoboThorEnvironment , Task [ RoboThorEnvironment ]]) [view_source] Sensor for RGB images in RoboTHOR. Returns from a running RoboThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"robothor_sensors"},{"location":"api/plugins/robothor_plugin/robothor_sensors/#pluginsrobothor_pluginrobothor_sensors","text":"[view_source]","title":"plugins.robothor_plugin.robothor_sensors"},{"location":"api/plugins/robothor_plugin/robothor_sensors/#rgbsensorrobothor","text":"class RGBSensorRoboThor ( RGBSensor [ RoboThorEnvironment , Task [ RoboThorEnvironment ]]) [view_source] Sensor for RGB images in RoboTHOR. Returns from a running RoboThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"RGBSensorRoboThor"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/","text":"plugins.robothor_plugin.robothor_task_samplers # [view_source] ObjectNavTaskSampler # class ObjectNavTaskSampler ( TaskSampler ) [view_source] ObjectNavTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). ObjectNavTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. ObjectNavDatasetTaskSampler # class ObjectNavDatasetTaskSampler ( TaskSampler ) [view_source] ObjectNavDatasetTaskSampler.__len__ # | @property | __len__ () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). ObjectNavDatasetTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. ObjectNavDatasetTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). PointNavTaskSampler # class PointNavTaskSampler ( TaskSampler ) [view_source] PointNavTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). PointNavTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. PointNavDatasetTaskSampler # class PointNavDatasetTaskSampler ( TaskSampler ) [view_source] PointNavDatasetTaskSampler.__len__ # | @property | __len__ () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). PointNavDatasetTaskSampler.all_observation_spaces_equal # | @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. PointNavDatasetTaskSampler.length # | @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"robothor_task_samplers"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pluginsrobothor_pluginrobothor_task_samplers","text":"[view_source]","title":"plugins.robothor_plugin.robothor_task_samplers"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#objectnavtasksampler","text":"class ObjectNavTaskSampler ( TaskSampler ) [view_source]","title":"ObjectNavTaskSampler"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#objectnavtasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ObjectNavTaskSampler.length"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#objectnavtasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"ObjectNavTaskSampler.all_observation_spaces_equal"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#objectnavdatasettasksampler","text":"class ObjectNavDatasetTaskSampler ( TaskSampler ) [view_source]","title":"ObjectNavDatasetTaskSampler"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#objectnavdatasettasksampler__len__","text":"| @property | __len__ () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ObjectNavDatasetTaskSampler.__len__"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#objectnavdatasettasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"ObjectNavDatasetTaskSampler.all_observation_spaces_equal"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#objectnavdatasettasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ObjectNavDatasetTaskSampler.length"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pointnavtasksampler","text":"class PointNavTaskSampler ( TaskSampler ) [view_source]","title":"PointNavTaskSampler"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pointnavtasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"PointNavTaskSampler.length"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pointnavtasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"PointNavTaskSampler.all_observation_spaces_equal"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pointnavdatasettasksampler","text":"class PointNavDatasetTaskSampler ( TaskSampler ) [view_source]","title":"PointNavDatasetTaskSampler"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pointnavdatasettasksampler__len__","text":"| @property | __len__ () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"PointNavDatasetTaskSampler.__len__"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pointnavdatasettasksamplerall_observation_spaces_equal","text":"| @property | all_observation_spaces_equal () -> bool [view_source] Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"PointNavDatasetTaskSampler.all_observation_spaces_equal"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pointnavdatasettasksamplerlength","text":"| @property | length () -> Union [ int , float ] [view_source] Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"PointNavDatasetTaskSampler.length"},{"location":"api/plugins/robothor_plugin/robothor_tasks/","text":"plugins.robothor_plugin.robothor_tasks # [view_source] PointNavTask # class PointNavTask ( Task [ RoboThorEnvironment ]) [view_source] PointNavTask.judge # | judge () -> float [view_source] Judge the last event. ObjectNavTask # class ObjectNavTask ( Task [ RoboThorEnvironment ]) [view_source] ObjectNavTask.judge # | judge () -> float [view_source] Judge the last event.","title":"robothor_tasks"},{"location":"api/plugins/robothor_plugin/robothor_tasks/#pluginsrobothor_pluginrobothor_tasks","text":"[view_source]","title":"plugins.robothor_plugin.robothor_tasks"},{"location":"api/plugins/robothor_plugin/robothor_tasks/#pointnavtask","text":"class PointNavTask ( Task [ RoboThorEnvironment ]) [view_source]","title":"PointNavTask"},{"location":"api/plugins/robothor_plugin/robothor_tasks/#pointnavtaskjudge","text":"| judge () -> float [view_source] Judge the last event.","title":"PointNavTask.judge"},{"location":"api/plugins/robothor_plugin/robothor_tasks/#objectnavtask","text":"class ObjectNavTask ( Task [ RoboThorEnvironment ]) [view_source]","title":"ObjectNavTask"},{"location":"api/plugins/robothor_plugin/robothor_tasks/#objectnavtaskjudge","text":"| judge () -> float [view_source] Judge the last event.","title":"ObjectNavTask.judge"},{"location":"api/plugins/robothor_plugin/robothor_viz/","text":"plugins.robothor_plugin.robothor_viz # [view_source]","title":"robothor_viz"},{"location":"api/plugins/robothor_plugin/robothor_viz/#pluginsrobothor_pluginrobothor_viz","text":"[view_source]","title":"plugins.robothor_plugin.robothor_viz"},{"location":"api/plugins/robothor_plugin/configs/debug_objectnav_robothor_rgb_ddppo/","text":"plugins.robothor_plugin.configs.debug_objectnav_robothor_rgb_ddppo # [view_source] ObjectNavRoboThorRGBPPOExperimentConfig # class ObjectNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorRGBPPOExperimentConfig ) [view_source] An Object Navigation experiment configuration in RoboThor.","title":"debug_objectnav_robothor_rgb_ddppo"},{"location":"api/plugins/robothor_plugin/configs/debug_objectnav_robothor_rgb_ddppo/#pluginsrobothor_pluginconfigsdebug_objectnav_robothor_rgb_ddppo","text":"[view_source]","title":"plugins.robothor_plugin.configs.debug_objectnav_robothor_rgb_ddppo"},{"location":"api/plugins/robothor_plugin/configs/debug_objectnav_robothor_rgb_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"class ObjectNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorRGBPPOExperimentConfig ) [view_source] An Object Navigation experiment configuration in RoboThor.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/plugins/robothor_plugin/configs/nav_base/","text":"plugins.robothor_plugin.configs.nav_base # [view_source] NavBaseConfig # class NavBaseConfig ( ExperimentConfig , abc . ABC ) [view_source] A Navigation base configuration in RoboTHOR.","title":"nav_base"},{"location":"api/plugins/robothor_plugin/configs/nav_base/#pluginsrobothor_pluginconfigsnav_base","text":"[view_source]","title":"plugins.robothor_plugin.configs.nav_base"},{"location":"api/plugins/robothor_plugin/configs/nav_base/#navbaseconfig","text":"class NavBaseConfig ( ExperimentConfig , abc . ABC ) [view_source] A Navigation base configuration in RoboTHOR.","title":"NavBaseConfig"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/","text":"plugins.robothor_plugin.configs.objectnav_base # [view_source] ObjectNavBaseConfig # class ObjectNavBaseConfig ( ExperimentConfig , abc . ABC ) [view_source] An Object Navigation base configuration.","title":"objectnav_base"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#pluginsrobothor_pluginconfigsobjectnav_base","text":"[view_source]","title":"plugins.robothor_plugin.configs.objectnav_base"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#objectnavbaseconfig","text":"class ObjectNavBaseConfig ( ExperimentConfig , abc . ABC ) [view_source] An Object Navigation base configuration.","title":"ObjectNavBaseConfig"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/","text":"plugins.robothor_plugin.configs.pointnav_base # [view_source] PointNavBaseConfig # class PointNavBaseConfig ( ExperimentConfig , abc . ABC ) [view_source] A Point Navigation base configuration.","title":"pointnav_base"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#pluginsrobothor_pluginconfigspointnav_base","text":"[view_source]","title":"plugins.robothor_plugin.configs.pointnav_base"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#pointnavbaseconfig","text":"class PointNavBaseConfig ( ExperimentConfig , abc . ABC ) [view_source] A Point Navigation base configuration.","title":"PointNavBaseConfig"},{"location":"api/plugins/robothor_plugin/configs/resnet18_nav_base/","text":"plugins.robothor_plugin.configs.resnet18_nav_base # [view_source] Resnet18NavBaseConfig # class Resnet18NavBaseConfig ( NavBaseConfig , abc . ABC ) [view_source] A Navigation base configuration in RoboTHOR.","title":"resnet18_nav_base"},{"location":"api/plugins/robothor_plugin/configs/resnet18_nav_base/#pluginsrobothor_pluginconfigsresnet18_nav_base","text":"[view_source]","title":"plugins.robothor_plugin.configs.resnet18_nav_base"},{"location":"api/plugins/robothor_plugin/configs/resnet18_nav_base/#resnet18navbaseconfig","text":"class Resnet18NavBaseConfig ( NavBaseConfig , abc . ABC ) [view_source] A Navigation base configuration in RoboTHOR.","title":"Resnet18NavBaseConfig"},{"location":"api/plugins/robothor_plugin/configs/resnet18_objectnav/","text":"plugins.robothor_plugin.configs.resnet18_objectnav # [view_source] Resnet18ObjectNavExperimentConfig # class Resnet18ObjectNavExperimentConfig ( ObjectNavBaseConfig , Resnet18NavBaseConfig ) [view_source] An Object Navigation experiment configuration.","title":"resnet18_objectnav"},{"location":"api/plugins/robothor_plugin/configs/resnet18_objectnav/#pluginsrobothor_pluginconfigsresnet18_objectnav","text":"[view_source]","title":"plugins.robothor_plugin.configs.resnet18_objectnav"},{"location":"api/plugins/robothor_plugin/configs/resnet18_objectnav/#resnet18objectnavexperimentconfig","text":"class Resnet18ObjectNavExperimentConfig ( ObjectNavBaseConfig , Resnet18NavBaseConfig ) [view_source] An Object Navigation experiment configuration.","title":"Resnet18ObjectNavExperimentConfig"},{"location":"api/plugins/robothor_plugin/configs/resnet18_pointnav/","text":"plugins.robothor_plugin.configs.resnet18_pointnav # [view_source] Resnet18PointNavExperimentConfig # class Resnet18PointNavExperimentConfig ( PointNavBaseConfig , Resnet18NavBaseConfig ) [view_source] A Point Navigation experiment configuration.","title":"resnet18_pointnav"},{"location":"api/plugins/robothor_plugin/configs/resnet18_pointnav/#pluginsrobothor_pluginconfigsresnet18_pointnav","text":"[view_source]","title":"plugins.robothor_plugin.configs.resnet18_pointnav"},{"location":"api/plugins/robothor_plugin/configs/resnet18_pointnav/#resnet18pointnavexperimentconfig","text":"class Resnet18PointNavExperimentConfig ( PointNavBaseConfig , Resnet18NavBaseConfig ) [view_source] A Point Navigation experiment configuration.","title":"Resnet18PointNavExperimentConfig"},{"location":"api/plugins/robothor_plugin/configs/simple_objectnav/","text":"plugins.robothor_plugin.configs.simple_objectnav # [view_source] SimpleObjectNavExperimentConfig # class SimpleObjectNavExperimentConfig ( ObjectNavBaseConfig , NavBaseConfig ) [view_source] A Point Navigation experiment configuration.","title":"simple_objectnav"},{"location":"api/plugins/robothor_plugin/configs/simple_objectnav/#pluginsrobothor_pluginconfigssimple_objectnav","text":"[view_source]","title":"plugins.robothor_plugin.configs.simple_objectnav"},{"location":"api/plugins/robothor_plugin/configs/simple_objectnav/#simpleobjectnavexperimentconfig","text":"class SimpleObjectNavExperimentConfig ( ObjectNavBaseConfig , NavBaseConfig ) [view_source] A Point Navigation experiment configuration.","title":"SimpleObjectNavExperimentConfig"},{"location":"api/plugins/robothor_plugin/configs/simple_pointnav/","text":"plugins.robothor_plugin.configs.simple_pointnav # [view_source] SimplePointNavExperimentConfig # class SimplePointNavExperimentConfig ( PointNavBaseConfig , NavBaseConfig ) [view_source] A Point Navigation experiment configuration.","title":"simple_pointnav"},{"location":"api/plugins/robothor_plugin/configs/simple_pointnav/#pluginsrobothor_pluginconfigssimple_pointnav","text":"[view_source]","title":"plugins.robothor_plugin.configs.simple_pointnav"},{"location":"api/plugins/robothor_plugin/configs/simple_pointnav/#simplepointnavexperimentconfig","text":"class SimplePointNavExperimentConfig ( PointNavBaseConfig , NavBaseConfig ) [view_source] A Point Navigation experiment configuration.","title":"SimplePointNavExperimentConfig"},{"location":"api/plugins/robothor_plugin/scripts/make_objectnav_debug_dataset/","text":"plugins.robothor_plugin.scripts.make_objectnav_debug_dataset # [view_source]","title":"make_objectnav_debug_dataset"},{"location":"api/plugins/robothor_plugin/scripts/make_objectnav_debug_dataset/#pluginsrobothor_pluginscriptsmake_objectnav_debug_dataset","text":"[view_source]","title":"plugins.robothor_plugin.scripts.make_objectnav_debug_dataset"},{"location":"api/projects/babyai_baselines/experiments/base/","text":"projects.babyai_baselines.experiments.base # [view_source] BaseBabyAIExperimentConfig # class BaseBabyAIExperimentConfig ( ExperimentConfig , ABC ) [view_source] Base experimental config.","title":"base"},{"location":"api/projects/babyai_baselines/experiments/base/#projectsbabyai_baselinesexperimentsbase","text":"[view_source]","title":"projects.babyai_baselines.experiments.base"},{"location":"api/projects/babyai_baselines/experiments/base/#basebabyaiexperimentconfig","text":"class BaseBabyAIExperimentConfig ( ExperimentConfig , ABC ) [view_source] Base experimental config.","title":"BaseBabyAIExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/","text":"projects.babyai_baselines.experiments.go_to_local.a2c # [view_source] A2CBabyAIGoToLocalExperimentConfig # class A2CBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] A2C only.","title":"a2c"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/#projectsbabyai_baselinesexperimentsgo_to_locala2c","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.a2c"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/#a2cbabyaigotolocalexperimentconfig","text":"class A2CBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] A2C only.","title":"A2CBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/","text":"projects.babyai_baselines.experiments.go_to_local.base # [view_source] BaseBabyAIGoToLocalExperimentConfig # class BaseBabyAIGoToLocalExperimentConfig ( BaseBabyAIExperimentConfig , ABC ) [view_source] Base experimental config.","title":"base"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#projectsbabyai_baselinesexperimentsgo_to_localbase","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.base"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#basebabyaigotolocalexperimentconfig","text":"class BaseBabyAIGoToLocalExperimentConfig ( BaseBabyAIExperimentConfig , ABC ) [view_source] Base experimental config.","title":"BaseBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc/","text":"projects.babyai_baselines.experiments.go_to_local.bc # [view_source] PPOBabyAIGoToLocalExperimentConfig # class PPOBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Behavior clone then PPO.","title":"bc"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc/#projectsbabyai_baselinesexperimentsgo_to_localbc","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.bc"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc/#ppobabyaigotolocalexperimentconfig","text":"class PPOBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Behavior clone then PPO.","title":"PPOBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_offpolicy/","text":"projects.babyai_baselines.experiments.go_to_local.bc_offpolicy # [view_source] BCOffPolicyBabyAIGoToLocalExperimentConfig # class BCOffPolicyBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] BC Off policy imitation.","title":"Bc offpolicy"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_offpolicy/#projectsbabyai_baselinesexperimentsgo_to_localbc_offpolicy","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.bc_offpolicy"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_offpolicy/#bcoffpolicybabyaigotolocalexperimentconfig","text":"class BCOffPolicyBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] BC Off policy imitation.","title":"BCOffPolicyBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_teacher_forcing/","text":"projects.babyai_baselines.experiments.go_to_local.bc_teacher_forcing # [view_source] BCTeacherForcingBabyAIGoToLocalExperimentConfig # class BCTeacherForcingBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Behavior clone with teacher forcing.","title":"bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_teacher_forcing/#projectsbabyai_baselinesexperimentsgo_to_localbc_teacher_forcing","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_teacher_forcing/#bcteacherforcingbabyaigotolocalexperimentconfig","text":"class BCTeacherForcingBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Behavior clone with teacher forcing.","title":"BCTeacherForcingBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/dagger/","text":"projects.babyai_baselines.experiments.go_to_local.dagger # [view_source] DaggerBabyAIGoToLocalExperimentConfig # class DaggerBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Find goal in lighthouse env using imitation learning. Training with Dagger.","title":"dagger"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/dagger/#projectsbabyai_baselinesexperimentsgo_to_localdagger","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.dagger"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/dagger/#daggerbabyaigotolocalexperimentconfig","text":"class DaggerBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] Find goal in lighthouse env using imitation learning. Training with Dagger.","title":"DaggerBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_offpolicy/","text":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_offpolicy # [view_source] DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig # class DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig ( BCOffPolicyBabyAIGoToLocalExperimentConfig ) [view_source] Distributed Off policy imitation.","title":"distributed_bc_offpolicy"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_offpolicy/#projectsbabyai_baselinesexperimentsgo_to_localdistributed_bc_offpolicy","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_offpolicy"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_offpolicy/#distributedbcoffpolicybabyaigotolocalexperimentconfig","text":"class DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig ( BCOffPolicyBabyAIGoToLocalExperimentConfig ) [view_source] Distributed Off policy imitation.","title":"DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_teacher_forcing/","text":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_teacher_forcing # [view_source] DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig # class DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig ( BCTeacherForcingBabyAIGoToLocalExperimentConfig ) [view_source] Distributed behavior clone with teacher forcing.","title":"distributed_bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_teacher_forcing/#projectsbabyai_baselinesexperimentsgo_to_localdistributed_bc_teacher_forcing","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_teacher_forcing/#distributedbcteacherforcingbabyaigotolocalexperimentconfig","text":"class DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig ( BCTeacherForcingBabyAIGoToLocalExperimentConfig ) [view_source] Distributed behavior clone with teacher forcing.","title":"DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/","text":"projects.babyai_baselines.experiments.go_to_local.ppo # [view_source] PPOBabyAIGoToLocalExperimentConfig # class PPOBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] PPO only.","title":"ppo"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/#projectsbabyai_baselinesexperimentsgo_to_localppo","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_local.ppo"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/#ppobabyaigotolocalexperimentconfig","text":"class PPOBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] PPO only.","title":"PPOBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/a2c/","text":"projects.babyai_baselines.experiments.go_to_obj.a2c # [view_source] A2CBabyAIGoToObjExperimentConfig # class A2CBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] A2C only.","title":"a2c"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/a2c/#projectsbabyai_baselinesexperimentsgo_to_obja2c","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.a2c"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/a2c/#a2cbabyaigotoobjexperimentconfig","text":"class A2CBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] A2C only.","title":"A2CBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/","text":"projects.babyai_baselines.experiments.go_to_obj.base # [view_source] BaseBabyAIGoToObjExperimentConfig # class BaseBabyAIGoToObjExperimentConfig ( BaseBabyAIExperimentConfig , ABC ) [view_source] Base experimental config.","title":"base"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#projectsbabyai_baselinesexperimentsgo_to_objbase","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.base"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#basebabyaigotoobjexperimentconfig","text":"class BaseBabyAIGoToObjExperimentConfig ( BaseBabyAIExperimentConfig , ABC ) [view_source] Base experimental config.","title":"BaseBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc/","text":"projects.babyai_baselines.experiments.go_to_obj.bc # [view_source] PPOBabyAIGoToObjExperimentConfig # class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Behavior clone then PPO.","title":"bc"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc/#projectsbabyai_baselinesexperimentsgo_to_objbc","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.bc"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc/#ppobabyaigotoobjexperimentconfig","text":"class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Behavior clone then PPO.","title":"PPOBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc_teacher_forcing/","text":"projects.babyai_baselines.experiments.go_to_obj.bc_teacher_forcing # [view_source] PPOBabyAIGoToObjExperimentConfig # class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Behavior clone (with teacher forcing) then PPO.","title":"bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc_teacher_forcing/#projectsbabyai_baselinesexperimentsgo_to_objbc_teacher_forcing","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc_teacher_forcing/#ppobabyaigotoobjexperimentconfig","text":"class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Behavior clone (with teacher forcing) then PPO.","title":"PPOBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/dagger/","text":"projects.babyai_baselines.experiments.go_to_obj.dagger # [view_source] DaggerBabyAIGoToObjExperimentConfig # class DaggerBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Find goal in lighthouse env using imitation learning. Training with Dagger.","title":"dagger"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/dagger/#projectsbabyai_baselinesexperimentsgo_to_objdagger","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.dagger"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/dagger/#daggerbabyaigotoobjexperimentconfig","text":"class DaggerBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] Find goal in lighthouse env using imitation learning. Training with Dagger.","title":"DaggerBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/ppo/","text":"projects.babyai_baselines.experiments.go_to_obj.ppo # [view_source] PPOBabyAIGoToObjExperimentConfig # class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] PPO only.","title":"ppo"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/ppo/#projectsbabyai_baselinesexperimentsgo_to_objppo","text":"[view_source]","title":"projects.babyai_baselines.experiments.go_to_obj.ppo"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/ppo/#ppobabyaigotoobjexperimentconfig","text":"class PPOBabyAIGoToObjExperimentConfig ( BaseBabyAIGoToObjExperimentConfig ) [view_source] PPO only.","title":"PPOBabyAIGoToObjExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_base/","text":"projects.objectnav_baselines.experiments.objectnav_base # [view_source] ObjectNavBaseConfig # class ObjectNavBaseConfig ( ExperimentConfig , ABC ) [view_source] The base object navigation configuration file.","title":"objectnav_base"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_base/#projectsobjectnav_baselinesexperimentsobjectnav_base","text":"[view_source]","title":"projects.objectnav_baselines.experiments.objectnav_base"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_base/#objectnavbaseconfig","text":"class ObjectNavBaseConfig ( ExperimentConfig , ABC ) [view_source] The base object navigation configuration file.","title":"ObjectNavBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_base/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_base # [view_source] ObjectNaviThorBaseConfig # class ObjectNaviThorBaseConfig ( ObjectNavBaseConfig , ABC ) [view_source] The base config for all iTHOR ObjectNav experiments.","title":"objectnav_ithor_base"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_base/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_base","text":"[view_source]","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_base"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_base/#objectnavithorbaseconfig","text":"class ObjectNaviThorBaseConfig ( ObjectNavBaseConfig , ABC ) [view_source] The base config for all iTHOR ObjectNav experiments.","title":"ObjectNaviThorBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_depth_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_depth_resnetgru_ddppo # [view_source] ObjectNavRoboThorRGBPPOExperimentConfig # class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig ) [view_source] An Object Navigation experiment configuration in iThor with Depth input.","title":"objectnav_ithor_depth_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_depth_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_depth_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_depth_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_depth_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig ) [view_source] An Object Navigation experiment configuration in iThor with Depth input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgb_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgb_resnetgru_ddppo # [view_source] ObjectNaviThorRGBPPOExperimentConfig # class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig ) [view_source] An Object Navigation experiment configuration in iThor with RGB input.","title":"objectnav_ithor_rgb_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgb_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_rgb_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgb_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgb_resnetgru_ddppo/#objectnavithorrgbppoexperimentconfig","text":"class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig ) [view_source] An Object Navigation experiment configuration in iThor with RGB input.","title":"ObjectNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgbd_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgbd_resnetgru_ddppo # [view_source] ObjectNavRoboThorRGBPPOExperimentConfig # class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig ) [view_source] An Object Navigation experiment configuration in iThor with RGBD input.","title":"objectnav_ithor_rgbd_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgbd_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_rgbd_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgbd_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgbd_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNaviThorBaseConfig ) [view_source] An Object Navigation experiment configuration in iThor with RGBD input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_base # [view_source] ObjectNavRoboThorBaseConfig # class ObjectNavRoboThorBaseConfig ( ObjectNavBaseConfig , ABC ) [view_source] The base config for all RoboTHOR ObjectNav experiments.","title":"objectnav_robothor_base"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_base","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_base"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base/#objectnavrobothorbaseconfig","text":"class ObjectNavRoboThorBaseConfig ( ObjectNavBaseConfig , ABC ) [view_source] The base config for all RoboTHOR ObjectNav experiments.","title":"ObjectNavRoboThorBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_depth_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_depth_resnetgru_ddppo # [view_source] ObjectNavRoboThorRGBPPOExperimentConfig # class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with Depth input.","title":"objectnav_robothor_depth_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_depth_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_depth_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_depth_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_depth_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with Depth input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_advisor/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_advisor # [view_source] ObjectNaviThorRGBAdvisorExperimentConfig # class ObjectNaviThorRGBAdvisorExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input with ADVISOR.","title":"Objectnav robothor rgb resnetgru advisor"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_advisor/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_advisor","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_advisor"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_advisor/#objectnavithorrgbadvisorexperimentconfig","text":"class ObjectNaviThorRGBAdvisorExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input with ADVISOR.","title":"ObjectNaviThorRGBAdvisorExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_advisor_random_sampler/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_advisor_random_sampler # [view_source] ObjectNaviThorRGBAdvisorExperimentConfig # class ObjectNaviThorRGBAdvisorExperimentConfig ( ObjectNavBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input with ADVISOR.","title":"Objectnav robothor rgb resnetgru advisor random sampler"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_advisor_random_sampler/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_advisor_random_sampler","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_advisor_random_sampler"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_advisor_random_sampler/#objectnavithorrgbadvisorexperimentconfig","text":"class ObjectNaviThorRGBAdvisorExperimentConfig ( ObjectNavBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input with ADVISOR.","title":"ObjectNaviThorRGBAdvisorExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_dagger/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_dagger # [view_source] ObjectNaviThorRGBDAggerExperimentConfig # class ObjectNaviThorRGBDAggerExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"Objectnav robothor rgb resnetgru dagger"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_dagger/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_dagger","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_dagger"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_dagger/#objectnavithorrgbdaggerexperimentconfig","text":"class ObjectNaviThorRGBDAggerExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"ObjectNaviThorRGBDAggerExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo # [view_source] ObjectNaviThorRGBPPOExperimentConfig # class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"objectnav_robothor_rgb_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo/#objectnavithorrgbppoexperimentconfig","text":"class ObjectNaviThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"ObjectNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_bc/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo_and_bc # [view_source] ObjectNaviThorRGBPPOAndSimpleBCExperimentConfig # class ObjectNaviThorRGBPPOAndSimpleBCExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"Objectnav robothor rgb resnetgru ddppo and bc"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_bc/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_ddppo_and_bc","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo_and_bc"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_bc/#objectnavithorrgbppoandsimplebcexperimentconfig","text":"class ObjectNaviThorRGBPPOAndSimpleBCExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"ObjectNaviThorRGBPPOAndSimpleBCExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_bc_random_sampler/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo_and_bc_random_sampler # [view_source] ObjectNavRoboThorBaseConfig # class ObjectNavRoboThorBaseConfig ( ObjectNavBaseConfig ) [view_source] The base config for all RoboTHOR ObjectNav experiments.","title":"Objectnav robothor rgb resnetgru ddppo and bc random sampler"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_bc_random_sampler/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_ddppo_and_bc_random_sampler","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo_and_bc_random_sampler"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_bc_random_sampler/#objectnavrobothorbaseconfig","text":"class ObjectNavRoboThorBaseConfig ( ObjectNavBaseConfig ) [view_source] The base config for all RoboTHOR ObjectNav experiments.","title":"ObjectNavRoboThorBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_simplebc/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo_and_simplebc # [view_source] ObjectNavThorRGBPPOAndSimpleBCExperimentConfig # class ObjectNavThorRGBPPOAndSimpleBCExperimentConfig ( ObjectNaviThorRGBPPOExperimentConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"Objectnav robothor rgb resnetgru ddppo and simplebc"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_simplebc/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_ddppo_and_simplebc","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo_and_simplebc"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_and_simplebc/#objectnavthorrgbppoandsimplebcexperimentconfig","text":"class ObjectNavThorRGBPPOAndSimpleBCExperimentConfig ( ObjectNaviThorRGBPPOExperimentConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGB input.","title":"ObjectNavThorRGBPPOAndSimpleBCExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_random_sampler/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo_random_sampler # [view_source] ObjectNavRoboThorBaseConfig # class ObjectNavRoboThorBaseConfig ( ObjectNavBaseConfig ) [view_source] The base config for all RoboTHOR ObjectNav experiments.","title":"Objectnav robothor rgb resnetgru ddppo random sampler"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_random_sampler/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_ddppo_random_sampler","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo_random_sampler"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo_random_sampler/#objectnavrobothorbaseconfig","text":"class ObjectNavRoboThorBaseConfig ( ObjectNavBaseConfig ) [view_source] The base config for all RoboTHOR ObjectNav experiments.","title":"ObjectNavRoboThorBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgbd_resnetgru_ddppo # [view_source] ObjectNavRoboThorRGBPPOExperimentConfig # class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGBD input.","title":"objectnav_robothor_rgbd_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgbd_resnetgru_ddppo","text":"[view_source]","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgbd_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"class ObjectNavRoboThorRGBPPOExperimentConfig ( ObjectNavRoboThorBaseConfig ) [view_source] An Object Navigation experiment configuration in RoboThor with RGBD input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/","text":"projects.objectnav_baselines.models.object_nav_models # [view_source] Baseline models for use in the object navigation task. Object navigation is currently available as a Task in AI2-THOR and Facebook's Habitat. ObjectNavBaselineActorCritic # class ObjectNavBaselineActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] Baseline recurrent actor critic model for object-navigation. Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type gym.spaces.Discrete ). observation_space : The observation space expected by the agent. This observation space should include (optionally) 'rgb' images and 'depth' images and is required to have a component corresponding to the goal goal_sensor_uuid . goal_sensor_uuid : The uuid of the sensor of the goal object. See GoalObjectTypeThorSensor as an example of such a sensor. hidden_size : The hidden size of the GRU RNN. object_type_embedding_dim : The dimensionality of the embedding corresponding to the goal object type. ObjectNavBaselineActorCritic.__init__ # | __init__ ( action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , trainable_masked_hidden_state : bool = False , num_rnn_layers = 1 , rnn_type = \"GRU\" ) [view_source] Initializer. See class documentation for parameter definitions. ObjectNavBaselineActorCritic.recurrent_hidden_state_size # | @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model. ObjectNavBaselineActorCritic.is_blind # | @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). ObjectNavBaselineActorCritic.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers. ObjectNavBaselineActorCritic.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ObjectNavBaselineActorCritic.forward # | forward ( observations : ObservationType , memory : Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]] [view_source] Processes input batched observations to produce new actor and critic values. Processes input batched observations (along with prior hidden states, previous actions, and masks denoting which recurrent hidden states should be masked) and returns an ActorCriticOutput object containing the model's policy (distribution over actions) and evaluation of the current state (value). Parameters observations : Batched input observations. rnn_hidden_states : Hidden states from initial timepoints. prev_actions : Tensor of previous actions taken. masks : Masks applied to hidden states. See RNNStateEncoder . Returns Tuple of the ActorCriticOutput and recurrent hidden state. ResnetTensorObjectNavActorCritic # class ResnetTensorObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] ResnetTensorObjectNavActorCritic.recurrent_hidden_state_size # | @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model. ResnetTensorObjectNavActorCritic.is_blind # | @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). ResnetTensorObjectNavActorCritic.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers. ResnetTensorObjectNavActorCritic.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetTensorGoalEncoder # class ResnetTensorGoalEncoder ( nn . Module ) [view_source] ResnetTensorGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetDualTensorGoalEncoder # class ResnetDualTensorGoalEncoder ( nn . Module ) [view_source] ResnetDualTensorGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"object_nav_models"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#projectsobjectnav_baselinesmodelsobject_nav_models","text":"[view_source] Baseline models for use in the object navigation task. Object navigation is currently available as a Task in AI2-THOR and Facebook's Habitat.","title":"projects.objectnav_baselines.models.object_nav_models"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcritic","text":"class ObjectNavBaselineActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] Baseline recurrent actor critic model for object-navigation. Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type gym.spaces.Discrete ). observation_space : The observation space expected by the agent. This observation space should include (optionally) 'rgb' images and 'depth' images and is required to have a component corresponding to the goal goal_sensor_uuid . goal_sensor_uuid : The uuid of the sensor of the goal object. See GoalObjectTypeThorSensor as an example of such a sensor. hidden_size : The hidden size of the GRU RNN. object_type_embedding_dim : The dimensionality of the embedding corresponding to the goal object type.","title":"ObjectNavBaselineActorCritic"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcritic__init__","text":"| __init__ ( action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , trainable_masked_hidden_state : bool = False , num_rnn_layers = 1 , rnn_type = \"GRU\" ) [view_source] Initializer. See class documentation for parameter definitions.","title":"ObjectNavBaselineActorCritic.__init__"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcriticrecurrent_hidden_state_size","text":"| @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model.","title":"ObjectNavBaselineActorCritic.recurrent_hidden_state_size"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcriticis_blind","text":"| @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"ObjectNavBaselineActorCritic.is_blind"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcriticnum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers.","title":"ObjectNavBaselineActorCritic.num_recurrent_layers"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcriticget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ObjectNavBaselineActorCritic.get_object_type_encoding"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcriticforward","text":"| forward ( observations : ObservationType , memory : Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]] [view_source] Processes input batched observations to produce new actor and critic values. Processes input batched observations (along with prior hidden states, previous actions, and masks denoting which recurrent hidden states should be masked) and returns an ActorCriticOutput object containing the model's policy (distribution over actions) and evaluation of the current state (value). Parameters observations : Batched input observations. rnn_hidden_states : Hidden states from initial timepoints. prev_actions : Tensor of previous actions taken. masks : Masks applied to hidden states. See RNNStateEncoder . Returns Tuple of the ActorCriticOutput and recurrent hidden state.","title":"ObjectNavBaselineActorCritic.forward"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcritic","text":"class ResnetTensorObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source]","title":"ResnetTensorObjectNavActorCritic"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcriticrecurrent_hidden_state_size","text":"| @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model.","title":"ResnetTensorObjectNavActorCritic.recurrent_hidden_state_size"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcriticis_blind","text":"| @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"ResnetTensorObjectNavActorCritic.is_blind"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcriticnum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers.","title":"ResnetTensorObjectNavActorCritic.num_recurrent_layers"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcriticget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetTensorObjectNavActorCritic.get_object_type_encoding"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorgoalencoder","text":"class ResnetTensorGoalEncoder ( nn . Module ) [view_source]","title":"ResnetTensorGoalEncoder"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetTensorGoalEncoder.get_object_type_encoding"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnetdualtensorgoalencoder","text":"class ResnetDualTensorGoalEncoder ( nn . Module ) [view_source]","title":"ResnetDualTensorGoalEncoder"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnetdualtensorgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetDualTensorGoalEncoder.get_object_type_encoding"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_base/","text":"projects.pointnav_baselines.experiments.pointnav_base # [view_source] PointNavBaseConfig # class PointNavBaseConfig ( ExperimentConfig , ABC ) [view_source] An Object Navigation experiment configuration in iThor.","title":"pointnav_base"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_base/#projectspointnav_baselinesexperimentspointnav_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.pointnav_base"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_base/#pointnavbaseconfig","text":"class PointNavBaseConfig ( ExperimentConfig , ABC ) [view_source] An Object Navigation experiment configuration in iThor.","title":"PointNavBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_base/","text":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_base # [view_source] DebugPointNavHabitatBaseConfig # class DebugPointNavHabitatBaseConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all Habitat PointNav experiments.","title":"debug_pointnav_habitat_base"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_base/#projectspointnav_baselinesexperimentshabitatdebug_pointnav_habitat_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_base"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_base/#debugpointnavhabitatbaseconfig","text":"class DebugPointNavHabitatBaseConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all Habitat PointNav experiments.","title":"DebugPointNavHabitatBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_bc/","text":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgb_simpleconvgru_bc # [view_source] PointNavHabitatRGBDeterministiSimpleConvGRUImitationExperimentConfig # class PointNavHabitatRGBDeterministiSimpleConvGRUImitationExperimentConfig ( DebugPointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"debug_pointnav_habitat_rgb_simpleconvgru_bc"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_bc/#projectspointnav_baselinesexperimentshabitatdebug_pointnav_habitat_rgb_simpleconvgru_bc","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgb_simpleconvgru_bc"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_bc/#pointnavhabitatrgbdeterministisimpleconvgruimitationexperimentconfig","text":"class PointNavHabitatRGBDeterministiSimpleConvGRUImitationExperimentConfig ( DebugPointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"PointNavHabitatRGBDeterministiSimpleConvGRUImitationExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgb_simpleconvgru_ddppo # [view_source] PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig # class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( DebugPointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"debug_pointnav_habitat_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentshabitatdebug_pointnav_habitat_rgb_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgb_simpleconvgru_ddppo/#pointnavhabitatdepthdeterministisimpleconvgruddppoexperimentconfig","text":"class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( DebugPointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgbd_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgbd_simpleconvgru_ddppo # [view_source] PointNavHabitatRGBDDeterministiSimpleConvGRUDDPPOExperimentConfig # class PointNavHabitatRGBDDeterministiSimpleConvGRUDDPPOExperimentConfig ( DebugPointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"debug_pointnav_habitat_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgbd_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentshabitatdebug_pointnav_habitat_rgbd_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.debug_pointnav_habitat_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/debug_pointnav_habitat_rgbd_simpleconvgru_ddppo/#pointnavhabitatrgbddeterministisimpleconvgruddppoexperimentconfig","text":"class PointNavHabitatRGBDDeterministiSimpleConvGRUDDPPOExperimentConfig ( DebugPointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"PointNavHabitatRGBDDeterministiSimpleConvGRUDDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_base/","text":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_base # [view_source] PointNavHabitatBaseConfig # class PointNavHabitatBaseConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all Habitat PointNav experiments.","title":"pointnav_habitat_base"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_base/#projectspointnav_baselinesexperimentshabitatpointnav_habitat_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_base"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_base/#pointnavhabitatbaseconfig","text":"class PointNavHabitatBaseConfig ( PointNavBaseConfig , ABC ) [view_source] The base config for all Habitat PointNav experiments.","title":"PointNavHabitatBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_depth_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_depth_simpleconvgru_ddppo # [view_source] PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig # class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"pointnav_habitat_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_depth_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentshabitatpointnav_habitat_depth_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_depth_simpleconvgru_ddppo/#pointnavhabitatdepthdeterministisimpleconvgruddppoexperimentconfig","text":"class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgb_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_rgb_simpleconvgru_ddppo # [view_source] PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig # class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"pointnav_habitat_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgb_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentshabitatpointnav_habitat_rgb_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgb_simpleconvgru_ddppo/#pointnavhabitatdepthdeterministisimpleconvgruddppoexperimentconfig","text":"class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with Depth input.","title":"PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgbd_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_rgbd_simpleconvgru_ddppo # [view_source] PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig # class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with RGBD input.","title":"pointnav_habitat_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgbd_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentshabitatpointnav_habitat_rgbd_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.habitat.pointnav_habitat_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/habitat/pointnav_habitat_rgbd_simpleconvgru_ddppo/#pointnavhabitatdepthdeterministisimpleconvgruddppoexperimentconfig","text":"class PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig ( PointNavHabitatBaseConfig ) [view_source] An Point Navigation experiment configuration in Habitat with RGBD input.","title":"PointNavHabitatDepthDeterministiSimpleConvGRUDDPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_base/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_base # [view_source] PointNaviThorBaseConfig # class PointNaviThorBaseConfig ( ObjectNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"pointnav_ithor_base"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_base/#projectspointnav_baselinesexperimentsithorpointnav_ithor_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_base"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_base/#pointnavithorbaseconfig","text":"class PointNaviThorBaseConfig ( ObjectNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"PointNaviThorBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_depth_simpleconvgru_ddppo # [view_source] PointNaviThorRGBPPOExperimentConfig # class PointNaviThorRGBPPOExperimentConfig ( PointNaviThorBaseConfig ) [view_source] An Point Navigation experiment configuration in iThor with Depth input.","title":"pointnav_ithor_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsithorpointnav_ithor_depth_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo/#pointnavithorrgbppoexperimentconfig","text":"class PointNaviThorRGBPPOExperimentConfig ( PointNaviThorBaseConfig ) [view_source] An Point Navigation experiment configuration in iThor with Depth input.","title":"PointNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgb_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgb_simpleconvgru_ddppo # [view_source] PointNaviThorRGBPPOExperimentConfig # class PointNaviThorRGBPPOExperimentConfig ( PointNaviThorBaseConfig ) [view_source] An Point Navigation experiment configuration in iThor with RGB input.","title":"pointnav_ithor_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgb_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsithorpointnav_ithor_rgb_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgb_simpleconvgru_ddppo/#pointnavithorrgbppoexperimentconfig","text":"class PointNaviThorRGBPPOExperimentConfig ( PointNaviThorBaseConfig ) [view_source] An Point Navigation experiment configuration in iThor with RGB input.","title":"PointNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgbd_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgbd_simpleconvgru_ddppo # [view_source] PointNaviThorRGBPPOExperimentConfig # class PointNaviThorRGBPPOExperimentConfig ( PointNaviThorBaseConfig ) [view_source] An Point Navigation experiment configuration in iThor with RGBD input.","title":"pointnav_ithor_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgbd_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsithorpointnav_ithor_rgbd_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgbd_simpleconvgru_ddppo/#pointnavithorrgbppoexperimentconfig","text":"class PointNaviThorRGBPPOExperimentConfig ( PointNaviThorBaseConfig ) [view_source] An Point Navigation experiment configuration in iThor with RGBD input.","title":"PointNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_base/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_base # [view_source] PointNavRoboThorBaseConfig # class PointNavRoboThorBaseConfig ( ObjectNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"pointnav_robothor_base"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_base/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_base","text":"[view_source]","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_base"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_base/#pointnavrobothorbaseconfig","text":"class PointNavRoboThorBaseConfig ( ObjectNavBaseConfig , ABC ) [view_source] The base config for all iTHOR PointNav experiments.","title":"PointNavRoboThorBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_depth_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_depth_simpleconvgru_ddppo # [view_source] PointNavRoboThorRGBPPOExperimentConfig # class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig ) [view_source] An Point Navigation experiment configuration in RoboTHOR with Depth input.","title":"pointnav_robothor_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_depth_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_depth_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_depth_simpleconvgru_ddppo/#pointnavrobothorrgbppoexperimentconfig","text":"class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig ) [view_source] An Point Navigation experiment configuration in RoboTHOR with Depth input.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgb_simpleconvgru_ddppo # [view_source] PointNavRoboThorRGBPPOExperimentConfig # class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig ) [view_source] An Point Navigation experiment configuration in RoboThor with RGB input.","title":"pointnav_robothor_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_rgb_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo/#pointnavrobothorrgbppoexperimentconfig","text":"class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig ) [view_source] An Point Navigation experiment configuration in RoboThor with RGB input.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgbd_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgbd_simpleconvgru_ddppo # [view_source] PointNavRoboThorRGBPPOExperimentConfig # class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig ) [view_source] An Point Navigation experiment configuration in RoboThor with RGBD input.","title":"pointnav_robothor_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgbd_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_rgbd_simpleconvgru_ddppo","text":"[view_source]","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgbd_simpleconvgru_ddppo/#pointnavrobothorrgbppoexperimentconfig","text":"class PointNavRoboThorRGBPPOExperimentConfig ( PointNavRoboThorBaseConfig ) [view_source] An Point Navigation experiment configuration in RoboThor with RGBD input.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/","text":"projects.pointnav_baselines.models.point_nav_models # [view_source] ResnetTensorPointNavActorCritic # class ResnetTensorPointNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source] ResnetTensorPointNavActorCritic.recurrent_hidden_state_size # | @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model. ResnetTensorPointNavActorCritic.is_blind # | @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). ResnetTensorPointNavActorCritic.num_recurrent_layers # | @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers. ResnetTensorGoalEncoder # class ResnetTensorGoalEncoder ( nn . Module ) [view_source] ResnetTensorGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations. ResnetDualTensorGoalEncoder # class ResnetDualTensorGoalEncoder ( nn . Module ) [view_source] ResnetDualTensorGoalEncoder.get_object_type_encoding # | get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"point_nav_models"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#projectspointnav_baselinesmodelspoint_nav_models","text":"[view_source]","title":"projects.pointnav_baselines.models.point_nav_models"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorpointnavactorcritic","text":"class ResnetTensorPointNavActorCritic ( ActorCriticModel [ CategoricalDistr ]) [view_source]","title":"ResnetTensorPointNavActorCritic"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorpointnavactorcriticrecurrent_hidden_state_size","text":"| @property | recurrent_hidden_state_size () -> int [view_source] The recurrent hidden state size of the model.","title":"ResnetTensorPointNavActorCritic.recurrent_hidden_state_size"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorpointnavactorcriticis_blind","text":"| @property | is_blind () -> bool [view_source] True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"ResnetTensorPointNavActorCritic.is_blind"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorpointnavactorcriticnum_recurrent_layers","text":"| @property | num_recurrent_layers () -> int [view_source] Number of recurrent hidden layers.","title":"ResnetTensorPointNavActorCritic.num_recurrent_layers"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorgoalencoder","text":"class ResnetTensorGoalEncoder ( nn . Module ) [view_source]","title":"ResnetTensorGoalEncoder"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetTensorGoalEncoder.get_object_type_encoding"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnetdualtensorgoalencoder","text":"class ResnetDualTensorGoalEncoder ( nn . Module ) [view_source]","title":"ResnetDualTensorGoalEncoder"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnetdualtensorgoalencoderget_object_type_encoding","text":"| get_object_type_encoding ( observations : Dict [ str , torch . FloatTensor ]) -> torch . FloatTensor [view_source] Get the object type encoding from input batched observations.","title":"ResnetDualTensorGoalEncoder.get_object_type_encoding"},{"location":"api/projects/tutorials/babyai_go_to_local_bc_offpolicy/","text":"projects.tutorials.babyai_go_to_local_bc_offpolicy # [view_source] BCOffPolicyBabyAIGoToLocalExperimentConfig # class BCOffPolicyBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] BC Off policy imitation.","title":"babyai_go_to_local_bc_offpolicy"},{"location":"api/projects/tutorials/babyai_go_to_local_bc_offpolicy/#projectstutorialsbabyai_go_to_local_bc_offpolicy","text":"[view_source]","title":"projects.tutorials.babyai_go_to_local_bc_offpolicy"},{"location":"api/projects/tutorials/babyai_go_to_local_bc_offpolicy/#bcoffpolicybabyaigotolocalexperimentconfig","text":"class BCOffPolicyBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ) [view_source] BC Off policy imitation.","title":"BCOffPolicyBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/tutorials/minigrid_tutorial/","text":"projects.tutorials.minigrid_tutorial # [view_source] Experiment Config for MiniGrid tutorial.","title":"minigrid_tutorial"},{"location":"api/projects/tutorials/minigrid_tutorial/#projectstutorialsminigrid_tutorial","text":"[view_source] Experiment Config for MiniGrid tutorial.","title":"projects.tutorials.minigrid_tutorial"},{"location":"api/projects/tutorials/object_nav_ithor_dagger_then_ppo_one_object/","text":"projects.tutorials.object_nav_ithor_dagger_then_ppo_one_object # [view_source] ObjectNavThorDaggerThenPPOExperimentConfig # class ObjectNavThorDaggerThenPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ) [view_source] A simple object navigation experiment in THOR. Training with DAgger and then PPO.","title":"object_nav_ithor_dagger_then_ppo_one_object"},{"location":"api/projects/tutorials/object_nav_ithor_dagger_then_ppo_one_object/#projectstutorialsobject_nav_ithor_dagger_then_ppo_one_object","text":"[view_source]","title":"projects.tutorials.object_nav_ithor_dagger_then_ppo_one_object"},{"location":"api/projects/tutorials/object_nav_ithor_dagger_then_ppo_one_object/#objectnavthordaggerthenppoexperimentconfig","text":"class ObjectNavThorDaggerThenPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ) [view_source] A simple object navigation experiment in THOR. Training with DAgger and then PPO.","title":"ObjectNavThorDaggerThenPPOExperimentConfig"},{"location":"api/projects/tutorials/object_nav_ithor_ppo_one_object/","text":"projects.tutorials.object_nav_ithor_ppo_one_object # [view_source] ObjectNavThorPPOExperimentConfig # class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ) [view_source] A simple object navigation experiment in THOR. Training with PPO.","title":"object_nav_ithor_ppo_one_object"},{"location":"api/projects/tutorials/object_nav_ithor_ppo_one_object/#projectstutorialsobject_nav_ithor_ppo_one_object","text":"[view_source]","title":"projects.tutorials.object_nav_ithor_ppo_one_object"},{"location":"api/projects/tutorials/object_nav_ithor_ppo_one_object/#objectnavthorppoexperimentconfig","text":"class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ) [view_source] A simple object navigation experiment in THOR. Training with PPO.","title":"ObjectNavThorPPOExperimentConfig"},{"location":"api/projects/tutorials/pointnav_habitat_rgb_ddppo/","text":"projects.tutorials.pointnav_habitat_rgb_ddppo # [view_source] PointNavHabitatRGBPPOTutorialExperimentConfig # class PointNavHabitatRGBPPOTutorialExperimentConfig ( ExperimentConfig ) [view_source] A Point Navigation experiment configuration in Habitat.","title":"pointnav_habitat_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_habitat_rgb_ddppo/#projectstutorialspointnav_habitat_rgb_ddppo","text":"[view_source]","title":"projects.tutorials.pointnav_habitat_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_habitat_rgb_ddppo/#pointnavhabitatrgbppotutorialexperimentconfig","text":"class PointNavHabitatRGBPPOTutorialExperimentConfig ( ExperimentConfig ) [view_source] A Point Navigation experiment configuration in Habitat.","title":"PointNavHabitatRGBPPOTutorialExperimentConfig"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/","text":"projects.tutorials.pointnav_ithor_rgb_ddppo # [view_source] ObjectNavRoboThorRGBPPOExperimentConfig # class ObjectNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ) [view_source] A Point Navigation experiment configuration in RoboThor.","title":"pointnav_ithor_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#projectstutorialspointnav_ithor_rgb_ddppo","text":"[view_source]","title":"projects.tutorials.pointnav_ithor_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"class ObjectNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ) [view_source] A Point Navigation experiment configuration in RoboThor.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/","text":"projects.tutorials.pointnav_robothor_rgb_ddppo # [view_source] PointNavRoboThorRGBPPOExperimentConfig # class PointNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ) [view_source] A Point Navigation experiment configuration in RoboThor.","title":"pointnav_robothor_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#projectstutorialspointnav_robothor_rgb_ddppo","text":"[view_source]","title":"projects.tutorials.pointnav_robothor_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#pointnavrobothorrgbppoexperimentconfig","text":"class PointNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ) [view_source] A Point Navigation experiment configuration in RoboThor.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo_viz/","text":"projects.tutorials.pointnav_robothor_rgb_ddppo_viz # [view_source] PointNavRoboThorRGBPPOVizExperimentConfig # class PointNavRoboThorRGBPPOVizExperimentConfig ( PointNavRoboThorRGBPPOExperimentConfig ) [view_source] ExperimentConfig used to demonstrate how to set up visualization code. Attributes viz_ep_ids : Scene names that will be visualized. viz_video_ids : Scene names that will have videos visualizations associated with them.","title":"pointnav_robothor_rgb_ddppo_viz"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo_viz/#projectstutorialspointnav_robothor_rgb_ddppo_viz","text":"[view_source]","title":"projects.tutorials.pointnav_robothor_rgb_ddppo_viz"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo_viz/#pointnavrobothorrgbppovizexperimentconfig","text":"class PointNavRoboThorRGBPPOVizExperimentConfig ( PointNavRoboThorRGBPPOExperimentConfig ) [view_source] ExperimentConfig used to demonstrate how to set up visualization code. Attributes viz_ep_ids : Scene names that will be visualized. viz_video_ids : Scene names that will have videos visualizations associated with them.","title":"PointNavRoboThorRGBPPOVizExperimentConfig"},{"location":"api/tests/multiprocessing/test_frozen_attribs/","text":"tests.multiprocessing.test_frozen_attribs # [view_source]","title":"test_frozen_attribs"},{"location":"api/tests/multiprocessing/test_frozen_attribs/#testsmultiprocessingtest_frozen_attribs","text":"[view_source]","title":"tests.multiprocessing.test_frozen_attribs"},{"location":"api/tests/sync_algs_cpu/test_to_to_obj_trains/","text":"tests.sync_algs_cpu.test_to_to_obj_trains # [view_source]","title":"test_to_to_obj_trains"},{"location":"api/tests/sync_algs_cpu/test_to_to_obj_trains/#testssync_algs_cputest_to_to_obj_trains","text":"[view_source]","title":"tests.sync_algs_cpu.test_to_to_obj_trains"},{"location":"api/utils/cache_utils/","text":"utils.cache_utils # [view_source]","title":"cache_utils"},{"location":"api/utils/cache_utils/#utilscache_utils","text":"[view_source]","title":"utils.cache_utils"},{"location":"api/utils/cacheless_frcnn/","text":"utils.cacheless_frcnn # [view_source]","title":"cacheless_frcnn"},{"location":"api/utils/cacheless_frcnn/#utilscacheless_frcnn","text":"[view_source]","title":"utils.cacheless_frcnn"},{"location":"api/utils/experiment_utils/","text":"utils.experiment_utils # [view_source] Utility classes and functions for running and designing experiments. recursive_update # recursive_update ( original : Union [ Dict , collections . abc . MutableMapping ], update : Union [ Dict , collections . abc . MutableMapping ]) [view_source] Recursively updates original dictionary with entries form update dict. Parameters original : Original dictionary to be updated. update : Dictionary with additional or replacement entries. Returns Updated original dictionary. Builder # class Builder ( tuple , typing . Generic [ ToBuildType ]) [view_source] Used to instantiate a given class with (default) parameters. Helper class that stores a class, default parameters for that class, and key word arguments that (possibly) overwrite the defaults. When calling this an object of the Builder class it generates a class of type class_type with parameters specified by the attributes default and kwargs (and possibly additional, overwriting, keyword arguments). Attributes class_type : The class to be instantiated when calling the object. kwargs : Keyword arguments used to instantiate an object of type class_type . default : Default parameters used when instantiating the class. Builder.__new__ # | __new__ ( cls , class_type : ToBuildType , kwargs : Optional [ Dict [ str , Any ]] = None , default : Optional [ Dict [ str , Any ]] = None ) [view_source] Create a new Builder. For parameter descriptions see the class documentation. Note that kwargs and default can be None in which case they are set to be empty dictionaries. Builder.__call__ # | __call__ ( ** kwargs ) -> ToBuildType [view_source] Build and return a new class. Parameters kwargs : additional keyword arguments to use when instantiating the object. These overwrite all arguments already in the self.kwargs and self.default attributes. Returns Class of type self.class_type with parameters taken from self.default , self.kwargs , and any keyword arguments additionally passed to __call__ . ScalarMeanTracker # class ScalarMeanTracker ( object ) [view_source] Track a collection scalar key -> mean pairs. ScalarMeanTracker.add_scalars # | add_scalars ( scalars : Dict [ str , Union [ float , int ]], n : Union [ int , Dict [ str , int ]] = 1 ) -> None [view_source] Add additional scalars to track. Parameters scalars : A dictionary of scalar key -> value pairs. ScalarMeanTracker.pop_and_reset # | pop_and_reset () -> Dict [ str , float ] [view_source] Return tracked means and reset. On resetting all previously tracked values are discarded. Returns A dictionary of scalar key -> current mean pairs corresponding to those values added with add_scalars . LoggingPackage # class LoggingPackage ( object ) [view_source] Data package used for logging. LinearDecay # class LinearDecay ( object ) [view_source] Linearly decay between two values over some number of steps. Obtain the value corresponding to the i th step by calling an instantiation of this object with the value i . Parameters steps : The number of steps over which to decay. startp : The starting value. endp : The ending value. LinearDecay.__init__ # | __init__ ( steps : int , startp : float = 1.0 , endp : float = 0.0 ) -> None [view_source] Initializer. See class documentation for parameter definitions. LinearDecay.__call__ # | __call__ ( epoch : int ) -> float [view_source] Get the decayed value for epoch number of steps. Parameters epoch : The number of steps. Returns Decayed value for epoch number of steps. set_deterministic_cudnn # set_deterministic_cudnn () -> None [view_source] Makes cudnn deterministic. This may slow down computations. set_seed # set_seed ( seed : Optional [ int ] = None ) -> None [view_source] Set seeds for multiple (cpu) sources of randomness. Sets seeds for (cpu) pytorch , base random , and numpy . Parameters seed : The seed to set. If set to None, keep using the current seed. EarlyStoppingCriterion # class EarlyStoppingCriterion ( abc . ABC ) [view_source] Abstract class for class who determines if training should stop early in a particular pipeline stage. EarlyStoppingCriterion.__call__ # | @abc . abstractmethod | __call__ ( stage_steps : int , total_steps : int , training_metrics : ScalarMeanTracker , test_valid_metrics : List [ Tuple [ str , int , Union [ float , np . ndarray ]]]) -> bool [view_source] Returns True if training should be stopped early. Parameters stage_steps : Total number of steps taken in the current pipeline stage. total_steps : Total number of steps taken during training so far (includes steps taken in prior pipeline stages). training_metrics : Metrics recovered over some fixed number of steps (see the metric_accumulate_interval attribute in the TrainingPipeline class) training. test_valid_metrics : A tuple (key, steps, value) where key is the metric's name prefixed by either \"valid/\" or \"test/\" , steps is the total number of steps that the validation/test model was trained for, and value is the value of the metric. NeverEarlyStoppingCriterion # class NeverEarlyStoppingCriterion ( EarlyStoppingCriterion ) [view_source] Implementation of EarlyStoppingCriterion which never stops early. OffPolicyPipelineComponent # class OffPolicyPipelineComponent ( NamedTuple ) [view_source] An off-policy component for a PipeLineStage. Attributes data_iterator_builder : A function to instantiate a Data Iterator (with a next (self) method) loss_names : list of unique names assigned to off-policy losses updates : number of off-policy updates between on-policy rollout collections loss_weights : A list of floating point numbers describing the relative weights applied to the losses referenced by loss_names . Should be the same length as loss_names . If this is None , all weights will be assumed to be one. data_iterator_kwargs_generator : Optional generator of keyword arguments for data_iterator_builder (useful for distributed training. It takes a cur_worker int value, a rollouts_per_worker list of number of samplers per training worker, and an optional random seed shared by all workers, which can be None. PipelineStage # class PipelineStage ( object ) [view_source] A single stage in a training pipeline. Attributes loss_name : A collection of unique names assigned to losses. These will reference the Loss objects in a TrainingPipeline instance. max_stage_steps : Either the total number of steps agents should take in this stage or a Callable object (e.g. a function) early_stopping_criterion : An EarlyStoppingCriterion object which determines if training in this stage should be stopped early. If None then no early stopping occurs. If early_stopping_criterion is not None then we do not guarantee reproducibility when restarting a model from a checkpoint (as the EarlyStoppingCriterion object may store internal state which is not saved in the checkpoint). loss_weights : A list of floating point numbers describing the relative weights applied to the losses referenced by loss_name . Should be the same length as loss_name . If this is None , all weights will be assumed to be one. teacher_forcing : If applicable, defines the probability an agent will take the expert action (as opposed to its own sampled action) at a given time point. TrainingPipeline # class TrainingPipeline ( object ) [view_source] Class defining the stages (and global parameters) in a training pipeline. The training pipeline can be used as an iterator to go through the pipeline stages in, for instance, a loop. Attributes named_losses : Dictionary mapping a the name of a loss to either an instantiation of that loss or a Builder that, when called, will return that loss. pipeline_stages : A list of PipelineStages. Each of these define how the agent will be trained and are executed sequentially. optimizer_builder : Builder object to instantiate the optimizer to use during training. num_mini_batch : The number of mini-batches to break a rollout into. update_repeats : The number of times we will cycle through the mini-batches corresponding to a single rollout doing gradient updates. max_grad_norm : The maximum \"inf\" norm of any gradient step (gradients are clipped to not exceed this). num_steps : Total number of steps a single agent takes in a rollout. gamma : Discount factor applied to rewards (should be in [0, 1]). use_gae : Whether or not to use generalized advantage estimation (GAE). gae_lambda : The additional parameter used in GAE. save_interval : The frequency with which to save (in total agent steps taken). If None then no checkpoints will be saved. Otherwise, in addition to the checkpoints being saved every save_interval steps, a checkpoint will always be saved at the end of each pipeline stage. If save_interval <= 0 then checkpoints will only be saved at the end of each pipeline stage. metric_accumulate_interval : The frequency with which training/validation metrics are accumulated (in total agent steps). Metrics accumulated in an interval are logged (if should_log is True ) and used by the stage's early stopping criterion (if any). should_log : True if metrics accumulated during training should be logged to the console as well as to a tensorboard file. lr_scheduler_builder : Optional builder object to instantiate the learning rate scheduler used through the pipeline. TrainingPipeline.__init__ # | __init__ ( named_losses : Dict [ str , Union [ Loss , Builder [ Loss ]]], pipeline_stages : List [ PipelineStage ], optimizer_builder : Builder [ optim . Optimizer ], num_mini_batch : int , update_repeats : int , max_grad_norm : float , num_steps : int , gamma : float , use_gae : bool , gae_lambda : float , advance_scene_rollout_period : Optional [ int ], save_interval : Optional [ int ], metric_accumulate_interval : int , should_log : bool = True , lr_scheduler_builder : Optional [ Builder [ optim . lr_scheduler . _LRScheduler ]] = None ) [view_source] Initializer. See class docstring for parameter definitions.","title":"experiment_utils"},{"location":"api/utils/experiment_utils/#utilsexperiment_utils","text":"[view_source] Utility classes and functions for running and designing experiments.","title":"utils.experiment_utils"},{"location":"api/utils/experiment_utils/#recursive_update","text":"recursive_update ( original : Union [ Dict , collections . abc . MutableMapping ], update : Union [ Dict , collections . abc . MutableMapping ]) [view_source] Recursively updates original dictionary with entries form update dict. Parameters original : Original dictionary to be updated. update : Dictionary with additional or replacement entries. Returns Updated original dictionary.","title":"recursive_update"},{"location":"api/utils/experiment_utils/#builder","text":"class Builder ( tuple , typing . Generic [ ToBuildType ]) [view_source] Used to instantiate a given class with (default) parameters. Helper class that stores a class, default parameters for that class, and key word arguments that (possibly) overwrite the defaults. When calling this an object of the Builder class it generates a class of type class_type with parameters specified by the attributes default and kwargs (and possibly additional, overwriting, keyword arguments). Attributes class_type : The class to be instantiated when calling the object. kwargs : Keyword arguments used to instantiate an object of type class_type . default : Default parameters used when instantiating the class.","title":"Builder"},{"location":"api/utils/experiment_utils/#builder__new__","text":"| __new__ ( cls , class_type : ToBuildType , kwargs : Optional [ Dict [ str , Any ]] = None , default : Optional [ Dict [ str , Any ]] = None ) [view_source] Create a new Builder. For parameter descriptions see the class documentation. Note that kwargs and default can be None in which case they are set to be empty dictionaries.","title":"Builder.__new__"},{"location":"api/utils/experiment_utils/#builder__call__","text":"| __call__ ( ** kwargs ) -> ToBuildType [view_source] Build and return a new class. Parameters kwargs : additional keyword arguments to use when instantiating the object. These overwrite all arguments already in the self.kwargs and self.default attributes. Returns Class of type self.class_type with parameters taken from self.default , self.kwargs , and any keyword arguments additionally passed to __call__ .","title":"Builder.__call__"},{"location":"api/utils/experiment_utils/#scalarmeantracker","text":"class ScalarMeanTracker ( object ) [view_source] Track a collection scalar key -> mean pairs.","title":"ScalarMeanTracker"},{"location":"api/utils/experiment_utils/#scalarmeantrackeradd_scalars","text":"| add_scalars ( scalars : Dict [ str , Union [ float , int ]], n : Union [ int , Dict [ str , int ]] = 1 ) -> None [view_source] Add additional scalars to track. Parameters scalars : A dictionary of scalar key -> value pairs.","title":"ScalarMeanTracker.add_scalars"},{"location":"api/utils/experiment_utils/#scalarmeantrackerpop_and_reset","text":"| pop_and_reset () -> Dict [ str , float ] [view_source] Return tracked means and reset. On resetting all previously tracked values are discarded. Returns A dictionary of scalar key -> current mean pairs corresponding to those values added with add_scalars .","title":"ScalarMeanTracker.pop_and_reset"},{"location":"api/utils/experiment_utils/#loggingpackage","text":"class LoggingPackage ( object ) [view_source] Data package used for logging.","title":"LoggingPackage"},{"location":"api/utils/experiment_utils/#lineardecay","text":"class LinearDecay ( object ) [view_source] Linearly decay between two values over some number of steps. Obtain the value corresponding to the i th step by calling an instantiation of this object with the value i . Parameters steps : The number of steps over which to decay. startp : The starting value. endp : The ending value.","title":"LinearDecay"},{"location":"api/utils/experiment_utils/#lineardecay__init__","text":"| __init__ ( steps : int , startp : float = 1.0 , endp : float = 0.0 ) -> None [view_source] Initializer. See class documentation for parameter definitions.","title":"LinearDecay.__init__"},{"location":"api/utils/experiment_utils/#lineardecay__call__","text":"| __call__ ( epoch : int ) -> float [view_source] Get the decayed value for epoch number of steps. Parameters epoch : The number of steps. Returns Decayed value for epoch number of steps.","title":"LinearDecay.__call__"},{"location":"api/utils/experiment_utils/#set_deterministic_cudnn","text":"set_deterministic_cudnn () -> None [view_source] Makes cudnn deterministic. This may slow down computations.","title":"set_deterministic_cudnn"},{"location":"api/utils/experiment_utils/#set_seed","text":"set_seed ( seed : Optional [ int ] = None ) -> None [view_source] Set seeds for multiple (cpu) sources of randomness. Sets seeds for (cpu) pytorch , base random , and numpy . Parameters seed : The seed to set. If set to None, keep using the current seed.","title":"set_seed"},{"location":"api/utils/experiment_utils/#earlystoppingcriterion","text":"class EarlyStoppingCriterion ( abc . ABC ) [view_source] Abstract class for class who determines if training should stop early in a particular pipeline stage.","title":"EarlyStoppingCriterion"},{"location":"api/utils/experiment_utils/#earlystoppingcriterion__call__","text":"| @abc . abstractmethod | __call__ ( stage_steps : int , total_steps : int , training_metrics : ScalarMeanTracker , test_valid_metrics : List [ Tuple [ str , int , Union [ float , np . ndarray ]]]) -> bool [view_source] Returns True if training should be stopped early. Parameters stage_steps : Total number of steps taken in the current pipeline stage. total_steps : Total number of steps taken during training so far (includes steps taken in prior pipeline stages). training_metrics : Metrics recovered over some fixed number of steps (see the metric_accumulate_interval attribute in the TrainingPipeline class) training. test_valid_metrics : A tuple (key, steps, value) where key is the metric's name prefixed by either \"valid/\" or \"test/\" , steps is the total number of steps that the validation/test model was trained for, and value is the value of the metric.","title":"EarlyStoppingCriterion.__call__"},{"location":"api/utils/experiment_utils/#neverearlystoppingcriterion","text":"class NeverEarlyStoppingCriterion ( EarlyStoppingCriterion ) [view_source] Implementation of EarlyStoppingCriterion which never stops early.","title":"NeverEarlyStoppingCriterion"},{"location":"api/utils/experiment_utils/#offpolicypipelinecomponent","text":"class OffPolicyPipelineComponent ( NamedTuple ) [view_source] An off-policy component for a PipeLineStage. Attributes data_iterator_builder : A function to instantiate a Data Iterator (with a next (self) method) loss_names : list of unique names assigned to off-policy losses updates : number of off-policy updates between on-policy rollout collections loss_weights : A list of floating point numbers describing the relative weights applied to the losses referenced by loss_names . Should be the same length as loss_names . If this is None , all weights will be assumed to be one. data_iterator_kwargs_generator : Optional generator of keyword arguments for data_iterator_builder (useful for distributed training. It takes a cur_worker int value, a rollouts_per_worker list of number of samplers per training worker, and an optional random seed shared by all workers, which can be None.","title":"OffPolicyPipelineComponent"},{"location":"api/utils/experiment_utils/#pipelinestage","text":"class PipelineStage ( object ) [view_source] A single stage in a training pipeline. Attributes loss_name : A collection of unique names assigned to losses. These will reference the Loss objects in a TrainingPipeline instance. max_stage_steps : Either the total number of steps agents should take in this stage or a Callable object (e.g. a function) early_stopping_criterion : An EarlyStoppingCriterion object which determines if training in this stage should be stopped early. If None then no early stopping occurs. If early_stopping_criterion is not None then we do not guarantee reproducibility when restarting a model from a checkpoint (as the EarlyStoppingCriterion object may store internal state which is not saved in the checkpoint). loss_weights : A list of floating point numbers describing the relative weights applied to the losses referenced by loss_name . Should be the same length as loss_name . If this is None , all weights will be assumed to be one. teacher_forcing : If applicable, defines the probability an agent will take the expert action (as opposed to its own sampled action) at a given time point.","title":"PipelineStage"},{"location":"api/utils/experiment_utils/#trainingpipeline","text":"class TrainingPipeline ( object ) [view_source] Class defining the stages (and global parameters) in a training pipeline. The training pipeline can be used as an iterator to go through the pipeline stages in, for instance, a loop. Attributes named_losses : Dictionary mapping a the name of a loss to either an instantiation of that loss or a Builder that, when called, will return that loss. pipeline_stages : A list of PipelineStages. Each of these define how the agent will be trained and are executed sequentially. optimizer_builder : Builder object to instantiate the optimizer to use during training. num_mini_batch : The number of mini-batches to break a rollout into. update_repeats : The number of times we will cycle through the mini-batches corresponding to a single rollout doing gradient updates. max_grad_norm : The maximum \"inf\" norm of any gradient step (gradients are clipped to not exceed this). num_steps : Total number of steps a single agent takes in a rollout. gamma : Discount factor applied to rewards (should be in [0, 1]). use_gae : Whether or not to use generalized advantage estimation (GAE). gae_lambda : The additional parameter used in GAE. save_interval : The frequency with which to save (in total agent steps taken). If None then no checkpoints will be saved. Otherwise, in addition to the checkpoints being saved every save_interval steps, a checkpoint will always be saved at the end of each pipeline stage. If save_interval <= 0 then checkpoints will only be saved at the end of each pipeline stage. metric_accumulate_interval : The frequency with which training/validation metrics are accumulated (in total agent steps). Metrics accumulated in an interval are logged (if should_log is True ) and used by the stage's early stopping criterion (if any). should_log : True if metrics accumulated during training should be logged to the console as well as to a tensorboard file. lr_scheduler_builder : Optional builder object to instantiate the learning rate scheduler used through the pipeline.","title":"TrainingPipeline"},{"location":"api/utils/experiment_utils/#trainingpipeline__init__","text":"| __init__ ( named_losses : Dict [ str , Union [ Loss , Builder [ Loss ]]], pipeline_stages : List [ PipelineStage ], optimizer_builder : Builder [ optim . Optimizer ], num_mini_batch : int , update_repeats : int , max_grad_norm : float , num_steps : int , gamma : float , use_gae : bool , gae_lambda : float , advance_scene_rollout_period : Optional [ int ], save_interval : Optional [ int ], metric_accumulate_interval : int , should_log : bool = True , lr_scheduler_builder : Optional [ Builder [ optim . lr_scheduler . _LRScheduler ]] = None ) [view_source] Initializer. See class docstring for parameter definitions.","title":"TrainingPipeline.__init__"},{"location":"api/utils/misc_utils/","text":"utils.misc_utils # [view_source] HashableDict # class HashableDict ( dict ) [view_source] A dictionary which is hashable so long as all of its values are hashable. A HashableDict object will allow setting / deleting of items until the first time that __hash__() is called on it after which attempts to set or delete items will throw RuntimeError exceptions.","title":"misc_utils"},{"location":"api/utils/misc_utils/#utilsmisc_utils","text":"[view_source]","title":"utils.misc_utils"},{"location":"api/utils/misc_utils/#hashabledict","text":"class HashableDict ( dict ) [view_source] A dictionary which is hashable so long as all of its values are hashable. A HashableDict object will allow setting / deleting of items until the first time that __hash__() is called on it after which attempts to set or delete items will throw RuntimeError exceptions.","title":"HashableDict"},{"location":"api/utils/model_utils/","text":"utils.model_utils # [view_source] Functions used to initialize and manipulate pytorch models. Flatten # class Flatten ( nn . Module ) [view_source] Flatten input tensor so that it is of shape (FLATTENED_BATCH x -1). Flatten.forward # | forward ( x ) [view_source] Flatten input tensor. Parameters x : Tensor of size (FLATTENED_BATCH x ...) to flatten to size (FLATTENED_BATCH x -1) Returns Flattened tensor. init_linear_layer # init_linear_layer ( module : nn . Linear , weight_init : Callable , bias_init : Callable , gain = 1 ) [view_source] Initialize a torch.nn.Linear layer. Parameters module : A torch linear layer. weight_init : Function used to initialize the weight parameters of the linear layer. Should take the weight data tensor and gain as input. bias_init : Function used to initialize the bias parameters of the linear layer. Should take the bias data tensor and gain as input. gain : The gain to apply. Returns The initialized linear layer. compute_cnn_output # compute_cnn_output ( cnn : nn . Module , cnn_input : torch . Tensor , permute_order : Optional [ Tuple [ int , ... ]] = ( 0 , # FLAT_BATCH (flattening steps, samplers and agents) 3 , # CHANNEL 1 , # ROW 2 , # COL )) [view_source] Computes CNN outputs for given inputs. Parameters cnn : A torch CNN. cnn_input : A torch Tensor with inputs. permute_order : A permutation Tuple to provide PyTorch dimension order, default (0, 3, 1, 2), where 0 corresponds to the flattened batch dimensions (combining step, sampler and agent) Returns CNN output with dimensions [STEP, SAMPLER, AGENT, CHANNEL, (HEIGHT, WIDTH)].","title":"model_utils"},{"location":"api/utils/model_utils/#utilsmodel_utils","text":"[view_source] Functions used to initialize and manipulate pytorch models.","title":"utils.model_utils"},{"location":"api/utils/model_utils/#flatten","text":"class Flatten ( nn . Module ) [view_source] Flatten input tensor so that it is of shape (FLATTENED_BATCH x -1).","title":"Flatten"},{"location":"api/utils/model_utils/#flattenforward","text":"| forward ( x ) [view_source] Flatten input tensor. Parameters x : Tensor of size (FLATTENED_BATCH x ...) to flatten to size (FLATTENED_BATCH x -1) Returns Flattened tensor.","title":"Flatten.forward"},{"location":"api/utils/model_utils/#init_linear_layer","text":"init_linear_layer ( module : nn . Linear , weight_init : Callable , bias_init : Callable , gain = 1 ) [view_source] Initialize a torch.nn.Linear layer. Parameters module : A torch linear layer. weight_init : Function used to initialize the weight parameters of the linear layer. Should take the weight data tensor and gain as input. bias_init : Function used to initialize the bias parameters of the linear layer. Should take the bias data tensor and gain as input. gain : The gain to apply. Returns The initialized linear layer.","title":"init_linear_layer"},{"location":"api/utils/model_utils/#compute_cnn_output","text":"compute_cnn_output ( cnn : nn . Module , cnn_input : torch . Tensor , permute_order : Optional [ Tuple [ int , ... ]] = ( 0 , # FLAT_BATCH (flattening steps, samplers and agents) 3 , # CHANNEL 1 , # ROW 2 , # COL )) [view_source] Computes CNN outputs for given inputs. Parameters cnn : A torch CNN. cnn_input : A torch Tensor with inputs. permute_order : A permutation Tuple to provide PyTorch dimension order, default (0, 3, 1, 2), where 0 corresponds to the flattened batch dimensions (combining step, sampler and agent) Returns CNN output with dimensions [STEP, SAMPLER, AGENT, CHANNEL, (HEIGHT, WIDTH)].","title":"compute_cnn_output"},{"location":"api/utils/system/","text":"utils.system # [view_source] HUMAN_LOG_LEVELS # [view_source] Available log levels: \"debug\", \"info\", \"warning\", \"error\", \"none\" get_logger # get_logger () -> logging . Logger [view_source] Get a logging.Logger to stderr. It can be called whenever we wish to log some message. Messages can get mixed-up (https://docs.python.org/3.6/library/multiprocessing.html#logging), but it works well in most cases. Returns logger : the logging.Logger object init_logging # init_logging ( human_log_level : str = \"info\" ) -> None [view_source] Init the logging.Logger . It should be called only once in the app (e.g. in main ). It sets the log_level to one of HUMAN_LOG_LEVELS . And sets up a handler for stderr. The logging level is propagated to all supproceeses. find_free_port # find_free_port ( address : str = \"127.0.0.1\" ) -> int [view_source] Finds a free port for distributed training. Returns port : port number that can be used to listen","title":"system"},{"location":"api/utils/system/#utilssystem","text":"[view_source]","title":"utils.system"},{"location":"api/utils/system/#human_log_levels","text":"[view_source] Available log levels: \"debug\", \"info\", \"warning\", \"error\", \"none\"","title":"HUMAN_LOG_LEVELS"},{"location":"api/utils/system/#get_logger","text":"get_logger () -> logging . Logger [view_source] Get a logging.Logger to stderr. It can be called whenever we wish to log some message. Messages can get mixed-up (https://docs.python.org/3.6/library/multiprocessing.html#logging), but it works well in most cases. Returns logger : the logging.Logger object","title":"get_logger"},{"location":"api/utils/system/#init_logging","text":"init_logging ( human_log_level : str = \"info\" ) -> None [view_source] Init the logging.Logger . It should be called only once in the app (e.g. in main ). It sets the log_level to one of HUMAN_LOG_LEVELS . And sets up a handler for stderr. The logging level is propagated to all supproceeses.","title":"init_logging"},{"location":"api/utils/system/#find_free_port","text":"find_free_port ( address : str = \"127.0.0.1\" ) -> int [view_source] Finds a free port for distributed training. Returns port : port number that can be used to listen","title":"find_free_port"},{"location":"api/utils/tensor_utils/","text":"utils.tensor_utils # [view_source] Functions used to manipulate pytorch tensors and numpy arrays. to_device_recursively # to_device_recursively ( input : Any , device : Union [ str , torch . device , int ], inplace : bool = True ) [view_source] Recursively places tensors on the appropriate device. detach_recursively # detach_recursively ( input : Any , inplace = True ) [view_source] Recursively detaches tensors in some data structure from their computation graph. batch_observations # batch_observations ( observations : List [ Dict ], device : Optional [ torch . device ] = None ) -> Dict [ str , Union [ Dict , torch . Tensor ]] [view_source] Transpose a batch of observation dicts to a dict of batched observations. Arguments observations : List of dicts of observations. device : The torch.device to put the resulting tensors on. Will not move the tensors if None. Returns Transposed dict of lists of observations. to_tensor # to_tensor ( v ) -> torch . Tensor [view_source] Return a torch.Tensor version of the input. Parameters v : Input values that can be coerced into being a tensor. Returns A tensor version of the input. tile_images # tile_images ( images : List [ np . ndarray ]) -> np . ndarray [view_source] Tile multiple images into single image. Parameters images : list of images where each image has dimension (height x width x channels) Returns Tiled image (new_height x width x channels). image # image ( tag , tensor , rescale = 1 , dataformats = \"CHW\" ) [view_source] Outputs a Summary protocol buffer with images. The summary has up to max_images summary values containing images. The images are built from tensor which must be 3-D with shape [height, width, channels] and where channels can be: 1: tensor is interpreted as Grayscale. 3: tensor is interpreted as RGB. 4: tensor is interpreted as RGBA. Parameters tag : A name for the generated node. Will also serve as a series name in TensorBoard. tensor : A 3-D uint8 or float32 Tensor of shape [height, width, channels] where channels is 1, 3, or 4. 'tensor' can either have values in [0, 1] (float32) or [0, 255] (uint8). The image() function will scale the image values to [0, 255] by applying a scale factor of either 1 (uint8) or 255 (float32). rescale : The scale. dataformats : Input image shape format. Returns A scalar Tensor of type string . The serialized Summary protocol buffer. ScaleBothSides # class ScaleBothSides ( object ) [view_source] Rescales the input PIL.Image to the given 'width' and height . Attributes width: new width height: new height interpolation: Default: PIL.Image.BILINEAR","title":"tensor_utils"},{"location":"api/utils/tensor_utils/#utilstensor_utils","text":"[view_source] Functions used to manipulate pytorch tensors and numpy arrays.","title":"utils.tensor_utils"},{"location":"api/utils/tensor_utils/#to_device_recursively","text":"to_device_recursively ( input : Any , device : Union [ str , torch . device , int ], inplace : bool = True ) [view_source] Recursively places tensors on the appropriate device.","title":"to_device_recursively"},{"location":"api/utils/tensor_utils/#detach_recursively","text":"detach_recursively ( input : Any , inplace = True ) [view_source] Recursively detaches tensors in some data structure from their computation graph.","title":"detach_recursively"},{"location":"api/utils/tensor_utils/#batch_observations","text":"batch_observations ( observations : List [ Dict ], device : Optional [ torch . device ] = None ) -> Dict [ str , Union [ Dict , torch . Tensor ]] [view_source] Transpose a batch of observation dicts to a dict of batched observations. Arguments observations : List of dicts of observations. device : The torch.device to put the resulting tensors on. Will not move the tensors if None. Returns Transposed dict of lists of observations.","title":"batch_observations"},{"location":"api/utils/tensor_utils/#to_tensor","text":"to_tensor ( v ) -> torch . Tensor [view_source] Return a torch.Tensor version of the input. Parameters v : Input values that can be coerced into being a tensor. Returns A tensor version of the input.","title":"to_tensor"},{"location":"api/utils/tensor_utils/#tile_images","text":"tile_images ( images : List [ np . ndarray ]) -> np . ndarray [view_source] Tile multiple images into single image. Parameters images : list of images where each image has dimension (height x width x channels) Returns Tiled image (new_height x width x channels).","title":"tile_images"},{"location":"api/utils/tensor_utils/#image","text":"image ( tag , tensor , rescale = 1 , dataformats = \"CHW\" ) [view_source] Outputs a Summary protocol buffer with images. The summary has up to max_images summary values containing images. The images are built from tensor which must be 3-D with shape [height, width, channels] and where channels can be: 1: tensor is interpreted as Grayscale. 3: tensor is interpreted as RGB. 4: tensor is interpreted as RGBA. Parameters tag : A name for the generated node. Will also serve as a series name in TensorBoard. tensor : A 3-D uint8 or float32 Tensor of shape [height, width, channels] where channels is 1, 3, or 4. 'tensor' can either have values in [0, 1] (float32) or [0, 255] (uint8). The image() function will scale the image values to [0, 255] by applying a scale factor of either 1 (uint8) or 255 (float32). rescale : The scale. dataformats : Input image shape format. Returns A scalar Tensor of type string . The serialized Summary protocol buffer.","title":"image"},{"location":"api/utils/tensor_utils/#scalebothsides","text":"class ScaleBothSides ( object ) [view_source] Rescales the input PIL.Image to the given 'width' and height . Attributes width: new width height: new height interpolation: Default: PIL.Image.BILINEAR","title":"ScaleBothSides"},{"location":"api/utils/viz_utils/","text":"utils.viz_utils # [view_source]","title":"viz_utils"},{"location":"api/utils/viz_utils/#utilsviz_utils","text":"[view_source]","title":"utils.viz_utils"},{"location":"getting_started/abstractions/","text":"Primary abstractions # Our package relies on a collection of fundamental abstractions to define how, and in what task, an agent should be trained and evaluated. A subset of these abstractions are described in plain language below. Each of the below sections end with a link to the (formal) documentation of the abstraction as well as a link to an example implementation of the abstraction (if relevant). The following provides a high-level illustration of how these abstractions interact. Experiment configuration # In allenact , experiments are defined by implementing the abstract ExperimentConfig class. The methods of this implementation are then called during training/inference to properly set up the desired experiment. For example, the ExperimentConfig.create_model method will be called at the beginning of training to create the model to be trained. See either the \"designing your first minigrid experiment\" or the \"designing an experiment for point navigation\" tutorials to get an in-depth description of how these experiment configurations are defined in practice. See also the abstract ExperimentConfig class and an example implementation . Task sampler # A task sampler is responsible for generating a sequence of tasks for agents to solve. The sequence of tasks can be randomly generated (e.g. in training) or extracted from an ordered pool (e.g. in validation or testing). See the abstract TaskSampler class and an example implementation . Task # Tasks define the scope of the interaction between agents and an environment (including the action types agents are allowed to execute), as well as metrics to evaluate the agents' performance. For example, we might define a task ObjectNavTask in which agents receive observations obtained from the environment (e.g. RGB images) or directly from the task (e.g. a target object class) and are allowed to execute actions such as MoveAhead , RotateRight , RotateLeft , and End whenever agents determine they have reached their target. The metrics might include a success indicator or some quantitative metric on the optimality of the followed path. See the abstract Task class and an example implementation . Sensor # Sensors provide observations extracted from an environment (e.g. RGB or depth images) or directly from a task (e.g. the end point in point navigation or target object class in semantic navigation) that can be directly consumed by agents. See the abstract Sensor class and an example implementation . Actor critic model # The actor-critic agent is responsible for computing batched action probabilities and state values given the observations provided by sensors, internal state representations, previous actions, and potentially other inputs. See the abstract ActorCriticModel class and an example implementation . Training pipeline # The training pipeline, defined in the ExperimentConfig 's training_pipeline method , contains one or more training stages where different losses can be combined or sequentially applied . Losses # Actor-critic losses compute a combination of action loss and value loss out of collected experience that can be used to train actor-critic models with back-propagation, e.g. PPO or A2C. See the AbstractActorCriticLoss class and an example implementation . Off-policy losses implement generic training iterations in which a batch of data is run through a model (that can be a subgraph of an ActorCriticModel ) and a loss is computed on the model's output. See the AbstractOffPolicyLoss class and an example implementation .","title":"Primary abstractions"},{"location":"getting_started/abstractions/#primary-abstractions","text":"Our package relies on a collection of fundamental abstractions to define how, and in what task, an agent should be trained and evaluated. A subset of these abstractions are described in plain language below. Each of the below sections end with a link to the (formal) documentation of the abstraction as well as a link to an example implementation of the abstraction (if relevant). The following provides a high-level illustration of how these abstractions interact.","title":"Primary abstractions"},{"location":"getting_started/abstractions/#experiment-configuration","text":"In allenact , experiments are defined by implementing the abstract ExperimentConfig class. The methods of this implementation are then called during training/inference to properly set up the desired experiment. For example, the ExperimentConfig.create_model method will be called at the beginning of training to create the model to be trained. See either the \"designing your first minigrid experiment\" or the \"designing an experiment for point navigation\" tutorials to get an in-depth description of how these experiment configurations are defined in practice. See also the abstract ExperimentConfig class and an example implementation .","title":"Experiment configuration"},{"location":"getting_started/abstractions/#task-sampler","text":"A task sampler is responsible for generating a sequence of tasks for agents to solve. The sequence of tasks can be randomly generated (e.g. in training) or extracted from an ordered pool (e.g. in validation or testing). See the abstract TaskSampler class and an example implementation .","title":"Task sampler"},{"location":"getting_started/abstractions/#task","text":"Tasks define the scope of the interaction between agents and an environment (including the action types agents are allowed to execute), as well as metrics to evaluate the agents' performance. For example, we might define a task ObjectNavTask in which agents receive observations obtained from the environment (e.g. RGB images) or directly from the task (e.g. a target object class) and are allowed to execute actions such as MoveAhead , RotateRight , RotateLeft , and End whenever agents determine they have reached their target. The metrics might include a success indicator or some quantitative metric on the optimality of the followed path. See the abstract Task class and an example implementation .","title":"Task"},{"location":"getting_started/abstractions/#sensor","text":"Sensors provide observations extracted from an environment (e.g. RGB or depth images) or directly from a task (e.g. the end point in point navigation or target object class in semantic navigation) that can be directly consumed by agents. See the abstract Sensor class and an example implementation .","title":"Sensor"},{"location":"getting_started/abstractions/#actor-critic-model","text":"The actor-critic agent is responsible for computing batched action probabilities and state values given the observations provided by sensors, internal state representations, previous actions, and potentially other inputs. See the abstract ActorCriticModel class and an example implementation .","title":"Actor critic model"},{"location":"getting_started/abstractions/#training-pipeline","text":"The training pipeline, defined in the ExperimentConfig 's training_pipeline method , contains one or more training stages where different losses can be combined or sequentially applied .","title":"Training pipeline"},{"location":"getting_started/abstractions/#losses","text":"Actor-critic losses compute a combination of action loss and value loss out of collected experience that can be used to train actor-critic models with back-propagation, e.g. PPO or A2C. See the AbstractActorCriticLoss class and an example implementation . Off-policy losses implement generic training iterations in which a batch of data is run through a model (that can be a subgraph of an ActorCriticModel ) and a loss is computed on the model's output. See the AbstractOffPolicyLoss class and an example implementation .","title":"Losses"},{"location":"getting_started/running-your-first-experiment/","text":"Running your first experiment # Assuming you have installed allenact , you can run your first experiment by calling python main.py minigrid_tutorial -b projects/tutorials -m 8 -o minigrid_output -s 12345 from the allenact root directory. With -b projects/tutorials we tell allenact that minigrid_tutorial experiment config file will be found in the projects/tutorials directory. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o minigrid_output we set the output folder into which results and logs will be saved. With -s 12345 we set the random seed. If everything was installed correctly, a simple model will be trained (and validated) in the MiniGrid environment and a new folder minigrid_output will be created containing: a checkpoints/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with model weight checkpoints, a used_configs/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with all used configuration files, and a tensorboard log file under tb/MiniGridTutorial/LOCAL_TIME_STR/ . Here LOCAL_TIME_STR is a string that records the time when the experiment was started (e.g. the string \"2020-08-21_18-19-47\" corresponds to an experiment started on August 21st 2020, 47 seconds past 6:19pm. If we have Tensorboard installed, we can track training progress with tensorboard --logdir minigrid_output/tb which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: A detailed tutorial describing how the minigrid_tutorial experiment configuration was created can be found here . To run your own custom experiment simply define a new experiment configuration in a file projects/YOUR_PROJECT_NAME/experiments/my_custom_experiment.py after which you may run it with python main.py my_custom_experiment -b projects/YOUR_PROJECT_NAME/experiments .","title":"Run your first experiment"},{"location":"getting_started/running-your-first-experiment/#running-your-first-experiment","text":"Assuming you have installed allenact , you can run your first experiment by calling python main.py minigrid_tutorial -b projects/tutorials -m 8 -o minigrid_output -s 12345 from the allenact root directory. With -b projects/tutorials we tell allenact that minigrid_tutorial experiment config file will be found in the projects/tutorials directory. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o minigrid_output we set the output folder into which results and logs will be saved. With -s 12345 we set the random seed. If everything was installed correctly, a simple model will be trained (and validated) in the MiniGrid environment and a new folder minigrid_output will be created containing: a checkpoints/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with model weight checkpoints, a used_configs/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with all used configuration files, and a tensorboard log file under tb/MiniGridTutorial/LOCAL_TIME_STR/ . Here LOCAL_TIME_STR is a string that records the time when the experiment was started (e.g. the string \"2020-08-21_18-19-47\" corresponds to an experiment started on August 21st 2020, 47 seconds past 6:19pm. If we have Tensorboard installed, we can track training progress with tensorboard --logdir minigrid_output/tb which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: A detailed tutorial describing how the minigrid_tutorial experiment configuration was created can be found here . To run your own custom experiment simply define a new experiment configuration in a file projects/YOUR_PROJECT_NAME/experiments/my_custom_experiment.py after which you may run it with python main.py my_custom_experiment -b projects/YOUR_PROJECT_NAME/experiments .","title":"Running your first experiment"},{"location":"getting_started/structure/","text":"Structure of the codebase # The codebase consists of the following directories: core , datasets , docs , overrides , plugins , pretrained_model_ckpts , projects , scripts , tests and utils . Below, we explain the overall structure and how different components of the codebase are organized. core directory # Contains runtime algorithms for on-policy and off-policy training and inference, base abstractions used throughout the code base and basic models to be used as building blocks in future models. core.algorithms includes on-policy and off-policy training nd inference algorithms and abstractions for losses, policies, rollout storage, etc. core.base_abstractions includes the base ExperimentConfig , distributions, base Sensor , TaskSampler , Task , etc. core.models includes basic CNN, and RNN state encoders, besides basic ActorCriticModel implementations. datasets directory # A directory made to store task-specific datasets. For example, the script datasets/download_navigation_datasets.sh can be used to automatically download task dataset files for Point Navigation within the RoboTHOR environment and it will place these files into a new datasets/robothor-pointnav directory. docs directory # Contains documentation for the framework, including guides for installation and first experiments, how-to's for the definition and usage of different abstractions, tutorials and per-project documentation. overrides directory # Files within this directory are used to the look and structure of the documentation generated when running mkdocs . See our FAQ for information on how to generate this documentation for yourself. plugins directory # Contains implementations of ActorCriticModel s and Task s in different environments. Each plugin folder is named as {environment}_plugin and contains three subfolders: configs to host useful configuration for the environment or tasks. data to store data to be consumed by the environment or tasks. scripts to setup the plugin or gather and process data. pretrained_model_ckpts directory # Directory into which pretrained model checkpoints will be saved. See also the pretrained_model_ckpts/download_navigation_model_ckpts.sh which can be used to download such checkpoints. projects directory # Contains project-specific code like experiment configurations and scripts to process results, generate visualizations or prepare data. scripts directory # Includes framework-wide scripts to build the documentation, format code, run_tests and start an xserver. The latter can be used for OpenGL-based environments having super-user privileges in Linux, assuming NVIDIA drivers and xserver-xorg are installed. tests directory # Includes unit tests for allenact . utils directory # It includes different types of utilities, mainly divided into: utils.experiment_utils , including the TrainingPipeline , PipelineStage and other utilities to configure an experiment. utils.model_utils , including generic CNN creation, forward-pass helpers and other utilities. utils.tensor_utils , including functions to batch observations, convert tensors into video, scale image tensors, etc. utils.viz_utils , including a VizSuite class that can be instantiated with different visualization plugins during inference. utils.system , including logging and networking helpers. Other utils files, including utils.misc_utils , contain a number of helper functions for different purposes.","title":"Structure of the codebase"},{"location":"getting_started/structure/#structure-of-the-codebase","text":"The codebase consists of the following directories: core , datasets , docs , overrides , plugins , pretrained_model_ckpts , projects , scripts , tests and utils . Below, we explain the overall structure and how different components of the codebase are organized.","title":"Structure of the codebase"},{"location":"getting_started/structure/#core-directory","text":"Contains runtime algorithms for on-policy and off-policy training and inference, base abstractions used throughout the code base and basic models to be used as building blocks in future models. core.algorithms includes on-policy and off-policy training nd inference algorithms and abstractions for losses, policies, rollout storage, etc. core.base_abstractions includes the base ExperimentConfig , distributions, base Sensor , TaskSampler , Task , etc. core.models includes basic CNN, and RNN state encoders, besides basic ActorCriticModel implementations.","title":"core directory"},{"location":"getting_started/structure/#datasets-directory","text":"A directory made to store task-specific datasets. For example, the script datasets/download_navigation_datasets.sh can be used to automatically download task dataset files for Point Navigation within the RoboTHOR environment and it will place these files into a new datasets/robothor-pointnav directory.","title":"datasets directory"},{"location":"getting_started/structure/#docs-directory","text":"Contains documentation for the framework, including guides for installation and first experiments, how-to's for the definition and usage of different abstractions, tutorials and per-project documentation.","title":"docs directory"},{"location":"getting_started/structure/#overrides-directory","text":"Files within this directory are used to the look and structure of the documentation generated when running mkdocs . See our FAQ for information on how to generate this documentation for yourself.","title":"overrides directory"},{"location":"getting_started/structure/#plugins-directory","text":"Contains implementations of ActorCriticModel s and Task s in different environments. Each plugin folder is named as {environment}_plugin and contains three subfolders: configs to host useful configuration for the environment or tasks. data to store data to be consumed by the environment or tasks. scripts to setup the plugin or gather and process data.","title":"plugins directory"},{"location":"getting_started/structure/#pretrained_model_ckpts-directory","text":"Directory into which pretrained model checkpoints will be saved. See also the pretrained_model_ckpts/download_navigation_model_ckpts.sh which can be used to download such checkpoints.","title":"pretrained_model_ckpts directory"},{"location":"getting_started/structure/#projects-directory","text":"Contains project-specific code like experiment configurations and scripts to process results, generate visualizations or prepare data.","title":"projects directory"},{"location":"getting_started/structure/#scripts-directory","text":"Includes framework-wide scripts to build the documentation, format code, run_tests and start an xserver. The latter can be used for OpenGL-based environments having super-user privileges in Linux, assuming NVIDIA drivers and xserver-xorg are installed.","title":"scripts directory"},{"location":"getting_started/structure/#tests-directory","text":"Includes unit tests for allenact .","title":"tests directory"},{"location":"getting_started/structure/#utils-directory","text":"It includes different types of utilities, mainly divided into: utils.experiment_utils , including the TrainingPipeline , PipelineStage and other utilities to configure an experiment. utils.model_utils , including generic CNN creation, forward-pass helpers and other utilities. utils.tensor_utils , including functions to batch observations, convert tensors into video, scale image tensors, etc. utils.viz_utils , including a VizSuite class that can be instantiated with different visualization plugins during inference. utils.system , including logging and networking helpers. Other utils files, including utils.misc_utils , contain a number of helper functions for different purposes.","title":"utils directory"},{"location":"howtos/changing-rewards-and-losses/","text":"Changing rewards and losses # In order to train actor-critic agents, we need to specify rewards at the task level, and losses at the training pipeline level. Rewards # We will use the object navigation task in iTHOR as a running example. We can see how the ObjectNavTask._step(self, action: int) -> RLStepResult method computes the reward for the latest action by invoking a function like: def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Any reward shaping can be easily added by e.g. modifying the definition of an existing class: class NavigationWithShaping ( plugins . ithor_plugin . ithor_tasks . ObjectNavTask ): def judge ( self ) -> float : reward = super () . judge () if self . previous_state is not None : reward += float ( my_reward_shaping_function ( self . previous_state , self . current_state , )) self . previous_state = self . current_state return reward Losses # We support A2C , PPO , and imitation losses amongst others. We can easily include DAgger or variations thereof by assuming the availability of an expert providing optimal actions to agents and combining imitation and PPO losses in different ways through multiple stages: class MyExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) ppo_steps = int ( 3e4 ) ppo_steps2 = int ( 1e6 ) ... return utils . experiment_utils . TrainingPipeline ( named_losses = { \"imitation_loss\" : core . algorithms . onpolicy_sync . losses . imitation . Imitation (), \"ppo_loss\" : core . algorithms . onpolicy_sync . losses . ppo . PPO ( ** core . algorithms . onpolicy_sync . losses . ppo . PPOConfig , ), }, ... pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ \"imitation_loss\" , \"ppo_loss\" ], teacher_forcing = utils . experiment_utils . LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = ppo_steps ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps2 , ), ], )","title":"Change rewards and losses"},{"location":"howtos/changing-rewards-and-losses/#changing-rewards-and-losses","text":"In order to train actor-critic agents, we need to specify rewards at the task level, and losses at the training pipeline level.","title":"Changing rewards and losses"},{"location":"howtos/changing-rewards-and-losses/#rewards","text":"We will use the object navigation task in iTHOR as a running example. We can see how the ObjectNavTask._step(self, action: int) -> RLStepResult method computes the reward for the latest action by invoking a function like: def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Any reward shaping can be easily added by e.g. modifying the definition of an existing class: class NavigationWithShaping ( plugins . ithor_plugin . ithor_tasks . ObjectNavTask ): def judge ( self ) -> float : reward = super () . judge () if self . previous_state is not None : reward += float ( my_reward_shaping_function ( self . previous_state , self . current_state , )) self . previous_state = self . current_state return reward","title":"Rewards"},{"location":"howtos/changing-rewards-and-losses/#losses","text":"We support A2C , PPO , and imitation losses amongst others. We can easily include DAgger or variations thereof by assuming the availability of an expert providing optimal actions to agents and combining imitation and PPO losses in different ways through multiple stages: class MyExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) ppo_steps = int ( 3e4 ) ppo_steps2 = int ( 1e6 ) ... return utils . experiment_utils . TrainingPipeline ( named_losses = { \"imitation_loss\" : core . algorithms . onpolicy_sync . losses . imitation . Imitation (), \"ppo_loss\" : core . algorithms . onpolicy_sync . losses . ppo . PPO ( ** core . algorithms . onpolicy_sync . losses . ppo . PPOConfig , ), }, ... pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ \"imitation_loss\" , \"ppo_loss\" ], teacher_forcing = utils . experiment_utils . LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = ppo_steps ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps2 , ), ], )","title":"Losses"},{"location":"howtos/defining-a-new-model/","text":"Defining a new model # All actor-critic models must implement the interface described by the ActorCriticModel class . This interface includes two methods that need to be implemented: recurrent_memory_specification , returning a description of the model's recurrent memory; and forward , returning an ActorCriticOutput given the current observation, hidden state and previous actions. For convenience, we provide a recurrent network module and a simple CNN module from the Habitat baseline navigation models, that will be used in this example. Actor-critic model interface # As an example, let's build an object navigation agent. class ObjectNavBaselineActorCritic ( ActorCriticModel [ CategoricalDistr ]): \"\"\"Baseline recurrent actor critic model for object-navigation. # Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type `gym.spaces.Discrete`). observation_space : The observation space expected by the agent. This observation space should include (optionally) 'rgb' images and 'depth' images and is required to have a component corresponding to the goal `goal_sensor_uuid`. goal_sensor_uuid : The uuid of the sensor of the goal object. See `GoalObjectTypeThorSensor` as an example of such a sensor. hidden_size : The hidden size of the GRU RNN. object_type_embedding_dim: The dimensionality of the embedding corresponding to the goal object type. \"\"\" def __init__ ( self , action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , trainable_masked_hidden_state : bool = False , num_rnn_layers = 1 , rnn_type = \"GRU\" , ): \"\"\"Initializer. See class documentation for parameter definitions. \"\"\" super () . __init__ ( action_space = action_space , observation_space = observation_space ) self . goal_sensor_uuid = goal_sensor_uuid self . _n_object_types = self . observation_space . spaces [ self . goal_sensor_uuid ] . n self . _hidden_size = hidden_size self . object_type_embedding_size = object_type_embedding_dim self . visual_encoder = SimpleCNN ( self . observation_space , self . _hidden_size ) self . state_encoder = RNNStateEncoder ( ( 0 if self . is_blind else self . _hidden_size ) + object_type_embedding_dim , self . _hidden_size , trainable_masked_hidden_state = trainable_masked_hidden_state , num_layers = num_rnn_layers , rnn_type = rnn_type , ) self . actor = LinearActorHead ( self . _hidden_size , action_space . n ) self . critic = LinearCriticHead ( self . _hidden_size ) self . object_type_embedding = nn . Embedding ( num_embeddings = self . _n_object_types , embedding_dim = object_type_embedding_dim , ) self . train () @property def recurrent_hidden_state_size ( self ) -> int : \"\"\"The recurrent hidden state size of the model.\"\"\" return self . _hidden_size @property def is_blind ( self ) -> bool : \"\"\"True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).\"\"\" return self . visual_encoder . is_blind @property def num_recurrent_layers ( self ) -> int : \"\"\"Number of recurrent hidden layers.\"\"\" return self . state_encoder . num_recurrent_layers def _recurrent_memory_specification ( self ): return dict ( rnn = ( ( ( \"layer\" , self . num_recurrent_layers ), ( \"sampler\" , None ), ( \"hidden\" , self . recurrent_hidden_state_size ), ), torch . float32 , ) ) def get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ] ) -> torch . FloatTensor : \"\"\"Get the object type encoding from input batched observations.\"\"\" return self . object_type_embedding ( # type:ignore observations [ self . goal_sensor_uuid ] . to ( torch . int64 ) ) def forward ( # type:ignore self , observations : ObservationType , memory : Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor , ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]]: \"\"\"Processes input batched observations to produce new actor and critic values. Processes input batched observations (along with prior hidden states, previous actions, and masks denoting which recurrent hidden states should be masked) and returns an `ActorCriticOutput` object containing the model's policy (distribution over actions) and evaluation of the current state (value). # Parameters observations : Batched input observations. memory : `Memory` containing the hidden states from initial timepoints. prev_actions : Tensor of previous actions taken. masks : Masks applied to hidden states. See `RNNStateEncoder`. # Returns Tuple of the `ActorCriticOutput` and recurrent hidden state. \"\"\" target_encoding = self . get_object_type_encoding ( cast ( Dict [ str , torch . FloatTensor ], observations ) ) x = [ target_encoding ] if not self . is_blind : perception_embed = self . visual_encoder ( observations ) x = [ perception_embed ] + x x_cat = torch . cat ( x , dim =- 1 ) # type: ignore x_out , rnn_hidden_states = self . state_encoder ( x_cat , memory . tensor ( \"rnn\" ), masks ) return ( ActorCriticOutput ( distributions = self . actor ( x_out ), values = self . critic ( x_out ), extras = {} ), memory . set_tensor ( \"rnn\" , rnn_hidden_states ), )","title":"Define a new model"},{"location":"howtos/defining-a-new-model/#defining-a-new-model","text":"All actor-critic models must implement the interface described by the ActorCriticModel class . This interface includes two methods that need to be implemented: recurrent_memory_specification , returning a description of the model's recurrent memory; and forward , returning an ActorCriticOutput given the current observation, hidden state and previous actions. For convenience, we provide a recurrent network module and a simple CNN module from the Habitat baseline navigation models, that will be used in this example.","title":"Defining a new model"},{"location":"howtos/defining-a-new-model/#actor-critic-model-interface","text":"As an example, let's build an object navigation agent. class ObjectNavBaselineActorCritic ( ActorCriticModel [ CategoricalDistr ]): \"\"\"Baseline recurrent actor critic model for object-navigation. # Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type `gym.spaces.Discrete`). observation_space : The observation space expected by the agent. This observation space should include (optionally) 'rgb' images and 'depth' images and is required to have a component corresponding to the goal `goal_sensor_uuid`. goal_sensor_uuid : The uuid of the sensor of the goal object. See `GoalObjectTypeThorSensor` as an example of such a sensor. hidden_size : The hidden size of the GRU RNN. object_type_embedding_dim: The dimensionality of the embedding corresponding to the goal object type. \"\"\" def __init__ ( self , action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , trainable_masked_hidden_state : bool = False , num_rnn_layers = 1 , rnn_type = \"GRU\" , ): \"\"\"Initializer. See class documentation for parameter definitions. \"\"\" super () . __init__ ( action_space = action_space , observation_space = observation_space ) self . goal_sensor_uuid = goal_sensor_uuid self . _n_object_types = self . observation_space . spaces [ self . goal_sensor_uuid ] . n self . _hidden_size = hidden_size self . object_type_embedding_size = object_type_embedding_dim self . visual_encoder = SimpleCNN ( self . observation_space , self . _hidden_size ) self . state_encoder = RNNStateEncoder ( ( 0 if self . is_blind else self . _hidden_size ) + object_type_embedding_dim , self . _hidden_size , trainable_masked_hidden_state = trainable_masked_hidden_state , num_layers = num_rnn_layers , rnn_type = rnn_type , ) self . actor = LinearActorHead ( self . _hidden_size , action_space . n ) self . critic = LinearCriticHead ( self . _hidden_size ) self . object_type_embedding = nn . Embedding ( num_embeddings = self . _n_object_types , embedding_dim = object_type_embedding_dim , ) self . train () @property def recurrent_hidden_state_size ( self ) -> int : \"\"\"The recurrent hidden state size of the model.\"\"\" return self . _hidden_size @property def is_blind ( self ) -> bool : \"\"\"True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).\"\"\" return self . visual_encoder . is_blind @property def num_recurrent_layers ( self ) -> int : \"\"\"Number of recurrent hidden layers.\"\"\" return self . state_encoder . num_recurrent_layers def _recurrent_memory_specification ( self ): return dict ( rnn = ( ( ( \"layer\" , self . num_recurrent_layers ), ( \"sampler\" , None ), ( \"hidden\" , self . recurrent_hidden_state_size ), ), torch . float32 , ) ) def get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ] ) -> torch . FloatTensor : \"\"\"Get the object type encoding from input batched observations.\"\"\" return self . object_type_embedding ( # type:ignore observations [ self . goal_sensor_uuid ] . to ( torch . int64 ) ) def forward ( # type:ignore self , observations : ObservationType , memory : Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor , ) -> Tuple [ ActorCriticOutput [ DistributionType ], Optional [ Memory ]]: \"\"\"Processes input batched observations to produce new actor and critic values. Processes input batched observations (along with prior hidden states, previous actions, and masks denoting which recurrent hidden states should be masked) and returns an `ActorCriticOutput` object containing the model's policy (distribution over actions) and evaluation of the current state (value). # Parameters observations : Batched input observations. memory : `Memory` containing the hidden states from initial timepoints. prev_actions : Tensor of previous actions taken. masks : Masks applied to hidden states. See `RNNStateEncoder`. # Returns Tuple of the `ActorCriticOutput` and recurrent hidden state. \"\"\" target_encoding = self . get_object_type_encoding ( cast ( Dict [ str , torch . FloatTensor ], observations ) ) x = [ target_encoding ] if not self . is_blind : perception_embed = self . visual_encoder ( observations ) x = [ perception_embed ] + x x_cat = torch . cat ( x , dim =- 1 ) # type: ignore x_out , rnn_hidden_states = self . state_encoder ( x_cat , memory . tensor ( \"rnn\" ), masks ) return ( ActorCriticOutput ( distributions = self . actor ( x_out ), values = self . critic ( x_out ), extras = {} ), memory . set_tensor ( \"rnn\" , rnn_hidden_states ), )","title":"Actor-critic model interface"},{"location":"howtos/defining-a-new-task/","text":"Defining a new task # In order to use new tasks in our experiments, we need to define two classes: A Task , including, among others, a step implementation providing a RLStepResult , a metrics method providing quantitative performance measurements for agents and, optionally, a query_expert method that can be used e.g. with an imitation loss during training. A TaskSampler , that allows instantiating new Tasks for the agents to solve during training, validation and testing. Task # Let's define a semantic navigation task, where agents have to navigate from a starting point in an environment to an object of a specific class using a minimal amount of steps and deciding when the goal has been reached. We need to define the methods action_space , render , _step , reached_terminal_state , class_action_names , close , metrics , and query_expert from the base Task definition. Initialization, action space and termination # Let's start with the definition of the action space and task initialization: ... from plugins.ithor_plugin.ithor_constants import ( MOVE_AHEAD , ROTATE_LEFT , ROTATE_RIGHT , LOOK_DOWN , LOOK_UP , END , ) ... class ObjectNavTask ( Task [ IThorEnvironment ]): _actions = ( MOVE_AHEAD , ROTATE_LEFT , ROTATE_RIGHT , LOOK_DOWN , LOOK_UP , END ) def __init__ ( self , env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) -> None : super () . __init__ ( env = env , sensors = sensors , task_info = task_info , max_steps = max_steps , ** kwargs ) self . _took_end_action : bool = False self . _success : Optional [ bool ] = False @property def action_space ( self ): return gym . spaces . Discrete ( len ( self . _actions )) @classmethod def class_action_names ( cls ) -> Tuple [ str , ... ]: return cls . _actions def reached_terminal_state ( self ) -> bool : return self . _took_end_action def close ( self ) -> None : self . env . stop () ... Step method # Next, we define the main method _step that will be called every time the agent produces a new action: class ObjectNavTask ( Task [ IThorEnvironment ]): ... def _step ( self , action : Union [ int , Sequence [ int ]]) -> RLStepResult : assert isinstance ( action , int ) action = cast ( int , action ) action_str = self . class_action_names ()[ action ] if action_str == END : self . _took_end_action = True self . _success = self . _is_goal_object_visible () self . last_action_success = self . _success else : self . env . step ({ \"action\" : action_str }) self . last_action_success = self . env . last_action_success step_result = RLStepResult ( observation = self . get_observations (), reward = self . judge (), done = self . is_done (), info = { \"last_action_success\" : self . last_action_success }, ) return step_result ... def _is_goal_object_visible ( self ) -> bool : return any ( o [ \"objectType\" ] == self . task_info [ \"object_type\" ] for o in self . env . visible_objects () ) def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Metrics, rendering and expert actions # Finally, we define methods to render and evaluate the current task, and optionally generate expert actions to be used e.g. for DAgger training. def render ( self , mode : str = \"rgb\" , * args , ** kwargs ) -> numpy . ndarray : assert mode == \"rgb\" , \"only rgb rendering is implemented\" return self . env . current_frame def metrics ( self ) -> Dict [ str , Any ]: if not self . is_done (): return {} else : return { \"success\" : self . _success , \"ep_length\" : self . num_steps_taken ()} def query_expert ( self , ** kwargs ) -> Tuple [ int , bool ]: return my_objnav_expert_implementation ( self ) TaskSampler # We also need to define the corresponding TaskSampler, which must contain implementations for methods __len__ , total_unique , last_sampled_task , next_task , close , reset , and set_seed . Currently, an additional method all_observation_spaces_equal is used to ensure compatibility with the current RolloutStorage . Let's define a tasks sampler able to provide an infinite number of object navigation tasks for AI2-THOR. Initialization and termination # class ObjectNavTaskSampler ( TaskSampler ): def __init__ ( self , scenes : List [ str ], object_types : str , sensors : List [ Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . Space , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , * args , ** kwargs ) -> None : self . env_args = env_args self . scenes = scenes self . object_types = object_types self . grid_size = 0.25 self . env : Optional [ IThorEnvironment ] = None self . sensors = sensors self . max_steps = max_steps self . _action_sapce = action_space self . scene_id : Optional [ int ] = None self . _last_sampled_task : Optional [ ObjectNavTask ] = None set_seed ( seed ) self . reset () def close ( self ) -> None : if self . env is not None : self . env . stop () def reset ( self ): self . scene_id = 0 def _create_environment ( self ) -> IThorEnvironment : env = IThorEnvironment ( make_agents_visible = False , object_open_speed = 0.05 , restrict_to_initially_reachable_points = True , ** self . env_args , ) return env Task sampling # Finally, we need to define methods to determine the number of available tasks (possibly infinite) and sample tasks: @property def length ( self ) -> Union [ int , float ]: return float ( \"inf\" ) @property def total_unique ( self ) -> Optional [ Union [ int , float ]]: return None @property def last_sampled_task ( self ) -> Optional [ ObjectNavTask ]: return self . _last_sampled_task @property def all_observation_spaces_equal ( self ) -> bool : return True def next_task ( self ) -> Optional [ ObjectNavTask ]: self . scene_id = random . randint ( 0 , len ( self . scenes ) - 1 ) self . scene = self . scenes [ self . scene_id ] if self . env is not None : if scene != self . env . scene_name : self . env . reset ( scene ) else : self . env = self . _create_environment () self . env . reset ( scene_name = scene ) self . env . randomize_agent_location () task_info = { \"object_type\" : random . sample ( self . object_types , 1 )} self . _last_sampled_task = ObjectNavTask ( env = self . env , sensors = self . sensors , task_info = task_info , max_steps = self . max_steps , action_space = self . _action_sapce , ) return self . _last_sampled_task","title":"Define a new task"},{"location":"howtos/defining-a-new-task/#defining-a-new-task","text":"In order to use new tasks in our experiments, we need to define two classes: A Task , including, among others, a step implementation providing a RLStepResult , a metrics method providing quantitative performance measurements for agents and, optionally, a query_expert method that can be used e.g. with an imitation loss during training. A TaskSampler , that allows instantiating new Tasks for the agents to solve during training, validation and testing.","title":"Defining a new task"},{"location":"howtos/defining-a-new-task/#task","text":"Let's define a semantic navigation task, where agents have to navigate from a starting point in an environment to an object of a specific class using a minimal amount of steps and deciding when the goal has been reached. We need to define the methods action_space , render , _step , reached_terminal_state , class_action_names , close , metrics , and query_expert from the base Task definition.","title":"Task"},{"location":"howtos/defining-a-new-task/#initialization-action-space-and-termination","text":"Let's start with the definition of the action space and task initialization: ... from plugins.ithor_plugin.ithor_constants import ( MOVE_AHEAD , ROTATE_LEFT , ROTATE_RIGHT , LOOK_DOWN , LOOK_UP , END , ) ... class ObjectNavTask ( Task [ IThorEnvironment ]): _actions = ( MOVE_AHEAD , ROTATE_LEFT , ROTATE_RIGHT , LOOK_DOWN , LOOK_UP , END ) def __init__ ( self , env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) -> None : super () . __init__ ( env = env , sensors = sensors , task_info = task_info , max_steps = max_steps , ** kwargs ) self . _took_end_action : bool = False self . _success : Optional [ bool ] = False @property def action_space ( self ): return gym . spaces . Discrete ( len ( self . _actions )) @classmethod def class_action_names ( cls ) -> Tuple [ str , ... ]: return cls . _actions def reached_terminal_state ( self ) -> bool : return self . _took_end_action def close ( self ) -> None : self . env . stop () ...","title":"Initialization, action space and termination"},{"location":"howtos/defining-a-new-task/#step-method","text":"Next, we define the main method _step that will be called every time the agent produces a new action: class ObjectNavTask ( Task [ IThorEnvironment ]): ... def _step ( self , action : Union [ int , Sequence [ int ]]) -> RLStepResult : assert isinstance ( action , int ) action = cast ( int , action ) action_str = self . class_action_names ()[ action ] if action_str == END : self . _took_end_action = True self . _success = self . _is_goal_object_visible () self . last_action_success = self . _success else : self . env . step ({ \"action\" : action_str }) self . last_action_success = self . env . last_action_success step_result = RLStepResult ( observation = self . get_observations (), reward = self . judge (), done = self . is_done (), info = { \"last_action_success\" : self . last_action_success }, ) return step_result ... def _is_goal_object_visible ( self ) -> bool : return any ( o [ \"objectType\" ] == self . task_info [ \"object_type\" ] for o in self . env . visible_objects () ) def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward )","title":"Step method"},{"location":"howtos/defining-a-new-task/#metrics-rendering-and-expert-actions","text":"Finally, we define methods to render and evaluate the current task, and optionally generate expert actions to be used e.g. for DAgger training. def render ( self , mode : str = \"rgb\" , * args , ** kwargs ) -> numpy . ndarray : assert mode == \"rgb\" , \"only rgb rendering is implemented\" return self . env . current_frame def metrics ( self ) -> Dict [ str , Any ]: if not self . is_done (): return {} else : return { \"success\" : self . _success , \"ep_length\" : self . num_steps_taken ()} def query_expert ( self , ** kwargs ) -> Tuple [ int , bool ]: return my_objnav_expert_implementation ( self )","title":"Metrics, rendering and expert actions"},{"location":"howtos/defining-a-new-task/#tasksampler","text":"We also need to define the corresponding TaskSampler, which must contain implementations for methods __len__ , total_unique , last_sampled_task , next_task , close , reset , and set_seed . Currently, an additional method all_observation_spaces_equal is used to ensure compatibility with the current RolloutStorage . Let's define a tasks sampler able to provide an infinite number of object navigation tasks for AI2-THOR.","title":"TaskSampler"},{"location":"howtos/defining-a-new-task/#initialization-and-termination","text":"class ObjectNavTaskSampler ( TaskSampler ): def __init__ ( self , scenes : List [ str ], object_types : str , sensors : List [ Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . Space , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , * args , ** kwargs ) -> None : self . env_args = env_args self . scenes = scenes self . object_types = object_types self . grid_size = 0.25 self . env : Optional [ IThorEnvironment ] = None self . sensors = sensors self . max_steps = max_steps self . _action_sapce = action_space self . scene_id : Optional [ int ] = None self . _last_sampled_task : Optional [ ObjectNavTask ] = None set_seed ( seed ) self . reset () def close ( self ) -> None : if self . env is not None : self . env . stop () def reset ( self ): self . scene_id = 0 def _create_environment ( self ) -> IThorEnvironment : env = IThorEnvironment ( make_agents_visible = False , object_open_speed = 0.05 , restrict_to_initially_reachable_points = True , ** self . env_args , ) return env","title":"Initialization and termination"},{"location":"howtos/defining-a-new-task/#task-sampling","text":"Finally, we need to define methods to determine the number of available tasks (possibly infinite) and sample tasks: @property def length ( self ) -> Union [ int , float ]: return float ( \"inf\" ) @property def total_unique ( self ) -> Optional [ Union [ int , float ]]: return None @property def last_sampled_task ( self ) -> Optional [ ObjectNavTask ]: return self . _last_sampled_task @property def all_observation_spaces_equal ( self ) -> bool : return True def next_task ( self ) -> Optional [ ObjectNavTask ]: self . scene_id = random . randint ( 0 , len ( self . scenes ) - 1 ) self . scene = self . scenes [ self . scene_id ] if self . env is not None : if scene != self . env . scene_name : self . env . reset ( scene ) else : self . env = self . _create_environment () self . env . reset ( scene_name = scene ) self . env . randomize_agent_location () task_info = { \"object_type\" : random . sample ( self . object_types , 1 )} self . _last_sampled_task = ObjectNavTask ( env = self . env , sensors = self . sensors , task_info = task_info , max_steps = self . max_steps , action_space = self . _action_sapce , ) return self . _last_sampled_task","title":"Task sampling"},{"location":"howtos/defining-a-new-training-pipeline/","text":"Defining a new training pipeline # Defining a new training pipeline, or even new learning algorithms, is straightforward with the modular design in AllenAct . A convenience Builder object allows us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. On-policy # We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 2 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) ... Alternatively, we could use a more complex pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows: class ObjectNavThorDaggerThenPPOExperimentConfig ( ExperimentConfig ): ... SENSORS = [ ... ExpertActionSensor ( nactions = 6 ), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 1e4 ) # Much smaller number of steps as we're using imitation learning ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 1 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), \"imitation_loss\" : Imitation (), # We add an imitation loss. }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) Off-policy # We can also define off-policy stages where an external dataset is used, in this case, for Behavior Cloning: class BCOffPolicyBabyAIGoToLocalExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = int ( 1e7 ) num_steps = 128 return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 0 , # no on-policy training update_repeats = 0 , # no on-policy training num_steps = num_steps // 4 , # rollouts from environment tasks named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss ( total_episodes_in_epoch = int ( 1e6 ) # dataset contains 1M episodes ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # We only train from off-policy data: offpolicy_component = OffPolicyPipelineComponent ( data_iterator_builder = lambda ** kwargs : create_minigrid_offpolicy_data_iterator ( path = DATASET_PATH , # external dataset nrollouts = 128 , # per trainer batch size rollout_len = num_steps , # For truncated-BPTT instr_len = 5 , ** kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = 16 , # 16 batches per rollout ), ), ], ) Note that, in this example, 128 / 4 = 32 steps will be sampled from tasks in a MiniGrid environment (which can be useful to track the agent's performance), while a subgraph of the model (in this case the entire Actor) is trained from batches of 128-step truncated episodes sampled from an offline dataset stored under DATASET_PATH .","title":"Define a new training pipeline"},{"location":"howtos/defining-a-new-training-pipeline/#defining-a-new-training-pipeline","text":"Defining a new training pipeline, or even new learning algorithms, is straightforward with the modular design in AllenAct . A convenience Builder object allows us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers.","title":"Defining a new training pipeline"},{"location":"howtos/defining-a-new-training-pipeline/#on-policy","text":"We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 2 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) ... Alternatively, we could use a more complex pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows: class ObjectNavThorDaggerThenPPOExperimentConfig ( ExperimentConfig ): ... SENSORS = [ ... ExpertActionSensor ( nactions = 6 ), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 1e4 ) # Much smaller number of steps as we're using imitation learning ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 1 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), \"imitation_loss\" : Imitation (), # We add an imitation loss. }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), )","title":"On-policy"},{"location":"howtos/defining-a-new-training-pipeline/#off-policy","text":"We can also define off-policy stages where an external dataset is used, in this case, for Behavior Cloning: class BCOffPolicyBabyAIGoToLocalExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = int ( 1e7 ) num_steps = 128 return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 0 , # no on-policy training update_repeats = 0 , # no on-policy training num_steps = num_steps // 4 , # rollouts from environment tasks named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss ( total_episodes_in_epoch = int ( 1e6 ) # dataset contains 1M episodes ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # We only train from off-policy data: offpolicy_component = OffPolicyPipelineComponent ( data_iterator_builder = lambda ** kwargs : create_minigrid_offpolicy_data_iterator ( path = DATASET_PATH , # external dataset nrollouts = 128 , # per trainer batch size rollout_len = num_steps , # For truncated-BPTT instr_len = 5 , ** kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = 16 , # 16 batches per rollout ), ), ], ) Note that, in this example, 128 / 4 = 32 steps will be sampled from tasks in a MiniGrid environment (which can be useful to track the agent's performance), while a subgraph of the model (in this case the entire Actor) is trained from batches of 128-step truncated episodes sampled from an offline dataset stored under DATASET_PATH .","title":"Off-policy"},{"location":"howtos/defining-an-experiment/","text":"Defining an experiment # Let's look at an example experiment configuration for an object navigation example with an actor-critic agent observing RGB images from the environment and target object classes from the task. This is a simplified example where the agent is confined to a single iTHOR scene ( FloorPlan1 ) and needs to find a single object (a tomato). To see how one might running a \"full\"/\"hard\" version of navigation within AI2-THOR, see our tutorials PointNav in RoboTHOR and Swapping in a new environment . The interface to be implemented by the experiment specification is defined in core.base_abstractions.experiment_config . If you'd like to skip ahead and see the finished configuration, see here . We begin by making the following imports: from math import ceil from typing import Dict , Any , List , Optional import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from core.algorithms.onpolicy_sync.losses import PPO from core.algorithms.onpolicy_sync.losses.ppo import PPOConfig from core.base_abstractions.experiment_config import ExperimentConfig from core.base_abstractions.sensor import SensorSuite from core.base_abstractions.task import TaskSampler from plugins.ithor_plugin.ithor_sensors import RGBSensorThor , GoalObjectTypeThorSensor from plugins.ithor_plugin.ithor_task_samplers import ObjectNavTaskSampler from plugins.ithor_plugin.ithor_tasks import ObjectNavTask from projects.objectnav_baselines.models.object_nav_models import ( ObjectNavBaselineActorCritic , ) from utils.experiment_utils import Builder , PipelineStage , TrainingPipeline , LinearDecay Now first method to implement is tag , which provides a string identifying the experiment: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def tag ( cls ): return \"ObjectNavThorPPO\" ... Model creation # Next, create_model will be used to instantiate an baseline object navigation actor-critic model : class ObjectNavThorExperimentConfig ( ExperimentConfig ): ... # A simple setting, train/valid/test are all the same single scene # and we're looking for a single object OBJECT_TYPES = [ \"Tomato\" ] TRAIN_SCENES = [ \"FloorPlan1_physics\" ] VALID_SCENES = [ \"FloorPlan1_physics\" ] TEST_SCENES = [ \"FloorPlan1_physics\" ] # Setting up sensors and basic environment details SCREEN_SIZE = 224 SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), GoalObjectTypeThorSensor ( object_types = OBJECT_TYPES ), ] ... @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ObjectNavBaselineActorCritic ( action_space = gym . spaces . Discrete ( len ( ObjectNavTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , goal_sensor_uuid = \"goal_object_type_ind\" , hidden_size = 512 , object_type_embedding_dim = 8 , ) ... Training pipeline # We now implement a training pipeline which trains with a single stage using PPO. In the below we use Builder objects, which allow us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. This is necessary when instantiating things like PyTorch optimizers who take as input the list of parameters associated with our agent's model (something we can't know until the create_model function has been called). class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 2 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) ... Alternatively, we could use a more sophisticated pipeline that begins training with dataset aggregation ( DAgger ) before moving to training with PPO. This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows class ObjectNavThorDaggerThenPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ): ... SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), GoalObjectTypeThorSensor ( object_types = OBJECT_TYPES ), ExpertActionSensor ( nactions = 6 ), # Notice that we have added an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 1e4 ) # Much smaller number of steps as we're using imitation learning ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 1 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), \"imitation_loss\" : Imitation (), # We add an imitation loss. }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) A version of our experiment config file for which we have implemented this two-stage training can be found here . This two-stage configuration ObjectNavThorDaggerThenPPOExperimentConfig is actually implemented as a subclass of ObjectNavThorPPOExperimentConfig . This is a common pattern used in AllenAct and lets one skip a great deal of boilerplate when defining a new experiment as a slight modification of an old one. Of course one must then be careful: changes to the superclass configuration will propagate to all subclassed configurations. Machine configuration # In machine_params we define machine configuration parameters that will be used for training, validation and test: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ): num_gpus = torch . cuda . device_count () has_gpu = num_gpus != 0 if mode == \"train\" : nprocesses = 20 if has_gpu else 4 gpu_ids = [ 0 ] if has_gpu else [] elif mode == \"valid\" : nprocesses = 1 gpu_ids = [ 1 % num_gpus ] if has_gpu else [] elif mode == \"test\" : nprocesses = 1 gpu_ids = [ 0 ] if has_gpu else [] else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids } ... In the above we use the availability of cuda ( torch.cuda.device_count() != 0 ) to determine whether we should use parameters appropriate for local machines or for a server. We might optionally add a list of sampler_devices to assign devices (likely those not used for running our agent) to task sampling workers. Task sampling # The above has defined the model we'd like to use, the types of losses we wish to use during training, and the machine specific parameters that should be used during training. Critically we have not yet defined which task we wish to train our agent to complete. This is done by implementing the ExperimentConfig.make_sampler_fn function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return ObjectNavTaskSampler ( ** kwargs ) ... Now, before training starts, our trainer will know to generate a collection of task samplers using make_sampler_fn for training (and possibly validation or testing). The kwargs parameters used in the above function call can be different for each training process, we implement such differences using the ExperimentConfig.train_task_sampler_args function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_SCENES , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_period\" ] = \"manual\" res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if len ( devices ) > 0 else None ) return res ... Now training process i out of n total processes will be instantiated with the parameters ObjectNavThorPPOExperimentConfig.train_task_sampler_args(i, n, ...) . Similar functions ( valid_task_sampler_args and test_task_sampler_args ) exist for generating validation and test parameters. Note also that with this function we can assign devices to run our environment for each worker. See the documentation of ExperimentConfig for more information. Running the experiment # We are now in the position to run the experiment (with seed 12345) using the command python main.py object_nav_ithor_ppo_one_object -b projects/tutorials -s 12345","title":"Define an experiment"},{"location":"howtos/defining-an-experiment/#defining-an-experiment","text":"Let's look at an example experiment configuration for an object navigation example with an actor-critic agent observing RGB images from the environment and target object classes from the task. This is a simplified example where the agent is confined to a single iTHOR scene ( FloorPlan1 ) and needs to find a single object (a tomato). To see how one might running a \"full\"/\"hard\" version of navigation within AI2-THOR, see our tutorials PointNav in RoboTHOR and Swapping in a new environment . The interface to be implemented by the experiment specification is defined in core.base_abstractions.experiment_config . If you'd like to skip ahead and see the finished configuration, see here . We begin by making the following imports: from math import ceil from typing import Dict , Any , List , Optional import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from core.algorithms.onpolicy_sync.losses import PPO from core.algorithms.onpolicy_sync.losses.ppo import PPOConfig from core.base_abstractions.experiment_config import ExperimentConfig from core.base_abstractions.sensor import SensorSuite from core.base_abstractions.task import TaskSampler from plugins.ithor_plugin.ithor_sensors import RGBSensorThor , GoalObjectTypeThorSensor from plugins.ithor_plugin.ithor_task_samplers import ObjectNavTaskSampler from plugins.ithor_plugin.ithor_tasks import ObjectNavTask from projects.objectnav_baselines.models.object_nav_models import ( ObjectNavBaselineActorCritic , ) from utils.experiment_utils import Builder , PipelineStage , TrainingPipeline , LinearDecay Now first method to implement is tag , which provides a string identifying the experiment: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def tag ( cls ): return \"ObjectNavThorPPO\" ...","title":"Defining an  experiment"},{"location":"howtos/defining-an-experiment/#model-creation","text":"Next, create_model will be used to instantiate an baseline object navigation actor-critic model : class ObjectNavThorExperimentConfig ( ExperimentConfig ): ... # A simple setting, train/valid/test are all the same single scene # and we're looking for a single object OBJECT_TYPES = [ \"Tomato\" ] TRAIN_SCENES = [ \"FloorPlan1_physics\" ] VALID_SCENES = [ \"FloorPlan1_physics\" ] TEST_SCENES = [ \"FloorPlan1_physics\" ] # Setting up sensors and basic environment details SCREEN_SIZE = 224 SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), GoalObjectTypeThorSensor ( object_types = OBJECT_TYPES ), ] ... @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ObjectNavBaselineActorCritic ( action_space = gym . spaces . Discrete ( len ( ObjectNavTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , goal_sensor_uuid = \"goal_object_type_ind\" , hidden_size = 512 , object_type_embedding_dim = 8 , ) ...","title":"Model creation"},{"location":"howtos/defining-an-experiment/#training-pipeline","text":"We now implement a training pipeline which trains with a single stage using PPO. In the below we use Builder objects, which allow us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. This is necessary when instantiating things like PyTorch optimizers who take as input the list of parameters associated with our agent's model (something we can't know until the create_model function has been called). class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 2 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) ... Alternatively, we could use a more sophisticated pipeline that begins training with dataset aggregation ( DAgger ) before moving to training with PPO. This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows class ObjectNavThorDaggerThenPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ): ... SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), GoalObjectTypeThorSensor ( object_types = OBJECT_TYPES ), ExpertActionSensor ( nactions = 6 ), # Notice that we have added an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 1e4 ) # Much smaller number of steps as we're using imitation learning ppo_steps = int ( 1e6 ) lr = 2.5e-4 num_mini_batch = 1 if not torch . cuda . is_available () else 6 update_repeats = 4 num_steps = 128 metric_accumulate_interval = cls . MAX_STEPS * 10 # Log every 10 max length tasks save_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 1.0 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : PPO ( clip_decay = LinearDecay ( ppo_steps ), ** PPOConfig ), \"imitation_loss\" : Imitation (), # We add an imitation loss. }, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ,), ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) A version of our experiment config file for which we have implemented this two-stage training can be found here . This two-stage configuration ObjectNavThorDaggerThenPPOExperimentConfig is actually implemented as a subclass of ObjectNavThorPPOExperimentConfig . This is a common pattern used in AllenAct and lets one skip a great deal of boilerplate when defining a new experiment as a slight modification of an old one. Of course one must then be careful: changes to the superclass configuration will propagate to all subclassed configurations.","title":"Training pipeline"},{"location":"howtos/defining-an-experiment/#machine-configuration","text":"In machine_params we define machine configuration parameters that will be used for training, validation and test: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ): num_gpus = torch . cuda . device_count () has_gpu = num_gpus != 0 if mode == \"train\" : nprocesses = 20 if has_gpu else 4 gpu_ids = [ 0 ] if has_gpu else [] elif mode == \"valid\" : nprocesses = 1 gpu_ids = [ 1 % num_gpus ] if has_gpu else [] elif mode == \"test\" : nprocesses = 1 gpu_ids = [ 0 ] if has_gpu else [] else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids } ... In the above we use the availability of cuda ( torch.cuda.device_count() != 0 ) to determine whether we should use parameters appropriate for local machines or for a server. We might optionally add a list of sampler_devices to assign devices (likely those not used for running our agent) to task sampling workers.","title":"Machine configuration"},{"location":"howtos/defining-an-experiment/#task-sampling","text":"The above has defined the model we'd like to use, the types of losses we wish to use during training, and the machine specific parameters that should be used during training. Critically we have not yet defined which task we wish to train our agent to complete. This is done by implementing the ExperimentConfig.make_sampler_fn function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return ObjectNavTaskSampler ( ** kwargs ) ... Now, before training starts, our trainer will know to generate a collection of task samplers using make_sampler_fn for training (and possibly validation or testing). The kwargs parameters used in the above function call can be different for each training process, we implement such differences using the ExperimentConfig.train_task_sampler_args function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_SCENES , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_period\" ] = \"manual\" res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if len ( devices ) > 0 else None ) return res ... Now training process i out of n total processes will be instantiated with the parameters ObjectNavThorPPOExperimentConfig.train_task_sampler_args(i, n, ...) . Similar functions ( valid_task_sampler_args and test_task_sampler_args ) exist for generating validation and test parameters. Note also that with this function we can assign devices to run our environment for each worker. See the documentation of ExperimentConfig for more information.","title":"Task sampling"},{"location":"howtos/defining-an-experiment/#running-the-experiment","text":"We are now in the position to run the experiment (with seed 12345) using the command python main.py object_nav_ithor_ppo_one_object -b projects/tutorials -s 12345","title":"Running the experiment"},{"location":"howtos/running-a-multi-agent-experiment/","text":"To-do #","title":"To-do"},{"location":"howtos/running-a-multi-agent-experiment/#to-do","text":"","title":"To-do"},{"location":"howtos/visualizing-results/","text":"To-do #","title":"To-do"},{"location":"howtos/visualizing-results/#to-do","text":"","title":"To-do"},{"location":"installation/download-datasets/","text":"Downloading datasets # The below provides instructions on how to download datasets necessary for defining the train, validation, and test sets used within the ObjectNav/PointNav tasks in the iTHOR , RoboTHOR , and habitat environments. Point Navigation (PointNav) # RoboTHOR # To get the PointNav dataset for RoboTHOR run the following command: bash datasets/download_navigation_datasets.sh robothor-pointnav This will download the dataset into datasets/robothor-pointnav . iTHOR # To get the PointNav dataset for iTHOR run the following command: bash datasets/download_navigation_datasets.sh ithor-pointnav This will download the dataset into datasets/ithor-pointnav . Habitat # To get the PointNav habitat dataset download and install the allenact-habitat docker container as described in this tutorial . The dataset is included in the docker image. Object Navigation (ObjectNav) # RoboTHOR # To get the ObjectNav dataset for RoboTHOR run the following command: bash datasets/download_navigation_datasets.sh robothor-objectnav This will download the dataset into datasets/robothor-objectnav . iTHOR # To get the ObjectNav dataset for iTHOR run the following command: bash datasets/download_navigation_datasets.sh ithor-objectnav This will download the dataset into datasets/ithor-objectnav .","title":"Download datasets"},{"location":"installation/download-datasets/#downloading-datasets","text":"The below provides instructions on how to download datasets necessary for defining the train, validation, and test sets used within the ObjectNav/PointNav tasks in the iTHOR , RoboTHOR , and habitat environments.","title":"Downloading datasets"},{"location":"installation/download-datasets/#point-navigation-pointnav","text":"","title":"Point Navigation (PointNav)"},{"location":"installation/download-datasets/#robothor","text":"To get the PointNav dataset for RoboTHOR run the following command: bash datasets/download_navigation_datasets.sh robothor-pointnav This will download the dataset into datasets/robothor-pointnav .","title":"RoboTHOR"},{"location":"installation/download-datasets/#ithor","text":"To get the PointNav dataset for iTHOR run the following command: bash datasets/download_navigation_datasets.sh ithor-pointnav This will download the dataset into datasets/ithor-pointnav .","title":"iTHOR"},{"location":"installation/download-datasets/#habitat","text":"To get the PointNav habitat dataset download and install the allenact-habitat docker container as described in this tutorial . The dataset is included in the docker image.","title":"Habitat"},{"location":"installation/download-datasets/#object-navigation-objectnav","text":"","title":"Object Navigation (ObjectNav)"},{"location":"installation/download-datasets/#robothor_1","text":"To get the ObjectNav dataset for RoboTHOR run the following command: bash datasets/download_navigation_datasets.sh robothor-objectnav This will download the dataset into datasets/robothor-objectnav .","title":"RoboTHOR"},{"location":"installation/download-datasets/#ithor_1","text":"To get the ObjectNav dataset for iTHOR run the following command: bash datasets/download_navigation_datasets.sh ithor-objectnav This will download the dataset into datasets/ithor-objectnav .","title":"iTHOR"},{"location":"installation/installation-allenact/","text":"Installation of AllenAct # Clone the repository to your local machine and move into the top-level directory git clone git@github.com:allenai/allenact.git cd allenact Note 1: This library has been tested only in python 3.6 . The following assumes you have a working version of python 3.6 installed locally. Note 2: If you are installing allenact intending to use a GPU for training/inference and your current machine uses an older version of CUDA you may need to manually install the version of PyTorch that supports your CUDA version. In such a case, after installing the below requirements, you should follow the directions for installing PyTorch with older versions of CUDA available on the PyTorch homepage . In order to install requirements we recommend creating a new python virtual environment and installing all of the below requirements into this virtual environment. Several tools exist to help manage virtual environments, we have had success in using pipenv and so provide instructions for installing the requirements using pipenv but also include instructions if you would prefer to install everything directly using pip . Installing requirements with pipenv # If you have already installed pipenv , you may run the following to install all requirements. pipenv install --skip-lock --dev Please see the documentation of pipenv to understand how to use the newly created virtual environment. Installing requirements with pip # Note: do not run the following if you have already installed requirements with pipenv as above. If you prefer managing your environment manually, all requirements may be installed using pip by running the following command: pip install -r requirements.txt Depending on your machine configuration, you may need to use pip3 instead of pip in the above. Installing requirements with conda (experimental) # This below is currently experimental and is provided without a guarantee of continued support. This folder contains YAML files specifying Conda environments compatible with AllenAct. These environment files include: environment-base.yml - A base environment file to be used on machines where GPU support is not needed (everything will be run on the CPU). environment-<CUDA_VERSION>.yml - where <CUDA_VERSION> is the CUDA version used on your machine (if you are using linux, you can generally find this version by running /usr/local/cuda/bin/nvcc --version ). Installing a Conda environment (experimental) # If you are unfamiliar with Conda, please familiarize yourself with their introductory documentation . If you have not already, you will need to first install Conda (i.e. Anaconda or Miniconda) on your machine. We suggest installing Miniconda as it's relatively lightweight. For the moment let's assume you're using environment-base.yml above. To install a conda environment with name allenact using this file you can simply run the following ( this will take a few minutes ): conda env create --file ./conda/environment-base.yml --name allenact The above is very simple but has the side effect of creating a new src directory where it will place some of AllenAct's dependencies. To get around this, instead of running the above you can instead run the commands: export MY_ENV_NAME = allenact export CONDA_BASE = \" $( dirname $( dirname \" ${ CONDA_EXE } \" )) \" PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env create --file ./conda/environment-base.yml --name $MY_ENV_NAME These additional commands tell conda to place these dependencies under the ${CONDA_BASE}/envs/${MY_ENV_NAME}/pipsrc directory rather than under src , this is more in line with where we'd expect dependencies to be placed when running pip install ... . Using the Conda environment # Now that you've installed the conda environment as above, you can activate it by running: conda activate allenact after which you can run everything as you would normally. Installing supported environments # We also provide installation instructions for the environments supported in AllenAct here .","title":"Install AllenAct"},{"location":"installation/installation-allenact/#installation-of-allenact","text":"Clone the repository to your local machine and move into the top-level directory git clone git@github.com:allenai/allenact.git cd allenact Note 1: This library has been tested only in python 3.6 . The following assumes you have a working version of python 3.6 installed locally. Note 2: If you are installing allenact intending to use a GPU for training/inference and your current machine uses an older version of CUDA you may need to manually install the version of PyTorch that supports your CUDA version. In such a case, after installing the below requirements, you should follow the directions for installing PyTorch with older versions of CUDA available on the PyTorch homepage . In order to install requirements we recommend creating a new python virtual environment and installing all of the below requirements into this virtual environment. Several tools exist to help manage virtual environments, we have had success in using pipenv and so provide instructions for installing the requirements using pipenv but also include instructions if you would prefer to install everything directly using pip .","title":"Installation of AllenAct"},{"location":"installation/installation-allenact/#installing-requirements-with-pipenv","text":"If you have already installed pipenv , you may run the following to install all requirements. pipenv install --skip-lock --dev Please see the documentation of pipenv to understand how to use the newly created virtual environment.","title":"Installing requirements with pipenv"},{"location":"installation/installation-allenact/#installing-requirements-with-pip","text":"Note: do not run the following if you have already installed requirements with pipenv as above. If you prefer managing your environment manually, all requirements may be installed using pip by running the following command: pip install -r requirements.txt Depending on your machine configuration, you may need to use pip3 instead of pip in the above.","title":"Installing requirements with pip"},{"location":"installation/installation-allenact/#installing-requirements-with-conda-experimental","text":"This below is currently experimental and is provided without a guarantee of continued support. This folder contains YAML files specifying Conda environments compatible with AllenAct. These environment files include: environment-base.yml - A base environment file to be used on machines where GPU support is not needed (everything will be run on the CPU). environment-<CUDA_VERSION>.yml - where <CUDA_VERSION> is the CUDA version used on your machine (if you are using linux, you can generally find this version by running /usr/local/cuda/bin/nvcc --version ).","title":"Installing requirements with conda (experimental)"},{"location":"installation/installation-allenact/#installing-a-conda-environment-experimental","text":"If you are unfamiliar with Conda, please familiarize yourself with their introductory documentation . If you have not already, you will need to first install Conda (i.e. Anaconda or Miniconda) on your machine. We suggest installing Miniconda as it's relatively lightweight. For the moment let's assume you're using environment-base.yml above. To install a conda environment with name allenact using this file you can simply run the following ( this will take a few minutes ): conda env create --file ./conda/environment-base.yml --name allenact The above is very simple but has the side effect of creating a new src directory where it will place some of AllenAct's dependencies. To get around this, instead of running the above you can instead run the commands: export MY_ENV_NAME = allenact export CONDA_BASE = \" $( dirname $( dirname \" ${ CONDA_EXE } \" )) \" PIP_SRC = \" ${ CONDA_BASE } /envs/ ${ MY_ENV_NAME } /pipsrc\" conda env create --file ./conda/environment-base.yml --name $MY_ENV_NAME These additional commands tell conda to place these dependencies under the ${CONDA_BASE}/envs/${MY_ENV_NAME}/pipsrc directory rather than under src , this is more in line with where we'd expect dependencies to be placed when running pip install ... .","title":"Installing a Conda environment (experimental)"},{"location":"installation/installation-allenact/#using-the-conda-environment","text":"Now that you've installed the conda environment as above, you can activate it by running: conda activate allenact after which you can run everything as you would normally.","title":"Using the Conda environment"},{"location":"installation/installation-allenact/#installing-supported-environments","text":"We also provide installation instructions for the environments supported in AllenAct here .","title":"Installing supported environments"},{"location":"installation/installation-framework/","text":"Installation of supported environments # Below we provide installation instructions for a number of environments that we support. Installation of MiniGrid # MiniGrid will automatically be installed when installing allenact and so nothing additional needs to be done. If you wish to (re)install it manually using pip , simply run the command: pip install gym-minigrid Installation of iTHOR # iTHOR will automatically be installed when installing allenact and so, if you have installed allenact on a machine with an attached display, nothing additional needs to be done. If you wish to (re)install it manually using pip , simply run the command: pip install ai2thor The first time you will run an experiment with iTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes automatically. Trying to use iTHOR on a machine without an attached display? If you wish to run iTHOR on a machine without an attached display (for instance, a remote server such as an AWS machine) you will also need to run a script that launches xserver processes on your GPUs. This can be done with the following command: sudo python scripts/startx.py & Notice that you need to run the command with sudo (i.e. administrator privileges). If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine). Installation of RoboTHOR # RoboTHOR is installed in tandem with iTHOR when installing the ai2thor library. For more information see the above section on installing iTHOR . Installation of Habitat # Using Docker # To run experiments using Habitat please use our docker image using the following command: docker pull allenact/allenact:latest This container includes the 0.1.0 release of allenact , the 0.1.5 release of habitat as well as the Gibson point navigation dataset. This dataset consists of a set of start and goal positions provided by habitat. You then need to launch the container and attach into it: docker run --runtime = nvidia -it allenact/allenact If you are running the container on a machine without an Nvidia GPU, omit the --runtime=nvidia flag. Once inside the container simply cd into the allenact directory where all the allenact and habitat code should be stored: Unfortunately we cannot legally redistribute the Gibson scenes by including them in the above container. Instead you will need to download these yourself by filling out this form and downloading the gibson_habitat_trainval data. Extract the scene assets ( .glb files) into habitat-lab/data/scene_datasets/ within the above container. You can then proceed to run your experiments using allenact as you normally would. Using conda (experimental) # The following is experimental, we do not guarantee that AllenAct will continue to support this installation procedure in future releases. Habitat has recently released the option to install their simulator using conda which avoids having to manually build dependencies or use Docker. This does not guarantee that the installation process is completely painless (it is difficult to avoid all possible build issues) but we've found it to be a nice alternative to using Docker. To use this installation option please first install an AllenAct conda environment using the instructions available under the Installing a Conda environment (experimental) section here . After installing this environment, you can then install habitat-sim by running: If you are on a machine with an attached display: conda install habitat-sim = 0 .1.5 -c conda-forge -c aihabitat --name allenact If you are on a machine without an attached display (e.g. a server): conda install habitat-sim = 0 .1.5 headless -c conda-forge -c aihabitat --name allenact","title":"Install environments"},{"location":"installation/installation-framework/#installation-of-supported-environments","text":"Below we provide installation instructions for a number of environments that we support.","title":"Installation of supported environments"},{"location":"installation/installation-framework/#installation-of-minigrid","text":"MiniGrid will automatically be installed when installing allenact and so nothing additional needs to be done. If you wish to (re)install it manually using pip , simply run the command: pip install gym-minigrid","title":"Installation of MiniGrid"},{"location":"installation/installation-framework/#installation-of-ithor","text":"iTHOR will automatically be installed when installing allenact and so, if you have installed allenact on a machine with an attached display, nothing additional needs to be done. If you wish to (re)install it manually using pip , simply run the command: pip install ai2thor The first time you will run an experiment with iTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes automatically. Trying to use iTHOR on a machine without an attached display? If you wish to run iTHOR on a machine without an attached display (for instance, a remote server such as an AWS machine) you will also need to run a script that launches xserver processes on your GPUs. This can be done with the following command: sudo python scripts/startx.py & Notice that you need to run the command with sudo (i.e. administrator privileges). If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine).","title":"Installation of iTHOR"},{"location":"installation/installation-framework/#installation-of-robothor","text":"RoboTHOR is installed in tandem with iTHOR when installing the ai2thor library. For more information see the above section on installing iTHOR .","title":"Installation of RoboTHOR"},{"location":"installation/installation-framework/#installation-of-habitat","text":"","title":"Installation of Habitat"},{"location":"installation/installation-framework/#using-docker","text":"To run experiments using Habitat please use our docker image using the following command: docker pull allenact/allenact:latest This container includes the 0.1.0 release of allenact , the 0.1.5 release of habitat as well as the Gibson point navigation dataset. This dataset consists of a set of start and goal positions provided by habitat. You then need to launch the container and attach into it: docker run --runtime = nvidia -it allenact/allenact If you are running the container on a machine without an Nvidia GPU, omit the --runtime=nvidia flag. Once inside the container simply cd into the allenact directory where all the allenact and habitat code should be stored: Unfortunately we cannot legally redistribute the Gibson scenes by including them in the above container. Instead you will need to download these yourself by filling out this form and downloading the gibson_habitat_trainval data. Extract the scene assets ( .glb files) into habitat-lab/data/scene_datasets/ within the above container. You can then proceed to run your experiments using allenact as you normally would.","title":"Using Docker"},{"location":"installation/installation-framework/#using-conda-experimental","text":"The following is experimental, we do not guarantee that AllenAct will continue to support this installation procedure in future releases. Habitat has recently released the option to install their simulator using conda which avoids having to manually build dependencies or use Docker. This does not guarantee that the installation process is completely painless (it is difficult to avoid all possible build issues) but we've found it to be a nice alternative to using Docker. To use this installation option please first install an AllenAct conda environment using the instructions available under the Installing a Conda environment (experimental) section here . After installing this environment, you can then install habitat-sim by running: If you are on a machine with an attached display: conda install habitat-sim = 0 .1.5 -c conda-forge -c aihabitat --name allenact If you are on a machine without an attached display (e.g. a server): conda install habitat-sim = 0 .1.5 headless -c conda-forge -c aihabitat --name allenact","title":"Using conda (experimental)"},{"location":"notebooks/firstbook/","text":"To-do #","title":"To-do"},{"location":"notebooks/firstbook/#to-do","text":"","title":"To-do"},{"location":"projects/advisor_2020/","text":"Experiments for Advisor # TODO: # Add details taken from https://unnat.github.io/advisor/. Cite the arxiv paper. Give a list of things you can run with bash commands. Ideally be able to recreate a large set of experiments.","title":"Experiments for Advisor"},{"location":"projects/advisor_2020/#experiments-for-advisor","text":"","title":"Experiments for Advisor"},{"location":"projects/advisor_2020/#todo","text":"Add details taken from https://unnat.github.io/advisor/. Cite the arxiv paper. Give a list of things you can run with bash commands. Ideally be able to recreate a large set of experiments.","title":"TODO:"},{"location":"projects/babyai_baselines/","text":"Baseline experiments for the BabyAI environment # We perform a collection of baseline experiments within the BabyAI environment on the GoToLocal task, see the projects/babyai_baselines/experiments/go_to_local directory. For instance, to train a model using PPO, run python main.py go_to_local.ppo --experiment_base projects/babyai_baselines/experiments Note that these experiments will be quite slow when not using a GPU as the BabyAI model architecture is surprisingly large. Specifying a GPU (if available) can be done from the command line using hooks we created using gin-config . E.g. to train using the 0th GPU device, add --gp \"machine_params.gpu_id = 0\" to the above command.","title":"BabyAI baselines"},{"location":"projects/babyai_baselines/#baseline-experiments-for-the-babyai-environment","text":"We perform a collection of baseline experiments within the BabyAI environment on the GoToLocal task, see the projects/babyai_baselines/experiments/go_to_local directory. For instance, to train a model using PPO, run python main.py go_to_local.ppo --experiment_base projects/babyai_baselines/experiments Note that these experiments will be quite slow when not using a GPU as the BabyAI model architecture is surprisingly large. Specifying a GPU (if available) can be done from the command line using hooks we created using gin-config . E.g. to train using the 0th GPU device, add --gp \"machine_params.gpu_id = 0\" to the above command.","title":"Baseline experiments for the BabyAI environment"},{"location":"projects/objectnav_baselines/","text":"Baseline models for the Object Navigation task in the RoboTHOR and iTHOR environments # This project contains the code for training baseline models on the PointNav task. In this setting the agent spawns at a location in an environment and is tasked to explore the environment until it finds an object of a certain type (such as TV or Basketball). Once the agent is confident that it has the object within sight it executes the END action which terminates the episode. If the agent is within a set distance to the target (in our case 1.5 meters) and the target is visible within its observation frame the agent succeeded, else it failed. Provided are experiment configs for training a simple convolutional model with an GRU using RGB , Depth or RGBD as inputs in RoboTHOR and iTHOR . The experiments are set up to train models using the DD-PPO Reinforcement Learning Algorithm. To train an experiment run the following command from the allenact root directory: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> Where <PATH_TO_OUTPUT> is the path of the directory where we want the model weights and logs to be stored, <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> is the directory where our experiment file is located and <EXPERIMENT_NAME> is the name of the python module containing the experiment. An example usage of this command would be: python main.py -o storage/objectnav-robothor-rgb -b projects/objectnav_baselines/experiments/robothor/ objectnav_robothor_rgb_resnet_ddppo This trains a simple convolutional neural network with a GRU using RGB input passed through a pretrained ResNet-18 visual encoder on the PointNav task in the RoboTHOR environment and stores the model weights and logs to storage/pointnav-robothor-rgb . hings you can run with bash commands","title":"ObjectNav baselines"},{"location":"projects/objectnav_baselines/#baseline-models-for-the-object-navigation-task-in-the-robothor-and-ithor-environments","text":"This project contains the code for training baseline models on the PointNav task. In this setting the agent spawns at a location in an environment and is tasked to explore the environment until it finds an object of a certain type (such as TV or Basketball). Once the agent is confident that it has the object within sight it executes the END action which terminates the episode. If the agent is within a set distance to the target (in our case 1.5 meters) and the target is visible within its observation frame the agent succeeded, else it failed. Provided are experiment configs for training a simple convolutional model with an GRU using RGB , Depth or RGBD as inputs in RoboTHOR and iTHOR . The experiments are set up to train models using the DD-PPO Reinforcement Learning Algorithm. To train an experiment run the following command from the allenact root directory: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> Where <PATH_TO_OUTPUT> is the path of the directory where we want the model weights and logs to be stored, <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> is the directory where our experiment file is located and <EXPERIMENT_NAME> is the name of the python module containing the experiment. An example usage of this command would be: python main.py -o storage/objectnav-robothor-rgb -b projects/objectnav_baselines/experiments/robothor/ objectnav_robothor_rgb_resnet_ddppo This trains a simple convolutional neural network with a GRU using RGB input passed through a pretrained ResNet-18 visual encoder on the PointNav task in the RoboTHOR environment and stores the model weights and logs to storage/pointnav-robothor-rgb . hings you can run with bash commands","title":"Baseline models for the Object Navigation task in the RoboTHOR and iTHOR environments"},{"location":"projects/pointnav_baselines/","text":"Baseline models for the Point Navigation task in the Habitat, RoboTHOR and iTHOR environments # This project contains the code for training baseline models on the PointNav task. In this setting the agent spawns at a location in an environment and is tasked to move to another location. The agent is given a \"compass\" that tells it the distance and bearing to the target position at every frame. Once the agent is confident that it has reached the end it executes the END action which terminates the episode. If the agent is within a set distance to the target (in our case 0.2 meters) the agent succeeded, else it failed. Provided are experiment configs for training a simple convolutional model with an GRU using RGB , Depth or RGBD as inputs in Habitat , RoboTHOR and iTHOR . The experiments are set up to train models using the DD-PPO Reinforcement Learning Algorithm. To train an experiment run the following command from the allenact root directory: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> Where <PATH_TO_OUTPUT> is the path of the directory where we want the model weights and logs to be stored, <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> is the directory where our experiment file is located and <EXPERIMENT_NAME> is the name of the python module containing the experiment. An example usage of this command would be: python main.py -o storage/pointnav-robothor-depth -b projects/pointnav_baselines/experiments/robothor/ pointnav_robothor_depth_simpleconvgru_ddppo This trains a simple convolutional neural network with a GRU using Depth input on the PointNav task in the RoboTHOR environment and stores the model weights and logs to storage/pointnav-robothor-rgb .","title":"PointNav baselines"},{"location":"projects/pointnav_baselines/#baseline-models-for-the-point-navigation-task-in-the-habitat-robothor-and-ithor-environments","text":"This project contains the code for training baseline models on the PointNav task. In this setting the agent spawns at a location in an environment and is tasked to move to another location. The agent is given a \"compass\" that tells it the distance and bearing to the target position at every frame. Once the agent is confident that it has reached the end it executes the END action which terminates the episode. If the agent is within a set distance to the target (in our case 0.2 meters) the agent succeeded, else it failed. Provided are experiment configs for training a simple convolutional model with an GRU using RGB , Depth or RGBD as inputs in Habitat , RoboTHOR and iTHOR . The experiments are set up to train models using the DD-PPO Reinforcement Learning Algorithm. To train an experiment run the following command from the allenact root directory: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> Where <PATH_TO_OUTPUT> is the path of the directory where we want the model weights and logs to be stored, <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> is the directory where our experiment file is located and <EXPERIMENT_NAME> is the name of the python module containing the experiment. An example usage of this command would be: python main.py -o storage/pointnav-robothor-depth -b projects/pointnav_baselines/experiments/robothor/ pointnav_robothor_depth_simpleconvgru_ddppo This trains a simple convolutional neural network with a GRU using Depth input on the PointNav task in the RoboTHOR environment and stores the model weights and logs to storage/pointnav-robothor-rgb .","title":"Baseline models for the Point Navigation task in the Habitat, RoboTHOR and iTHOR environments"},{"location":"projects/two_body_problem_2019/","text":"Experiments for the Two Body Problem paper # TODO: # Add details taken from https://prior.allenai.org/projects/two-body-problem Cite the CVPR paper. Give a list of things you can run with bash commands. At least a subset of the experiments.","title":"Experiments for the Two Body Problem paper"},{"location":"projects/two_body_problem_2019/#experiments-for-the-two-body-problem-paper","text":"","title":"Experiments for the Two Body Problem paper"},{"location":"projects/two_body_problem_2019/#todo","text":"Add details taken from https://prior.allenai.org/projects/two-body-problem Cite the CVPR paper. Give a list of things you can run with bash commands. At least a subset of the experiments.","title":"TODO:"},{"location":"tutorials/","text":"AllenAct Tutorials # We provide several tutorials to help ramp up researchers to the field of Embodied-AI as well as to the AllenAct framework. Navigation in MiniGrid # We train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. This tutorial presents: Writing an experiment configuration file with a simple training pipeline from scratch. Using one of the supported environments with minimal user effort. Training, validation and testing your experiment from the command line. Follow the tutorial here. PointNav in RoboTHOR # We train an agent on the Point Navigation task within the RoboTHOR Embodied-AI environment. This tutorial presents: The basics of the Point Navigation task, a common task in Embodied AI Using an external dataset Writing an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. Testing a pre-trained model Follow the tutorial here. Swapping in a new environment # This tutorial demonstrates how easy it is modify the experiment config created in the RoboTHOR PointNav tutorial to work with the iTHOR and Habitat environments. Follow the tutorial here. Using a pretrained model # This tutorial shows how to run inference on one or more checkpoints of a pretrained model and generate visualizations of different types. Follow the tutorial here. Off-policy training # This tutorial shows how to train an Actor using an off-policy dataset with expert actions. Follow the tutorial here.","title":"AllenAct Tutorials"},{"location":"tutorials/#allenact-tutorials","text":"We provide several tutorials to help ramp up researchers to the field of Embodied-AI as well as to the AllenAct framework.","title":"AllenAct Tutorials"},{"location":"tutorials/#navigation-in-minigrid","text":"We train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. This tutorial presents: Writing an experiment configuration file with a simple training pipeline from scratch. Using one of the supported environments with minimal user effort. Training, validation and testing your experiment from the command line. Follow the tutorial here.","title":"Navigation in MiniGrid"},{"location":"tutorials/#pointnav-in-robothor","text":"We train an agent on the Point Navigation task within the RoboTHOR Embodied-AI environment. This tutorial presents: The basics of the Point Navigation task, a common task in Embodied AI Using an external dataset Writing an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. Testing a pre-trained model Follow the tutorial here.","title":"PointNav in RoboTHOR"},{"location":"tutorials/#swapping-in-a-new-environment","text":"This tutorial demonstrates how easy it is modify the experiment config created in the RoboTHOR PointNav tutorial to work with the iTHOR and Habitat environments. Follow the tutorial here.","title":"Swapping in a new environment"},{"location":"tutorials/#using-a-pretrained-model","text":"This tutorial shows how to run inference on one or more checkpoints of a pretrained model and generate visualizations of different types. Follow the tutorial here.","title":"Using a pretrained model"},{"location":"tutorials/#off-policy-training","text":"This tutorial shows how to train an Actor using an off-policy dataset with expert actions. Follow the tutorial here.","title":"Off-policy training"},{"location":"tutorials/minigrid-tutorial/","text":"Tutorial: Navigation in MiniGrid # In this tutorial, we will train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. We will demonstrate how to: Write an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. This tutorial assumes the installation instructions have already been followed and, to some extent, this framework's abstractions are known. The task # A MiniGrid-Empty-Random-5x5-v0 task consists of a grid of dimensions 5x5 where an agent spawned at a random location and orientation has to navigate to the visitable bottom right corner cell of the grid by sequences of three possible actions (rotate left/right and move forward). A visualization of the environment with expert steps in a random MiniGrid-Empty-Random-5x5-v0 task looks like The observation for the agent is a subset of the entire grid, simulating a simplified limited field of view, as depicted by the highlighted rectangle (observed subset of the grid) around the agent (red arrow). Gray cells correspond to walls. Experiment configuration file # Our complete experiment consists of: Training a basic actor-critic agent with memory to solve randomly sampled navigation tasks. Validation on a fixed set of tasks (running in parallel with training). A second stage where we test saved checkpoints with a larger fixed set of tasks. The entire configuration for the experiment, including training, validation, and testing, is encapsulated in a single class implementing the ExperimentConfig abstraction. For this tutorial, we will follow the config under projects/tutorials/minigrid_tutorial.py . The ExperimentConfig abstraction is used by the OnPolicyTrainer class (for training) and the OnPolicyInference class (for validation and testing) invoked through the entry script main.py that calls an orchestrating OnPolicyRunner class. It includes: A tag method to identify the experiment. A create_model method to instantiate actor-critic models. A make_sampler_fn method to instantiate task samplers. Three {train,valid,test}_task_sampler_args methods describing initialization parameters for task samplers used in training, validation, and testing; including assignment of workers to devices for simulation. A machine_params method with configuration parameters that will be used for training, validation, and testing. A training_pipeline method describing a possibly multi-staged training pipeline with different types of losses, an optimizer, and other parameters like learning rates, batch sizes, etc. Preliminaries # We first identify the experiment through a tag . class MiniGridTutorialExperimentConfig ( ExperimentConfig ): @classmethod def tag ( cls ) -> str : return \"MiniGridTutorial\" Sensors and Model # A readily available Sensor type for MiniGrid, EgocentricMiniGridSensor , allows us to extract observations in a format consumable by an ActorCriticModel agent: SENSORS = [ EgocentricMiniGridSensor ( agent_view_size = 5 , view_channels = 3 ), ] The three view_channels include objects, colors and states corresponding to a partial observation of the environment as an image tensor, equivalent to that from ImgObsWrapper in MiniGrid . The relatively large agent_view_size means the view will only be clipped by the environment walls in the forward and lateral directions with respect to the agent's orientation. We define our ActorCriticModel agent using a lightweight implementation with recurrent memory for MiniGrid environments, MiniGridSimpleConvRNN : @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return MiniGridSimpleConvRNN ( action_space = gym . spaces . Discrete ( len ( MiniGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , num_objects = cls . SENSORS [ 0 ] . num_objects , num_colors = cls . SENSORS [ 0 ] . num_colors , num_states = cls . SENSORS [ 0 ] . num_states , ) Task samplers # We use an available TaskSampler implementation for MiniGrid environments that allows to sample both random and deterministic MiniGridTasks , MiniGridTaskSampler : @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return MiniGridTaskSampler ( ** kwargs ) This task sampler will during training (or validation/testing), randomly initialize new tasks for the agent to complete. While it is not quite as important for this task type (as we test our agent in the same setting it is trained on) there are a lot of good reasons we would like to sample tasks differently during training than during validation or testing. One good reason, that is applicable in this tutorial, is that, during training, we would like to be able to sample tasks forever while, during testing, we would like to sample a fixed number of tasks (as otherwise we would never finish testing!). In allenact this is made possible by defining different arguments for the task sampler: def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"train\" ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"valid\" ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"test\" ) where, for convenience, we have defined a _get_sampler_args method: def _get_sampler_args ( self , process_ind : int , mode : str ) -> Dict [ str , Any ]: \"\"\" Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` \"\"\" if mode == \"train\" : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 20 + 20 * ( mode == \"test\" ) # 20 tasks for valid, 40 for test # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks )) deterministic_sampling = True # deterministically sample task in validation/testing return dict ( max_tasks = max_tasks , # see above env_class = self . make_env , # builder for third-party environment (defined below) sensors = self . SENSORS , # sensors used to return observations to the agent env_info = dict (), # parameters for environment builder (none for now) task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling # see above ) @staticmethod def make_env ( * args , ** kwargs ): return EmptyRandomEnv5x5 () Note that the env_class argument to the Task Sampler is the one determining which task type we are going to train the model for (in this case, MiniGrid-Empty-Random-5x5-v0 from gym-minigrid ) . The sparse reward is given by the environment , and the maximum task length is 100. For training, we opt for a default random sampling, whereas for validation and test we define fixed sets of randomly sampled tasks without needing to explicitly define a dataset. In this toy example, the maximum number of different tasks is 32. For validation we sample 320 tasks using 16 samplers, or 640 for testing, so we can be fairly sure that all possible tasks are visited at least once during evaluation. Machine parameters # Given the simplicity of the task and model, we can quickly train the model on the CPU: @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ) -> Dict [ str , Any ]: return { \"nprocesses\" : 128 if mode == \"train\" else 16 , \"gpu_ids\" : [], } We allocate a larger number of samplers for training (128) than for validation or testing (16), and we default to CPU usage by returning an empty list of gpu_ids . Training pipeline # The last definition required before starting to train is a training pipeline. In this case, we just use a single PPO stage with linearly decaying learning rate: @classmethod def training_pipeline ( cls , ** kwargs ) -> TrainingPipeline : ppo_steps = int ( 150000 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = PPO ( ** PPOConfig )), pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], optimizer_builder = Builder ( optim . Adam , dict ( lr = 1e-4 )), num_mini_batch = 4 , update_repeats = 3 , max_grad_norm = 0.5 , num_steps = 16 , gamma = 0.99 , use_gae = True , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 10000 , metric_accumulate_interval = 1 , lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) You can see that we use a Builder class to postpone the construction of some of the elements, like the optimizer, for which the model weights need to be known. Training and validation # We have a complete implementation of this experiment's configuration class in projects/tutorials/minigrid_tutorial.py . To start training from scratch, we just need to invoke python main.py minigrid_tutorial -b projects/tutorials -m 8 -o /PATH/TO/minigrid_output -s 12345 from the allenact root directory. With -b projects/tutorials we tell allenact that minigrid_tutorial experiment config file will be found in the projects/tutorials directory. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o minigrid_output we set the output folder into which results and logs will be saved. With -s 12345 we set the random seed. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/minigrid_output which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: Testing # The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to test for a specific experiment, we need to pass its training start date with the option -t EXPERIMENT_DATE : python main.py minigrid_tutorial -b projects/tutorials -m 1 -o /PATH/TO/minigrid_output -s 12345 -t EXPERIMENT_DATE Again, if everything went well, the test success rate should converge to 1 and the mean episode length to a value below 4. Detailed results are saved under a metrics subfolder in the output folder. The test curves should look similar to:","title":"Navigation in Minigrid"},{"location":"tutorials/minigrid-tutorial/#tutorial-navigation-in-minigrid","text":"In this tutorial, we will train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. We will demonstrate how to: Write an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. This tutorial assumes the installation instructions have already been followed and, to some extent, this framework's abstractions are known.","title":"Tutorial: Navigation in MiniGrid"},{"location":"tutorials/minigrid-tutorial/#the-task","text":"A MiniGrid-Empty-Random-5x5-v0 task consists of a grid of dimensions 5x5 where an agent spawned at a random location and orientation has to navigate to the visitable bottom right corner cell of the grid by sequences of three possible actions (rotate left/right and move forward). A visualization of the environment with expert steps in a random MiniGrid-Empty-Random-5x5-v0 task looks like The observation for the agent is a subset of the entire grid, simulating a simplified limited field of view, as depicted by the highlighted rectangle (observed subset of the grid) around the agent (red arrow). Gray cells correspond to walls.","title":"The task"},{"location":"tutorials/minigrid-tutorial/#experiment-configuration-file","text":"Our complete experiment consists of: Training a basic actor-critic agent with memory to solve randomly sampled navigation tasks. Validation on a fixed set of tasks (running in parallel with training). A second stage where we test saved checkpoints with a larger fixed set of tasks. The entire configuration for the experiment, including training, validation, and testing, is encapsulated in a single class implementing the ExperimentConfig abstraction. For this tutorial, we will follow the config under projects/tutorials/minigrid_tutorial.py . The ExperimentConfig abstraction is used by the OnPolicyTrainer class (for training) and the OnPolicyInference class (for validation and testing) invoked through the entry script main.py that calls an orchestrating OnPolicyRunner class. It includes: A tag method to identify the experiment. A create_model method to instantiate actor-critic models. A make_sampler_fn method to instantiate task samplers. Three {train,valid,test}_task_sampler_args methods describing initialization parameters for task samplers used in training, validation, and testing; including assignment of workers to devices for simulation. A machine_params method with configuration parameters that will be used for training, validation, and testing. A training_pipeline method describing a possibly multi-staged training pipeline with different types of losses, an optimizer, and other parameters like learning rates, batch sizes, etc.","title":"Experiment configuration file"},{"location":"tutorials/minigrid-tutorial/#preliminaries","text":"We first identify the experiment through a tag . class MiniGridTutorialExperimentConfig ( ExperimentConfig ): @classmethod def tag ( cls ) -> str : return \"MiniGridTutorial\"","title":"Preliminaries"},{"location":"tutorials/minigrid-tutorial/#sensors-and-model","text":"A readily available Sensor type for MiniGrid, EgocentricMiniGridSensor , allows us to extract observations in a format consumable by an ActorCriticModel agent: SENSORS = [ EgocentricMiniGridSensor ( agent_view_size = 5 , view_channels = 3 ), ] The three view_channels include objects, colors and states corresponding to a partial observation of the environment as an image tensor, equivalent to that from ImgObsWrapper in MiniGrid . The relatively large agent_view_size means the view will only be clipped by the environment walls in the forward and lateral directions with respect to the agent's orientation. We define our ActorCriticModel agent using a lightweight implementation with recurrent memory for MiniGrid environments, MiniGridSimpleConvRNN : @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return MiniGridSimpleConvRNN ( action_space = gym . spaces . Discrete ( len ( MiniGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , num_objects = cls . SENSORS [ 0 ] . num_objects , num_colors = cls . SENSORS [ 0 ] . num_colors , num_states = cls . SENSORS [ 0 ] . num_states , )","title":"Sensors and Model"},{"location":"tutorials/minigrid-tutorial/#task-samplers","text":"We use an available TaskSampler implementation for MiniGrid environments that allows to sample both random and deterministic MiniGridTasks , MiniGridTaskSampler : @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return MiniGridTaskSampler ( ** kwargs ) This task sampler will during training (or validation/testing), randomly initialize new tasks for the agent to complete. While it is not quite as important for this task type (as we test our agent in the same setting it is trained on) there are a lot of good reasons we would like to sample tasks differently during training than during validation or testing. One good reason, that is applicable in this tutorial, is that, during training, we would like to be able to sample tasks forever while, during testing, we would like to sample a fixed number of tasks (as otherwise we would never finish testing!). In allenact this is made possible by defining different arguments for the task sampler: def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"train\" ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"valid\" ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"test\" ) where, for convenience, we have defined a _get_sampler_args method: def _get_sampler_args ( self , process_ind : int , mode : str ) -> Dict [ str , Any ]: \"\"\" Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` \"\"\" if mode == \"train\" : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 20 + 20 * ( mode == \"test\" ) # 20 tasks for valid, 40 for test # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks )) deterministic_sampling = True # deterministically sample task in validation/testing return dict ( max_tasks = max_tasks , # see above env_class = self . make_env , # builder for third-party environment (defined below) sensors = self . SENSORS , # sensors used to return observations to the agent env_info = dict (), # parameters for environment builder (none for now) task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling # see above ) @staticmethod def make_env ( * args , ** kwargs ): return EmptyRandomEnv5x5 () Note that the env_class argument to the Task Sampler is the one determining which task type we are going to train the model for (in this case, MiniGrid-Empty-Random-5x5-v0 from gym-minigrid ) . The sparse reward is given by the environment , and the maximum task length is 100. For training, we opt for a default random sampling, whereas for validation and test we define fixed sets of randomly sampled tasks without needing to explicitly define a dataset. In this toy example, the maximum number of different tasks is 32. For validation we sample 320 tasks using 16 samplers, or 640 for testing, so we can be fairly sure that all possible tasks are visited at least once during evaluation.","title":"Task samplers"},{"location":"tutorials/minigrid-tutorial/#machine-parameters","text":"Given the simplicity of the task and model, we can quickly train the model on the CPU: @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ) -> Dict [ str , Any ]: return { \"nprocesses\" : 128 if mode == \"train\" else 16 , \"gpu_ids\" : [], } We allocate a larger number of samplers for training (128) than for validation or testing (16), and we default to CPU usage by returning an empty list of gpu_ids .","title":"Machine parameters"},{"location":"tutorials/minigrid-tutorial/#training-pipeline","text":"The last definition required before starting to train is a training pipeline. In this case, we just use a single PPO stage with linearly decaying learning rate: @classmethod def training_pipeline ( cls , ** kwargs ) -> TrainingPipeline : ppo_steps = int ( 150000 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = PPO ( ** PPOConfig )), pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], optimizer_builder = Builder ( optim . Adam , dict ( lr = 1e-4 )), num_mini_batch = 4 , update_repeats = 3 , max_grad_norm = 0.5 , num_steps = 16 , gamma = 0.99 , use_gae = True , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 10000 , metric_accumulate_interval = 1 , lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) You can see that we use a Builder class to postpone the construction of some of the elements, like the optimizer, for which the model weights need to be known.","title":"Training pipeline"},{"location":"tutorials/minigrid-tutorial/#training-and-validation","text":"We have a complete implementation of this experiment's configuration class in projects/tutorials/minigrid_tutorial.py . To start training from scratch, we just need to invoke python main.py minigrid_tutorial -b projects/tutorials -m 8 -o /PATH/TO/minigrid_output -s 12345 from the allenact root directory. With -b projects/tutorials we tell allenact that minigrid_tutorial experiment config file will be found in the projects/tutorials directory. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o minigrid_output we set the output folder into which results and logs will be saved. With -s 12345 we set the random seed. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/minigrid_output which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to:","title":"Training and validation"},{"location":"tutorials/minigrid-tutorial/#testing","text":"The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to test for a specific experiment, we need to pass its training start date with the option -t EXPERIMENT_DATE : python main.py minigrid_tutorial -b projects/tutorials -m 1 -o /PATH/TO/minigrid_output -s 12345 -t EXPERIMENT_DATE Again, if everything went well, the test success rate should converge to 1 and the mean episode length to a value below 4. Detailed results are saved under a metrics subfolder in the output folder. The test curves should look similar to:","title":"Testing"},{"location":"tutorials/offpolicy-tutorial/","text":"Tutorial: Off-policy training # In this tutorial we'll learn how to train an agent from an external dataset by imitating expert actions via Behavior Cloning. We'll use a BabyAI agent to solve GoToLocal tasks on MiniGrid ; see the projects/babyai_baselines/experiments/go_to_local directory for more details. This tutorial assumes the installation instructions have already been followed and, to some extent, this framework's abstractions are known. The task # In a GoToLocal task, the agent immersed in a grid world has to navigate to a specific object in the presence of multiple distractors, requiring the agent to understand go to instructions like \"go to the red ball\". For further details, please consult the original paper . Getting the dataset # We will use a large dataset ( more than 4 GB ) including expert demonstrations for GoToLocal tasks. To download the data we'll run PYTHONPATH = . python plugins/babyai_plugin/scripts/download_babyai_expert_demos.py GoToLocal from the project's root directory, which will download BabyAI-GoToLocal-v0.pkl and BabyAI-GoToLocal-v0_valid.pkl to the plugins/babyai_plugin/data/demos directory. We will also generate small versions of the datasets, which will be useful if running on CPU, by calling PYTHONPATH = . python plugins/babyai_plugin/scripts/truncate_expert_demos.py from the project's root directory, which will generate BabyAI-GoToLocal-v0-small.pkl under the same plugins/babyai_plugin/data/demos directory. Data iterator # In order to train with an off-policy dataset, we need to define a data Iterator . The Data Iterator merges the functionality of the Dataset and Dataloader in PyTorch, in that it defines the way to both sample data from the dataset and convert them into batches to be used for training. An example of a Data Iterator for BabyAI expert demos might look as follows: class ExpertTrajectoryIterator ( Iterator ): def __init__ ( self , data : List [ Tuple [ str , bytes , List [ int ], MiniGridEnv . Actions ]], nrollouts : int , # equivalent to batch size in a PyTorch RNN rollout_len : int , # number of steps in each rollout ... ): super () . __init__ () ... self . data = data self . nrollouta = nrollouts self . rollout_len = rollout_len ... def get_data_for_rollout_ind ( self , rollout_ind : int ) -> Dict [ str , np . ndarray ]: masks : List [ bool ] = [] ... while len ( masks ) != self . rollout_len : # collect data, including an is_first_obs boolean for rollout_ind, # or raise StopIteration if finished ... masks . append ( not is_first_obs ) ... return { \"masks\" : np . array ( masks , dtype = np . float32 ) . reshape ( ( self . rollout_len , 1 , 1 ) # steps x agent x mask ), ... } def __next__ ( self ) -> Dict [ str , torch . Tensor ]: all_data = defaultdict ( lambda : []) for rollout_ind in range ( self . nrollouts ): data_for_ind = self . get_data_for_rollout_ind ( rollout_ind = rollout_ind ) for key in data_for_ind : all_data [ key ] . append ( data_for_ind [ key ]) return { key : torch . from_numpy ( np . stack ( all_data [ key ], axis = 1 )) # new sampler dim for key in all_data } A complete example can be found in ExpertTrajectoryIterator . Loss function # Off-policy losses must implement the AbstractOffPolicyLoss interface. In this case, we minimize the cross-entropy between the actor's policy and the expert action: class MiniGridOffPolicyExpertCELoss ( AbstractOffPolicyLoss [ ActorCriticModel ]): def loss ( self , model : ActorCriticModel , batch : ObservationType , memory : Memory , ) -> Tuple [ torch . FloatTensor , Dict [ str , float ], Memory , int ]: rollout_len , nrollouts = batch [ \"minigrid_ego_image\" ] . shape [: 2 ] # Initialize Memory if empty if len ( memory ) == 0 : for key in model . recurrent_memory_specification : dims_template , dtype = spec [ key ] # get sampler_dim and all_dims from dims_template (and nrollouts) ... memory . check_append ( key = key , tensor = torch . zeros ( * all_dims , dtype = dtype , device = batch [ \"minigrid_ego_image\" ] . device ), sampler_dim = sampler_dim , ) # Forward data (through the actor and critic) ac_out , memory = model . forward ( observations = batch , memory = memory , prev_actions = None , # unused by BabyAI ActorCriticModel masks = batch [ \"masks\" ], ) # Compute the loss from the actor's output and expert action expert_ce_loss = - ac_out . distributions . log_probs ( batch [ \"expert_action\" ]) . mean () info = { \"expert_ce\" : expert_ce_loss . item ()} return expert_ce_loss , info , memory , rollout_len * nrollouts A complete example can be found in MiniGridOffPolicyExpertCELoss . Note that in this case we train the entire actor, but it would also be possible to forward data through a different subgraph of the ActorCriticModel. Experiment configuration # For the experiment configuration, we'll build on top of an existing base BabyAI GoToLocal Experiment Config . The complete ExperimentConfig file for off-policy training is here , but let's focus on the most relevant aspect to enable this type of training: providing an OffPolicyPipelineComponent object as input to a PipelineStage when instantiating the TrainingPipeline in the training_pipeline method. class BCOffPolicyBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ): ... DATA = babyai . utils . load_demos ( os . path . join ( BABYAI_EXPERT_TRAJECTORIES_DIR , \"BabyAI-GoToLocal-v0 {} .pkl\" . format ( \"\" if torch . cuda . is_available () else \"-small\" ), ) ) @classmethod def tag ( cls ): return \"BabyAIGoToLocalBCOffPolicy\" @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = int ( 1e7 ) num_steps = 128 return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), # As we don't have any on-policy losses, we set the next # two values to zero to ensure we don't attempt to # compute gradients for on-policy rollouts: num_mini_batch = 0 , update_repeats = 0 , num_steps = num_steps // 4 , # rollout length for tasks sampled from env. # Instantiate the off-policy loss named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss (), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ # Single stage, only with off-policy training PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # keep sampling episodes in the stage # Enable off-policy training: offpolicy_component = OffPolicyPipelineComponent ( # Pass a method to instantiate data iterators data_iterator_builder = lambda ** kwargs : ExpertTrajectoryIterator ( data = cls . DATA , nrollouts = 128 , # per trainer batch size rollout_len = num_steps , # For truncated-BPTT ** kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = 16 , # 16 batches per rollout ), ), ], ) You'll have noted that it is possible to combine on-policy and off-policy training in the same stage, even though here we apply pure off-policy training. Training # We recommend using a machine with a CUDA-capable GPU for this experiment. In order to start training, we just need to invoke python main.py -b projects/tutorials babyai_go_to_local_bc_offpolicy -m 8 -o <OUTPUT_PATH> Note that with the -m 8 option we limit to 8 the number of on-policy task sampling processes used between off-policy updates. If everything goes well, the training success should quickly reach values around 0.7-0.8 on GPU and converge to values close to 1 if given sufficient time to train. If running tensorboard, you'll notice a separate group of scalars named offpolicy with losses, approximate frame rate and other tracked values in addition to the standard train used for on-policy training. A view of the training progress about 5 minutes after starting on a CUDA-capable GPU should look similar to","title":"Off-policy training"},{"location":"tutorials/offpolicy-tutorial/#tutorial-off-policy-training","text":"In this tutorial we'll learn how to train an agent from an external dataset by imitating expert actions via Behavior Cloning. We'll use a BabyAI agent to solve GoToLocal tasks on MiniGrid ; see the projects/babyai_baselines/experiments/go_to_local directory for more details. This tutorial assumes the installation instructions have already been followed and, to some extent, this framework's abstractions are known.","title":"Tutorial: Off-policy training"},{"location":"tutorials/offpolicy-tutorial/#the-task","text":"In a GoToLocal task, the agent immersed in a grid world has to navigate to a specific object in the presence of multiple distractors, requiring the agent to understand go to instructions like \"go to the red ball\". For further details, please consult the original paper .","title":"The task"},{"location":"tutorials/offpolicy-tutorial/#getting-the-dataset","text":"We will use a large dataset ( more than 4 GB ) including expert demonstrations for GoToLocal tasks. To download the data we'll run PYTHONPATH = . python plugins/babyai_plugin/scripts/download_babyai_expert_demos.py GoToLocal from the project's root directory, which will download BabyAI-GoToLocal-v0.pkl and BabyAI-GoToLocal-v0_valid.pkl to the plugins/babyai_plugin/data/demos directory. We will also generate small versions of the datasets, which will be useful if running on CPU, by calling PYTHONPATH = . python plugins/babyai_plugin/scripts/truncate_expert_demos.py from the project's root directory, which will generate BabyAI-GoToLocal-v0-small.pkl under the same plugins/babyai_plugin/data/demos directory.","title":"Getting the dataset"},{"location":"tutorials/offpolicy-tutorial/#data-iterator","text":"In order to train with an off-policy dataset, we need to define a data Iterator . The Data Iterator merges the functionality of the Dataset and Dataloader in PyTorch, in that it defines the way to both sample data from the dataset and convert them into batches to be used for training. An example of a Data Iterator for BabyAI expert demos might look as follows: class ExpertTrajectoryIterator ( Iterator ): def __init__ ( self , data : List [ Tuple [ str , bytes , List [ int ], MiniGridEnv . Actions ]], nrollouts : int , # equivalent to batch size in a PyTorch RNN rollout_len : int , # number of steps in each rollout ... ): super () . __init__ () ... self . data = data self . nrollouta = nrollouts self . rollout_len = rollout_len ... def get_data_for_rollout_ind ( self , rollout_ind : int ) -> Dict [ str , np . ndarray ]: masks : List [ bool ] = [] ... while len ( masks ) != self . rollout_len : # collect data, including an is_first_obs boolean for rollout_ind, # or raise StopIteration if finished ... masks . append ( not is_first_obs ) ... return { \"masks\" : np . array ( masks , dtype = np . float32 ) . reshape ( ( self . rollout_len , 1 , 1 ) # steps x agent x mask ), ... } def __next__ ( self ) -> Dict [ str , torch . Tensor ]: all_data = defaultdict ( lambda : []) for rollout_ind in range ( self . nrollouts ): data_for_ind = self . get_data_for_rollout_ind ( rollout_ind = rollout_ind ) for key in data_for_ind : all_data [ key ] . append ( data_for_ind [ key ]) return { key : torch . from_numpy ( np . stack ( all_data [ key ], axis = 1 )) # new sampler dim for key in all_data } A complete example can be found in ExpertTrajectoryIterator .","title":"Data iterator"},{"location":"tutorials/offpolicy-tutorial/#loss-function","text":"Off-policy losses must implement the AbstractOffPolicyLoss interface. In this case, we minimize the cross-entropy between the actor's policy and the expert action: class MiniGridOffPolicyExpertCELoss ( AbstractOffPolicyLoss [ ActorCriticModel ]): def loss ( self , model : ActorCriticModel , batch : ObservationType , memory : Memory , ) -> Tuple [ torch . FloatTensor , Dict [ str , float ], Memory , int ]: rollout_len , nrollouts = batch [ \"minigrid_ego_image\" ] . shape [: 2 ] # Initialize Memory if empty if len ( memory ) == 0 : for key in model . recurrent_memory_specification : dims_template , dtype = spec [ key ] # get sampler_dim and all_dims from dims_template (and nrollouts) ... memory . check_append ( key = key , tensor = torch . zeros ( * all_dims , dtype = dtype , device = batch [ \"minigrid_ego_image\" ] . device ), sampler_dim = sampler_dim , ) # Forward data (through the actor and critic) ac_out , memory = model . forward ( observations = batch , memory = memory , prev_actions = None , # unused by BabyAI ActorCriticModel masks = batch [ \"masks\" ], ) # Compute the loss from the actor's output and expert action expert_ce_loss = - ac_out . distributions . log_probs ( batch [ \"expert_action\" ]) . mean () info = { \"expert_ce\" : expert_ce_loss . item ()} return expert_ce_loss , info , memory , rollout_len * nrollouts A complete example can be found in MiniGridOffPolicyExpertCELoss . Note that in this case we train the entire actor, but it would also be possible to forward data through a different subgraph of the ActorCriticModel.","title":"Loss function"},{"location":"tutorials/offpolicy-tutorial/#experiment-configuration","text":"For the experiment configuration, we'll build on top of an existing base BabyAI GoToLocal Experiment Config . The complete ExperimentConfig file for off-policy training is here , but let's focus on the most relevant aspect to enable this type of training: providing an OffPolicyPipelineComponent object as input to a PipelineStage when instantiating the TrainingPipeline in the training_pipeline method. class BCOffPolicyBabyAIGoToLocalExperimentConfig ( BaseBabyAIGoToLocalExperimentConfig ): ... DATA = babyai . utils . load_demos ( os . path . join ( BABYAI_EXPERT_TRAJECTORIES_DIR , \"BabyAI-GoToLocal-v0 {} .pkl\" . format ( \"\" if torch . cuda . is_available () else \"-small\" ), ) ) @classmethod def tag ( cls ): return \"BabyAIGoToLocalBCOffPolicy\" @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = int ( 1e7 ) num_steps = 128 return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), # As we don't have any on-policy losses, we set the next # two values to zero to ensure we don't attempt to # compute gradients for on-policy rollouts: num_mini_batch = 0 , update_repeats = 0 , num_steps = num_steps // 4 , # rollout length for tasks sampled from env. # Instantiate the off-policy loss named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss (), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ # Single stage, only with off-policy training PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # keep sampling episodes in the stage # Enable off-policy training: offpolicy_component = OffPolicyPipelineComponent ( # Pass a method to instantiate data iterators data_iterator_builder = lambda ** kwargs : ExpertTrajectoryIterator ( data = cls . DATA , nrollouts = 128 , # per trainer batch size rollout_len = num_steps , # For truncated-BPTT ** kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = 16 , # 16 batches per rollout ), ), ], ) You'll have noted that it is possible to combine on-policy and off-policy training in the same stage, even though here we apply pure off-policy training.","title":"Experiment configuration"},{"location":"tutorials/offpolicy-tutorial/#training","text":"We recommend using a machine with a CUDA-capable GPU for this experiment. In order to start training, we just need to invoke python main.py -b projects/tutorials babyai_go_to_local_bc_offpolicy -m 8 -o <OUTPUT_PATH> Note that with the -m 8 option we limit to 8 the number of on-policy task sampling processes used between off-policy updates. If everything goes well, the training success should quickly reach values around 0.7-0.8 on GPU and converge to values close to 1 if given sufficient time to train. If running tensorboard, you'll notice a separate group of scalars named offpolicy with losses, approximate frame rate and other tracked values in addition to the standard train used for on-policy training. A view of the training progress about 5 minutes after starting on a CUDA-capable GPU should look similar to","title":"Training"},{"location":"tutorials/running_inference_on_a_pretrained_model/","text":"Tutorial: Inference with a pre-trained model # In this tutorial we will run inference on a pre-trained model for the PointNav task in the RoboTHOR environment. In this task the agent is tasked with going to a specific location within a realistic 3D environment. For information on how to train a PointNav Model see this tutorial We will need to install the RoboTHOR environment and download the RoboTHOR Pointnav dataset before we get started. For this tutorial we will download the weights of a model trained on the debug dataset. This can be done with a handy script in the pretrained_model_ckpts directory: bash pretrained_model_ckpts/download_navigation_model_ckpts.sh robothor-pointnav-rgb-resnet This will download the weights for an RGB model that has been trained on the PointNav task in RoboTHOR to pretrained_model_ckpts/robothor-pointnav-rgb-resnet Next we need to run the inference, using the PointNav experiment config from the tutorial on making a PointNav experiment . We can do this with the following command: python main.py -o <PATH_TO_OUTPUT> -c <PATH_TO_CHECKPOINT> -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> -t <TIMESTAMP> Where <PATH_TO_OUTPUT> is the location where the results of the test will be dumped, <PATH_TO_CHECKPOINT> is the location of the downloaded model weights, <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> is a path to the directory where our experiment definition is stored, and <TIMESTAMP> is the unique timestamp associated with when the model was trained. For our current setup the following command would work: python main.py \\ pointnav_robothor_rgb_ddppo \\ -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -c pretrained_model_ckpts/robothor-pointnav-rgb-resnet/checkpoints/PointNavRobothorRGBPPO/2020-08-31_12-13-30/exp_PointNavRobothorRGBPPO__stage_00__steps_000039031200.pt \\ -b projects/tutorials \\ -t 2020 -08-31_12-13-30 For testing on all saved checkpoints we just need to omit <PATH_TO_CHECKPOINT> : python main.py \\ pointnav_robothor_rgb_ddppo \\ -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -b projects/tutorials \\ -t 2020 -08-31_12-13-30 Visualization # We also show examples of visualizations that can be extracted from the \"valid\" and \"test\" modes. Currently, visualization is still undergoing design changes and does not support multi-agent tasks, but the available functionality is sufficient for pointnav in RoboThor. Following up on the example above, we can make a specialized pontnav ExperimentConfig where we instantiate the base visualization class, VizSuite , defined in utils.viz_utils , when in test mode. Each visualization type can be thought of as a plugin to the base VizSuite . For example, all episode_ids passed to VizSuite will be processed with each of the instantiated visualization types (possibly with the exception of the AgentViewViz ). In the example below we show how to instantiate different visualization types from 4 different data sources. The data sources available to VizSuite are: Task output (e.g. 2D trajectories) Vector task (e.g. egocentric views) Rollout storage (e.g. recurrent memory, taken action logprobs...) ActorCriticOutput (e.g. action probabilities) The visualization types included below are: TrajectoryViz : Generic 2D trajectory view. AgentViewViz : RGB egocentric view. ActorViz : Action probabilities from ActorCriticOutput[CategoricalDistr] . TensorViz1D : Evolution of a point from RolloutStorage over time. TensorViz2D : Evolution of a vector from RolloutStorage over time. ThorViz : Specialized 2D trajectory view for RoboThor . Note that we need to explicitly set the episode_ids that we wish to visualize. For AgentViewViz we have the option of using a different (typically shorter) list of episodes or enforce the ones used for the rest of visualizations. class PointNavRoboThorRGBPPOVizExperimentConfig ( PointNavRoboThorRGBPPOExperimentConfig ): ... viz_ep_ids = [ \"FloorPlan_Train1_1_0\" , \"FloorPlan_Train1_1_7\" , \"FloorPlan_Train1_1_11\" , \"FloorPlan_Train1_1_12\" , ] viz_video_ids = [[ \"FloorPlan_Train1_1_7\" ], [ \"FloorPlan_Train1_1_11\" ]] viz : Optional [ VizSuite ] = None def get_viz ( self , mode ): if self . viz is not None : return self . viz self . viz = VizSuite ( episode_ids = self . viz_ep_ids , mode = mode , # Basic 2D trajectory visualizer (task output source): base_trajectory = TrajectoryViz ( path_to_target_location = ( \"task_info\" , \"target\" ,), ), # Egocentric view visualizer (vector task source): egeocentric = AgentViewViz ( max_video_length = 100 , episode_ids = self . viz_video_ids ), # Default action probability visualizer (actor critic output source): action_probs = ActorViz ( figsize = ( 3.25 , 10 ), fontsize = 18 ), # Default taken action logprob visualizer (rollout storage source): taken_action_logprobs = TensorViz1D (), # Same episode mask visualizer (rollout storage source): episode_mask = TensorViz1D ( rollout_source = ( \"masks\" ,)), # Default recurrent memory visualizer (rollout storage source): rnn_memory = TensorViz2D (), # Specialized 2D trajectory visualizer (task output source): thor_trajectory = ThorViz ( figsize = ( 16 , 8 ), viz_rows_cols = ( 448 , 448 ), scenes = ( \"FloorPlan_Train {} _ {} \" , 1 , 1 , 1 , 1 ), ), ) return self . viz def machine_params ( self , mode = \"train\" , ** kwargs ): res = super () . machine_params ( mode , ** kwargs ) res [ \"visualizer\" ] = None if mode == \"test\" : res [ \"visualizer\" ] = self . get_viz ( mode ) return res Running test on the same downloaded models, but using the visualization-enabled ExperimentConfig with python main.py \\ pointnav_robothor_rgb_ddppo_viz -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -c pretrained_model_ckpts/robothor-pointnav-rgb-resnet/checkpoints/PointNavRobothorRGBPPO/2020-08-31_12-13-30/exp_PointNavRobothorRGBPPO__stage_00__steps_000039031200.pt \\ -b projects/tutorials \\ -t 2020 -08-31_12-13-30 generates different types of visualization and logs them in tensorboard. If everything is properly setup and tensorboard includes the robothor-pointnav-rgb-resnet folder, under the IMAGES tab, we should see something similar to","title":"Using a pre-trained model"},{"location":"tutorials/running_inference_on_a_pretrained_model/#tutorial-inference-with-a-pre-trained-model","text":"In this tutorial we will run inference on a pre-trained model for the PointNav task in the RoboTHOR environment. In this task the agent is tasked with going to a specific location within a realistic 3D environment. For information on how to train a PointNav Model see this tutorial We will need to install the RoboTHOR environment and download the RoboTHOR Pointnav dataset before we get started. For this tutorial we will download the weights of a model trained on the debug dataset. This can be done with a handy script in the pretrained_model_ckpts directory: bash pretrained_model_ckpts/download_navigation_model_ckpts.sh robothor-pointnav-rgb-resnet This will download the weights for an RGB model that has been trained on the PointNav task in RoboTHOR to pretrained_model_ckpts/robothor-pointnav-rgb-resnet Next we need to run the inference, using the PointNav experiment config from the tutorial on making a PointNav experiment . We can do this with the following command: python main.py -o <PATH_TO_OUTPUT> -c <PATH_TO_CHECKPOINT> -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> -t <TIMESTAMP> Where <PATH_TO_OUTPUT> is the location where the results of the test will be dumped, <PATH_TO_CHECKPOINT> is the location of the downloaded model weights, <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> is a path to the directory where our experiment definition is stored, and <TIMESTAMP> is the unique timestamp associated with when the model was trained. For our current setup the following command would work: python main.py \\ pointnav_robothor_rgb_ddppo \\ -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -c pretrained_model_ckpts/robothor-pointnav-rgb-resnet/checkpoints/PointNavRobothorRGBPPO/2020-08-31_12-13-30/exp_PointNavRobothorRGBPPO__stage_00__steps_000039031200.pt \\ -b projects/tutorials \\ -t 2020 -08-31_12-13-30 For testing on all saved checkpoints we just need to omit <PATH_TO_CHECKPOINT> : python main.py \\ pointnav_robothor_rgb_ddppo \\ -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -b projects/tutorials \\ -t 2020 -08-31_12-13-30","title":"Tutorial: Inference with a pre-trained model"},{"location":"tutorials/running_inference_on_a_pretrained_model/#visualization","text":"We also show examples of visualizations that can be extracted from the \"valid\" and \"test\" modes. Currently, visualization is still undergoing design changes and does not support multi-agent tasks, but the available functionality is sufficient for pointnav in RoboThor. Following up on the example above, we can make a specialized pontnav ExperimentConfig where we instantiate the base visualization class, VizSuite , defined in utils.viz_utils , when in test mode. Each visualization type can be thought of as a plugin to the base VizSuite . For example, all episode_ids passed to VizSuite will be processed with each of the instantiated visualization types (possibly with the exception of the AgentViewViz ). In the example below we show how to instantiate different visualization types from 4 different data sources. The data sources available to VizSuite are: Task output (e.g. 2D trajectories) Vector task (e.g. egocentric views) Rollout storage (e.g. recurrent memory, taken action logprobs...) ActorCriticOutput (e.g. action probabilities) The visualization types included below are: TrajectoryViz : Generic 2D trajectory view. AgentViewViz : RGB egocentric view. ActorViz : Action probabilities from ActorCriticOutput[CategoricalDistr] . TensorViz1D : Evolution of a point from RolloutStorage over time. TensorViz2D : Evolution of a vector from RolloutStorage over time. ThorViz : Specialized 2D trajectory view for RoboThor . Note that we need to explicitly set the episode_ids that we wish to visualize. For AgentViewViz we have the option of using a different (typically shorter) list of episodes or enforce the ones used for the rest of visualizations. class PointNavRoboThorRGBPPOVizExperimentConfig ( PointNavRoboThorRGBPPOExperimentConfig ): ... viz_ep_ids = [ \"FloorPlan_Train1_1_0\" , \"FloorPlan_Train1_1_7\" , \"FloorPlan_Train1_1_11\" , \"FloorPlan_Train1_1_12\" , ] viz_video_ids = [[ \"FloorPlan_Train1_1_7\" ], [ \"FloorPlan_Train1_1_11\" ]] viz : Optional [ VizSuite ] = None def get_viz ( self , mode ): if self . viz is not None : return self . viz self . viz = VizSuite ( episode_ids = self . viz_ep_ids , mode = mode , # Basic 2D trajectory visualizer (task output source): base_trajectory = TrajectoryViz ( path_to_target_location = ( \"task_info\" , \"target\" ,), ), # Egocentric view visualizer (vector task source): egeocentric = AgentViewViz ( max_video_length = 100 , episode_ids = self . viz_video_ids ), # Default action probability visualizer (actor critic output source): action_probs = ActorViz ( figsize = ( 3.25 , 10 ), fontsize = 18 ), # Default taken action logprob visualizer (rollout storage source): taken_action_logprobs = TensorViz1D (), # Same episode mask visualizer (rollout storage source): episode_mask = TensorViz1D ( rollout_source = ( \"masks\" ,)), # Default recurrent memory visualizer (rollout storage source): rnn_memory = TensorViz2D (), # Specialized 2D trajectory visualizer (task output source): thor_trajectory = ThorViz ( figsize = ( 16 , 8 ), viz_rows_cols = ( 448 , 448 ), scenes = ( \"FloorPlan_Train {} _ {} \" , 1 , 1 , 1 , 1 ), ), ) return self . viz def machine_params ( self , mode = \"train\" , ** kwargs ): res = super () . machine_params ( mode , ** kwargs ) res [ \"visualizer\" ] = None if mode == \"test\" : res [ \"visualizer\" ] = self . get_viz ( mode ) return res Running test on the same downloaded models, but using the visualization-enabled ExperimentConfig with python main.py \\ pointnav_robothor_rgb_ddppo_viz -o pretrained_model_ckpts/robothor-pointnav-rgb-resnet/ \\ -c pretrained_model_ckpts/robothor-pointnav-rgb-resnet/checkpoints/PointNavRobothorRGBPPO/2020-08-31_12-13-30/exp_PointNavRobothorRGBPPO__stage_00__steps_000039031200.pt \\ -b projects/tutorials \\ -t 2020 -08-31_12-13-30 generates different types of visualization and logs them in tensorboard. If everything is properly setup and tensorboard includes the robothor-pointnav-rgb-resnet folder, under the IMAGES tab, we should see something similar to","title":"Visualization"},{"location":"tutorials/training-a-pointnav-model/","text":"Tutorial: PointNav in RoboTHOR # Introduction # One of the most obvious tasks that an embodied agent should master is navigating the world it inhabits. Before we can teach a robot to cook or clean it first needs to be able to move around. The simplest way to formulate \"moving around\" into a task is by making your agent find a beacon somewhere in the environment. This beacon transmits its location, such that at any time, the agent can get the direction and euclidian distance to the beacon. This particular task is often called Point Navigation, or PointNav for short. PointNav # At first glance, this task seems trivial. If the agent is given the direction and distance of the target at all times, can it not simply follow this signal directly? The answer is no, because agents are often trained on this task in environments that emulate real-world buildings which are not wide-open spaces, but rather contain many smaller rooms. Because of this, the agent has to learn to navigate human spaces and use doors and hallways to efficiently navigate from one side of the building to the other. This task becomes particularly difficult when the agent is tested in an environment that it is not trained in. If the agent does not know how the floor plan of an environment looks, it has to learn to predict the design of man-made structures, to efficiently navigate across them, much like how people instinctively know how to move around a building they have never seen before based on their experience navigating similar buildings. What is an environment anyways? # Environments are worlds in which embodied agents exist. If our embodied agent is simply a neural network that is being trained in a simulator, then that simulator is its environment. Similarly, if our agent is a physical robot then its environment is the real world. The agent interacts with the environment by taking one of several available actions (such as \"move forward\", or \"turn left\"). After each action, the environment produces a new frame that the agent can analyze to determine its next step. For many tasks, including PointNav the agent also has a special \"stop\" action which indicates that the agent thinks it has reached the target. After this action is called the agent will be reset to a new location, regardless if it reached the target. The hope is that after enough training the agent will learn to correctly assess that it has successfully navigated to the target. There are many simulators designed for the training of embodied agents. In this tutorial, we will be using a simulator called RoboTHOR , which is designed specifically to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment that researchers can have access to. RoboTHOR contains 60 different virtual scenes with different floor plans and furniture and 15 validation scenes. It is also important to mention that AllenAct has a class abstraction called Environment. This is not the actual simulator game engine or robotics controller, but rather a shallow wrapper that provides a uniform interface to the actual environment. Learning algorithm # Finally, let us briefly touch on the algorithm that we will use to train our embodied agent to navigate. While AllenAct offers us great flexibility to train models using complex pipelines, we will be using a simple pure reinforcement learning approach for this tutorial. More specifically, we will be using DD-PPO, a decentralized and distributed variant of the ubiquitous PPO algorithm. For those unfamiliar with Reinforcement Learning we highly recommend this tutorial by Andrej Karpathy, and this book by Sutton and Barto. Essentially what we are doing is letting our agent explore the environment on its own, rewarding it for taking actions that bring it closer to its goal and penalizing it for actions that take it away from its goal. We then optimize the agent's model to maximize this reward. Requirements # To train the model on the PointNav task, we need to install the RoboTHOR environment and download the RoboTHOR PointNav dataset The dataset contains a list of episodes with thousands of randomly generated starting positions and target locations for each of the scenes as well as a precomputed cache of distances, containing the shortest path from each point in a scene, to every other point in that scene. This is used to reward the agent for moving closer to the target in terms of geodesic distance - the actual path distance (as opposed to a straight line distance). Config File Setup # Now comes the most important part of the tutorial, we are going to write an experiment config file. If this is your first experience with experiment config files in AllenAct, we suggest that you first see our how-to on defining an experiment which will walk you through creating a simplified experiment config file. Unlike a library that can be imported into python, AllenAct is structured as a framework with a runner script called main.py which will run the experiment specified in a config file. This design forces us to keep meticulous records of exactly which settings were used to produce a particular result, which can be very useful given how expensive RL models are to train. The projects/ directory is home to different projects using AllenAct . Currently it is populated with baselines of popular tasks and tutorials. We already have all the code for this tutorial stored in projects/tutorials/pointnav_robothor_rgb_ddppo.py . We will be using this file to run our experiments, but you can create a new directory in projects/ and start writing your experiment there. We start off by importing everything we will need: import glob from math import ceil from typing import Dict , Any , List , Optional import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from torchvision import models from core.algorithms.onpolicy_sync.losses import PPO from core.algorithms.onpolicy_sync.losses.ppo import PPOConfig from projects.pointnav_baselines.models.point_nav_models import ( ResnetTensorPointNavActorCritic , ) from plugins.ithor_plugin.ithor_sensors import RGBSensorThor from core.base_abstractions.experiment_config import ExperimentConfig from core.base_abstractions.preprocessor import ObservationSet from core.base_abstractions.task import TaskSampler from plugins.habitat_plugin.habitat_preprocessors import ResnetPreProcessorHabitat from plugins.robothor_plugin.robothor_sensors import GPSCompassSensorRoboThor from plugins.robothor_plugin.robothor_task_samplers import PointNavDatasetTaskSampler from plugins.robothor_plugin.robothor_tasks import PointNavTask from utils.experiment_utils import Builder , PipelineStage , TrainingPipeline , LinearDecay Next we define a new experiment config class: class PointNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ): We then define the task parameters. For PointNav, these include the maximum number of steps our agent can take before being reset (this prevents the agent from wandering on forever), and a configuration for the reward function that we will be using. # Task Parameters MAX_STEPS = 500 REWARD_CONFIG = { \"step_penalty\" : - 0.01 , \"goal_success_reward\" : 10.0 , \"failed_stop_reward\" : 0.0 , \"shaping_weight\" : 1.0 , } In this case, we set the maximum number of steps to 500. We give the agent a reward of -0.01 for each action that it takes (this is to encourage it to reach the goal in as few actions as possible), and a reward of 10.0 if the agent manages to successfully reach its destination. If the agent selects the stop action without reaching the target we do not punish it (although this is sometimes useful for preventing the agent from stopping prematurely). Finally, our agent gets rewarded if it moves closer to the target and gets punished if it moves further away. shaping_weight controls how strong this signal should be and is here set to 1.0. These parameters work well for training an agent on PointNav, but feel free to play around with them. Next, we set the parameters of the simulator itself. Here we select a resolution at which the engine will render every frame (640 by 480) and a resolution at which the image will be fed into the neural network (here it is set to a 224 by 224 box). # Simulator Parameters CAMERA_WIDTH = 640 CAMERA_HEIGHT = 480 SCREEN_SIZE = 224 Next, we set the hardware parameters for the training engine. NUM_PROCESSES sets the total number of parallel processes that will be used to train the model. In general, more processes result in faster training, but since each process is a unique instance of the environment in which we are training they can take up a lot of memory. Depending on the size of the model, the environment, and the hardware we are using, we may need to adjust this number, but for a setup with 8 GTX Titans, 60 processes work fine. 60 also happens to be the number of training scenes in RoboTHOR, which allows each process to load only a single scene into memory, saving time and space. TRAINING_GPUS takes the ids of the GPUS on which the model should be trained. Similarly VALIDATION_GPUS and TESTING_GPUS hold the ids of the GPUS on which the validation and testing will occur. During training, a validation process is constantly running and evaluating the current model, to show the progress on the validation set, so reserving a GPU for validation can be a good idea. If our hardware setup does not include a GPU, these fields can be set to empty lists, as the codebase will default to running everything on the CPU with only 1 process. # Training Engine Parameters ADVANCE_SCENE_ROLLOUT_PERIOD : Optional [ int ] = None NUM_PROCESSES = 60 TRAINING_GPUS = list ( range ( torch . cuda . device_count ())) VALIDATION_GPUS = [ torch . cuda . device_count () - 1 ] TESTING_GPUS = [ torch . cuda . device_count () - 1 ] Since we are using a dataset to train our model we need to define the path to where we have stored it. If we download the dataset instructed above we can define the path as follows # Dataset Parameters TRAIN_DATASET_DIR = \"datasets/robothor-pointnav/train\" VAL_DATASET_DIR = \"datasets/robothor-pointnav/val\" Next, we define the sensors. RGBSensorThor is the environment's implementation of an RGB sensor. It takes the raw image outputted by the simulator and resizes it, to the input dimensions for our neural network that we specified above. It also performs normalization if we want. GPSCompassSensorRoboThor is a sensor that tracks the point our agent needs to move to. It tells us the direction and distance to our goal at every time step. SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , uuid = \"rgb_lowres\" , ), GPSCompassSensorRoboThor (), ] For the sake of this example, we are also going to be using a preprocessor with our model. In AllenAct the preprocessor abstraction is designed with large models with frozen weights in mind. These models often hail from the ResNet family and transform the raw pixels that our agent observes in the environment, into a complex embedding, which then gets stored and used as input to our trainable model instead of the original image. Most other preprocessing work is done in the sensor classes (as we just saw with the RGB sensor scaling and normalizing our input), but for the sake of efficiency, all neural network preprocessing should use this abstraction. PREPROCESSORS = [ Builder ( ResnetPreProcessorHabitat , { \"input_height\" : SCREEN_SIZE , \"input_width\" : SCREEN_SIZE , \"output_width\" : 7 , \"output_height\" : 7 , \"output_dims\" : 512 , \"pool\" : False , \"torchvision_resnet_model\" : models . resnet18 , \"input_uuids\" : [ \"rgb_lowres\" ], \"output_uuid\" : \"rgb_resnet\" , \"parallel\" : False , # TODO False for debugging } ), ] Next, we must define all of the observation inputs that our model will use. These are just the hardcoded ids of the sensors we are using in the experiment. OBSERVATIONS = [ \"rgb_resnet\" , \"target_coordinates_ind\" , ] Finally, we must define the settings of our simulator. We set the camera dimensions to the values we defined earlier. We set rotateStepDegrees to 30 degrees, which means that every time the agent takes a turn action, they will rotate by 30 degrees. We set grid size to 0.25 which means that every time the agent moves forward, it will do so by 0.25 meters. ENV_ARGS = dict ( width = CAMERA_WIDTH , height = CAMERA_HEIGHT , rotateStepDegrees = 30.0 , gridSize = 0.25 , ) Now we move on to the methods that we must define to finish implementing an experiment config. Firstly we have a simple method that just returns the name of the experiment. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" Next, we define the training pipeline. In this function, we specify exactly which algorithm or algorithms we will use to train our model. In this simple example, we are using the PPO loss with a learning rate of 3e-4. We specify 250 million steps of training and a rollout length of 30 with the ppo_steps and num_steps parameters respectively. All the other standard PPO parameters are also present in this function. metric_accumulate_interval sets the frequency at which data is accumulated from all the processes and logged while save_interval sets how often we save the model weights and run validation on them. @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 250000000 ) lr = 3e-4 num_mini_batch = 1 update_repeats = 3 num_steps = 30 save_interval = 5000000 metric_accumulate_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 0.95 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : Builder ( PPO , kwargs = {}, default = PPOConfig , )}, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) We define the helper method split_num_processes to split the different scenes that we want to train with amongst the different available devices. \"machine_params\" returns the hardware parameters of each process, based on the list of devices we defined above. def split_num_processes ( self , ndevices ): assert self . NUM_PROCESSES >= ndevices , \"NUM_PROCESSES {} < ndevices\" . format ( self . NUM_PROCESSES , ndevices ) res = [ 0 ] * ndevices for it in range ( self . NUM_PROCESSES ): res [ it % ndevices ] += 1 return res def machine_params ( self , mode = \"train\" , ** kwargs ): if mode == \"train\" : workers_per_device = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TRAINING_GPUS * workers_per_device nprocesses = 1 if not torch . cuda . is_available () else self . split_num_processes ( len ( gpu_ids )) sampler_devices = self . TRAINING_GPUS elif mode == \"valid\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . VALIDATION_GPUS elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TESTING_GPUS else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) # Disable parallelization for validation process if mode == \"valid\" : for prep in self . PREPROCESSORS : prep . kwargs [ \"parallel\" ] = False observation_set = Builder ( ObservationSet , kwargs = dict ( source_ids = self . OBSERVATIONS , all_preprocessors = self . PREPROCESSORS , all_sensors = self . SENSORS )) if mode == 'train' or nprocesses > 0 else None return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids , \"sampler_devices\" : sampler_devices if mode == \"train\" else gpu_ids , \"observation_set\" : observation_set , } Now we define the actual model that we will be using. AllenAct offers first-class support for PyTorch, so any PyTorch model that implements the provided ActorCriticModel class will work here. Here we borrow a model from the pointnav_baselines project (which unsurprisingly contains several PointNav baselines). It is a small convolutional network that expects the output of a ResNet as its rgb input followed by a single-layered GRU. The model accepts as input the number of different actions our agent can perform in the environment through the action_space parameter, which we get from the task definition. We also define the shape of the inputs we are going to be passing to the model with observation_space We specify the names of our sensors with goal_sensor_uuid and rgb_resnet_preprocessor_uuid . Finally, we define the size of our RNN with hidden_layer and the size of the embedding of our goal sensor data (the direction and distance to the target) with goal_dims . # Define Model @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ResnetTensorPointNavActorCritic ( action_space = gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), observation_space = kwargs [ \"observation_set\" ] . observation_spaces , goal_sensor_uuid = \"target_coordinates_ind\" , rgb_resnet_preprocessor_uuid = \"rgb_resnet\" , hidden_size = 512 , goal_dims = 32 , ) We also need to define the task sampler that we will be using. This is a piece of code that generates instances of tasks for our agent to perform (essentially starting locations and targets for PointNav). Since we are getting our tasks from a dataset, the task sampler is a very simple code that just reads the specified file and sets the agent to the next starting locations whenever the agent exceeds the maximum number of steps or selects the stop action. # Define Task Sampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavDatasetTaskSampler ( ** kwargs ) You might notice that we did not specify the task sampler's arguments, but are rather passing them in. The reason for this is that each process will have its own task sampler, and we need to specify exactly which scenes each process should work with. If we have several GPUS and many scenes this process of distributing the work can be rather complicated so we define a few helper functions to do just this. # Utility Functions for distributing scenes between GPUs @staticmethod def _partition_inds ( n : int , num_parts : int ): return np . round ( np . linspace ( 0 , n , num_parts + 1 , endpoint = True )) . astype ( np . int32 ) def _get_sampler_args_for_scene_split ( self , scenes_dir : str , process_ind : int , total_processes : int , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: path = os . path . join ( scenes_dir , \"*.json.gz\" ) scenes = [ scene . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for scene in glob . glob ( path )] if total_processes > len ( scenes ): # oversample some scenes -> bias if total_processes % len ( scenes ) != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisible by the number of scenes\" ) scenes = scenes * int ( ceil ( total_processes / len ( scenes ))) scenes = scenes [: total_processes * ( len ( scenes ) // total_processes )] else : if len ( scenes ) % total_processes != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisor of the number of scenes\" ) inds = self . _partition_inds ( len ( scenes ), total_processes ) return { \"scenes\" : scenes [ inds [ process_ind ]: inds [ process_ind + 1 ]], \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), \"seed\" : seeds [ process_ind ] if seeds is not None else None , \"deterministic_cudnn\" : deterministic_cudnn , \"rewards_config\" : self . REWARD_CONFIG } The very last things we need to define are the sampler arguments themselves. We define them separately for a train, validation, and test sampler, but in this case, they are almost the same. The arguments need to include the location of the dataset and distance cache as well as the environment arguments for our simulator, both of which we defined above and are just referencing here. The only consequential differences between these task samplers are the path to the dataset we are using (train or validation) and whether we want to loop over the dataset or not (we want this for training since we want to train for several epochs, but we do not need this for validation and testing). Since the test scenes of RoboTHOR are private we are also testing on our validation set. def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( os . path . join ( self . TRAIN_DATASET_DIR , \"episodes\" ), process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . TRAIN_DATASET_DIR res [ \"loop_dataset\" ] = True res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) res [ \"allow_flipping\" ] = True return res def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) return res def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = \"10.0\" return res This is it! If we copy all of the code into a file we should be able to run our experiment! Training Model On Debug Dataset # We can test if our installation worked properly by training our model on a small dataset of 4 episodes. This should take about 20 minutes on a computer with a NVIDIA GPU. First we need to change the dataset path to point to our small debug dataset. Modify these lines in your file # Dataset Parameters TRAIN_DATASET_DIR = \"datasets/robothor-pointnav/debug\" VAL_DATASET_DIR = \"datasets/robothor-pointnav/debug\" Note that we changed both the train and test dataset to debug, so the model will train and validate on the same 4 episodes. We can now train a model by running: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> If using the same configuration as we have set up, the following command should work: python main.py -o storage/robothor-pointnav-rgb-resnet-resnet -b projects/tutorials pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this: Training Model On Full Dataset # We can also train the model on the full dataset by changing back our dataset path and running the same command as above. But be aware, training this takes nearly 2 days on a machine with 8 GPU. Testing Model # To test the performance of a model please refer to this tutorial . Conclusion # In this tutorial, we learned how to create a new PointNav experiment using AllenAct . There are many simple and obvious ways to modify the experiment from here - changing the model, the learning algorithm and the environment each requires very few lines of code changed in the above file, allowing us to explore our embodied ai research ideas across different frameworks with ease.","title":"PointNav in RoboTHOR"},{"location":"tutorials/training-a-pointnav-model/#tutorial-pointnav-in-robothor","text":"","title":"Tutorial: PointNav in RoboTHOR"},{"location":"tutorials/training-a-pointnav-model/#introduction","text":"One of the most obvious tasks that an embodied agent should master is navigating the world it inhabits. Before we can teach a robot to cook or clean it first needs to be able to move around. The simplest way to formulate \"moving around\" into a task is by making your agent find a beacon somewhere in the environment. This beacon transmits its location, such that at any time, the agent can get the direction and euclidian distance to the beacon. This particular task is often called Point Navigation, or PointNav for short.","title":"Introduction"},{"location":"tutorials/training-a-pointnav-model/#pointnav","text":"At first glance, this task seems trivial. If the agent is given the direction and distance of the target at all times, can it not simply follow this signal directly? The answer is no, because agents are often trained on this task in environments that emulate real-world buildings which are not wide-open spaces, but rather contain many smaller rooms. Because of this, the agent has to learn to navigate human spaces and use doors and hallways to efficiently navigate from one side of the building to the other. This task becomes particularly difficult when the agent is tested in an environment that it is not trained in. If the agent does not know how the floor plan of an environment looks, it has to learn to predict the design of man-made structures, to efficiently navigate across them, much like how people instinctively know how to move around a building they have never seen before based on their experience navigating similar buildings.","title":"PointNav"},{"location":"tutorials/training-a-pointnav-model/#what-is-an-environment-anyways","text":"Environments are worlds in which embodied agents exist. If our embodied agent is simply a neural network that is being trained in a simulator, then that simulator is its environment. Similarly, if our agent is a physical robot then its environment is the real world. The agent interacts with the environment by taking one of several available actions (such as \"move forward\", or \"turn left\"). After each action, the environment produces a new frame that the agent can analyze to determine its next step. For many tasks, including PointNav the agent also has a special \"stop\" action which indicates that the agent thinks it has reached the target. After this action is called the agent will be reset to a new location, regardless if it reached the target. The hope is that after enough training the agent will learn to correctly assess that it has successfully navigated to the target. There are many simulators designed for the training of embodied agents. In this tutorial, we will be using a simulator called RoboTHOR , which is designed specifically to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment that researchers can have access to. RoboTHOR contains 60 different virtual scenes with different floor plans and furniture and 15 validation scenes. It is also important to mention that AllenAct has a class abstraction called Environment. This is not the actual simulator game engine or robotics controller, but rather a shallow wrapper that provides a uniform interface to the actual environment.","title":"What is an environment anyways?"},{"location":"tutorials/training-a-pointnav-model/#learning-algorithm","text":"Finally, let us briefly touch on the algorithm that we will use to train our embodied agent to navigate. While AllenAct offers us great flexibility to train models using complex pipelines, we will be using a simple pure reinforcement learning approach for this tutorial. More specifically, we will be using DD-PPO, a decentralized and distributed variant of the ubiquitous PPO algorithm. For those unfamiliar with Reinforcement Learning we highly recommend this tutorial by Andrej Karpathy, and this book by Sutton and Barto. Essentially what we are doing is letting our agent explore the environment on its own, rewarding it for taking actions that bring it closer to its goal and penalizing it for actions that take it away from its goal. We then optimize the agent's model to maximize this reward.","title":"Learning algorithm"},{"location":"tutorials/training-a-pointnav-model/#requirements","text":"To train the model on the PointNav task, we need to install the RoboTHOR environment and download the RoboTHOR PointNav dataset The dataset contains a list of episodes with thousands of randomly generated starting positions and target locations for each of the scenes as well as a precomputed cache of distances, containing the shortest path from each point in a scene, to every other point in that scene. This is used to reward the agent for moving closer to the target in terms of geodesic distance - the actual path distance (as opposed to a straight line distance).","title":"Requirements"},{"location":"tutorials/training-a-pointnav-model/#config-file-setup","text":"Now comes the most important part of the tutorial, we are going to write an experiment config file. If this is your first experience with experiment config files in AllenAct, we suggest that you first see our how-to on defining an experiment which will walk you through creating a simplified experiment config file. Unlike a library that can be imported into python, AllenAct is structured as a framework with a runner script called main.py which will run the experiment specified in a config file. This design forces us to keep meticulous records of exactly which settings were used to produce a particular result, which can be very useful given how expensive RL models are to train. The projects/ directory is home to different projects using AllenAct . Currently it is populated with baselines of popular tasks and tutorials. We already have all the code for this tutorial stored in projects/tutorials/pointnav_robothor_rgb_ddppo.py . We will be using this file to run our experiments, but you can create a new directory in projects/ and start writing your experiment there. We start off by importing everything we will need: import glob from math import ceil from typing import Dict , Any , List , Optional import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from torchvision import models from core.algorithms.onpolicy_sync.losses import PPO from core.algorithms.onpolicy_sync.losses.ppo import PPOConfig from projects.pointnav_baselines.models.point_nav_models import ( ResnetTensorPointNavActorCritic , ) from plugins.ithor_plugin.ithor_sensors import RGBSensorThor from core.base_abstractions.experiment_config import ExperimentConfig from core.base_abstractions.preprocessor import ObservationSet from core.base_abstractions.task import TaskSampler from plugins.habitat_plugin.habitat_preprocessors import ResnetPreProcessorHabitat from plugins.robothor_plugin.robothor_sensors import GPSCompassSensorRoboThor from plugins.robothor_plugin.robothor_task_samplers import PointNavDatasetTaskSampler from plugins.robothor_plugin.robothor_tasks import PointNavTask from utils.experiment_utils import Builder , PipelineStage , TrainingPipeline , LinearDecay Next we define a new experiment config class: class PointNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ): We then define the task parameters. For PointNav, these include the maximum number of steps our agent can take before being reset (this prevents the agent from wandering on forever), and a configuration for the reward function that we will be using. # Task Parameters MAX_STEPS = 500 REWARD_CONFIG = { \"step_penalty\" : - 0.01 , \"goal_success_reward\" : 10.0 , \"failed_stop_reward\" : 0.0 , \"shaping_weight\" : 1.0 , } In this case, we set the maximum number of steps to 500. We give the agent a reward of -0.01 for each action that it takes (this is to encourage it to reach the goal in as few actions as possible), and a reward of 10.0 if the agent manages to successfully reach its destination. If the agent selects the stop action without reaching the target we do not punish it (although this is sometimes useful for preventing the agent from stopping prematurely). Finally, our agent gets rewarded if it moves closer to the target and gets punished if it moves further away. shaping_weight controls how strong this signal should be and is here set to 1.0. These parameters work well for training an agent on PointNav, but feel free to play around with them. Next, we set the parameters of the simulator itself. Here we select a resolution at which the engine will render every frame (640 by 480) and a resolution at which the image will be fed into the neural network (here it is set to a 224 by 224 box). # Simulator Parameters CAMERA_WIDTH = 640 CAMERA_HEIGHT = 480 SCREEN_SIZE = 224 Next, we set the hardware parameters for the training engine. NUM_PROCESSES sets the total number of parallel processes that will be used to train the model. In general, more processes result in faster training, but since each process is a unique instance of the environment in which we are training they can take up a lot of memory. Depending on the size of the model, the environment, and the hardware we are using, we may need to adjust this number, but for a setup with 8 GTX Titans, 60 processes work fine. 60 also happens to be the number of training scenes in RoboTHOR, which allows each process to load only a single scene into memory, saving time and space. TRAINING_GPUS takes the ids of the GPUS on which the model should be trained. Similarly VALIDATION_GPUS and TESTING_GPUS hold the ids of the GPUS on which the validation and testing will occur. During training, a validation process is constantly running and evaluating the current model, to show the progress on the validation set, so reserving a GPU for validation can be a good idea. If our hardware setup does not include a GPU, these fields can be set to empty lists, as the codebase will default to running everything on the CPU with only 1 process. # Training Engine Parameters ADVANCE_SCENE_ROLLOUT_PERIOD : Optional [ int ] = None NUM_PROCESSES = 60 TRAINING_GPUS = list ( range ( torch . cuda . device_count ())) VALIDATION_GPUS = [ torch . cuda . device_count () - 1 ] TESTING_GPUS = [ torch . cuda . device_count () - 1 ] Since we are using a dataset to train our model we need to define the path to where we have stored it. If we download the dataset instructed above we can define the path as follows # Dataset Parameters TRAIN_DATASET_DIR = \"datasets/robothor-pointnav/train\" VAL_DATASET_DIR = \"datasets/robothor-pointnav/val\" Next, we define the sensors. RGBSensorThor is the environment's implementation of an RGB sensor. It takes the raw image outputted by the simulator and resizes it, to the input dimensions for our neural network that we specified above. It also performs normalization if we want. GPSCompassSensorRoboThor is a sensor that tracks the point our agent needs to move to. It tells us the direction and distance to our goal at every time step. SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , uuid = \"rgb_lowres\" , ), GPSCompassSensorRoboThor (), ] For the sake of this example, we are also going to be using a preprocessor with our model. In AllenAct the preprocessor abstraction is designed with large models with frozen weights in mind. These models often hail from the ResNet family and transform the raw pixels that our agent observes in the environment, into a complex embedding, which then gets stored and used as input to our trainable model instead of the original image. Most other preprocessing work is done in the sensor classes (as we just saw with the RGB sensor scaling and normalizing our input), but for the sake of efficiency, all neural network preprocessing should use this abstraction. PREPROCESSORS = [ Builder ( ResnetPreProcessorHabitat , { \"input_height\" : SCREEN_SIZE , \"input_width\" : SCREEN_SIZE , \"output_width\" : 7 , \"output_height\" : 7 , \"output_dims\" : 512 , \"pool\" : False , \"torchvision_resnet_model\" : models . resnet18 , \"input_uuids\" : [ \"rgb_lowres\" ], \"output_uuid\" : \"rgb_resnet\" , \"parallel\" : False , # TODO False for debugging } ), ] Next, we must define all of the observation inputs that our model will use. These are just the hardcoded ids of the sensors we are using in the experiment. OBSERVATIONS = [ \"rgb_resnet\" , \"target_coordinates_ind\" , ] Finally, we must define the settings of our simulator. We set the camera dimensions to the values we defined earlier. We set rotateStepDegrees to 30 degrees, which means that every time the agent takes a turn action, they will rotate by 30 degrees. We set grid size to 0.25 which means that every time the agent moves forward, it will do so by 0.25 meters. ENV_ARGS = dict ( width = CAMERA_WIDTH , height = CAMERA_HEIGHT , rotateStepDegrees = 30.0 , gridSize = 0.25 , ) Now we move on to the methods that we must define to finish implementing an experiment config. Firstly we have a simple method that just returns the name of the experiment. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" Next, we define the training pipeline. In this function, we specify exactly which algorithm or algorithms we will use to train our model. In this simple example, we are using the PPO loss with a learning rate of 3e-4. We specify 250 million steps of training and a rollout length of 30 with the ppo_steps and num_steps parameters respectively. All the other standard PPO parameters are also present in this function. metric_accumulate_interval sets the frequency at which data is accumulated from all the processes and logged while save_interval sets how often we save the model weights and run validation on them. @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 250000000 ) lr = 3e-4 num_mini_batch = 1 update_repeats = 3 num_steps = 30 save_interval = 5000000 metric_accumulate_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 0.95 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : Builder ( PPO , kwargs = {}, default = PPOConfig , )}, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) We define the helper method split_num_processes to split the different scenes that we want to train with amongst the different available devices. \"machine_params\" returns the hardware parameters of each process, based on the list of devices we defined above. def split_num_processes ( self , ndevices ): assert self . NUM_PROCESSES >= ndevices , \"NUM_PROCESSES {} < ndevices\" . format ( self . NUM_PROCESSES , ndevices ) res = [ 0 ] * ndevices for it in range ( self . NUM_PROCESSES ): res [ it % ndevices ] += 1 return res def machine_params ( self , mode = \"train\" , ** kwargs ): if mode == \"train\" : workers_per_device = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TRAINING_GPUS * workers_per_device nprocesses = 1 if not torch . cuda . is_available () else self . split_num_processes ( len ( gpu_ids )) sampler_devices = self . TRAINING_GPUS elif mode == \"valid\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . VALIDATION_GPUS elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TESTING_GPUS else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) # Disable parallelization for validation process if mode == \"valid\" : for prep in self . PREPROCESSORS : prep . kwargs [ \"parallel\" ] = False observation_set = Builder ( ObservationSet , kwargs = dict ( source_ids = self . OBSERVATIONS , all_preprocessors = self . PREPROCESSORS , all_sensors = self . SENSORS )) if mode == 'train' or nprocesses > 0 else None return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids , \"sampler_devices\" : sampler_devices if mode == \"train\" else gpu_ids , \"observation_set\" : observation_set , } Now we define the actual model that we will be using. AllenAct offers first-class support for PyTorch, so any PyTorch model that implements the provided ActorCriticModel class will work here. Here we borrow a model from the pointnav_baselines project (which unsurprisingly contains several PointNav baselines). It is a small convolutional network that expects the output of a ResNet as its rgb input followed by a single-layered GRU. The model accepts as input the number of different actions our agent can perform in the environment through the action_space parameter, which we get from the task definition. We also define the shape of the inputs we are going to be passing to the model with observation_space We specify the names of our sensors with goal_sensor_uuid and rgb_resnet_preprocessor_uuid . Finally, we define the size of our RNN with hidden_layer and the size of the embedding of our goal sensor data (the direction and distance to the target) with goal_dims . # Define Model @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ResnetTensorPointNavActorCritic ( action_space = gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), observation_space = kwargs [ \"observation_set\" ] . observation_spaces , goal_sensor_uuid = \"target_coordinates_ind\" , rgb_resnet_preprocessor_uuid = \"rgb_resnet\" , hidden_size = 512 , goal_dims = 32 , ) We also need to define the task sampler that we will be using. This is a piece of code that generates instances of tasks for our agent to perform (essentially starting locations and targets for PointNav). Since we are getting our tasks from a dataset, the task sampler is a very simple code that just reads the specified file and sets the agent to the next starting locations whenever the agent exceeds the maximum number of steps or selects the stop action. # Define Task Sampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavDatasetTaskSampler ( ** kwargs ) You might notice that we did not specify the task sampler's arguments, but are rather passing them in. The reason for this is that each process will have its own task sampler, and we need to specify exactly which scenes each process should work with. If we have several GPUS and many scenes this process of distributing the work can be rather complicated so we define a few helper functions to do just this. # Utility Functions for distributing scenes between GPUs @staticmethod def _partition_inds ( n : int , num_parts : int ): return np . round ( np . linspace ( 0 , n , num_parts + 1 , endpoint = True )) . astype ( np . int32 ) def _get_sampler_args_for_scene_split ( self , scenes_dir : str , process_ind : int , total_processes : int , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: path = os . path . join ( scenes_dir , \"*.json.gz\" ) scenes = [ scene . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for scene in glob . glob ( path )] if total_processes > len ( scenes ): # oversample some scenes -> bias if total_processes % len ( scenes ) != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisible by the number of scenes\" ) scenes = scenes * int ( ceil ( total_processes / len ( scenes ))) scenes = scenes [: total_processes * ( len ( scenes ) // total_processes )] else : if len ( scenes ) % total_processes != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisor of the number of scenes\" ) inds = self . _partition_inds ( len ( scenes ), total_processes ) return { \"scenes\" : scenes [ inds [ process_ind ]: inds [ process_ind + 1 ]], \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), \"seed\" : seeds [ process_ind ] if seeds is not None else None , \"deterministic_cudnn\" : deterministic_cudnn , \"rewards_config\" : self . REWARD_CONFIG } The very last things we need to define are the sampler arguments themselves. We define them separately for a train, validation, and test sampler, but in this case, they are almost the same. The arguments need to include the location of the dataset and distance cache as well as the environment arguments for our simulator, both of which we defined above and are just referencing here. The only consequential differences between these task samplers are the path to the dataset we are using (train or validation) and whether we want to loop over the dataset or not (we want this for training since we want to train for several epochs, but we do not need this for validation and testing). Since the test scenes of RoboTHOR are private we are also testing on our validation set. def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( os . path . join ( self . TRAIN_DATASET_DIR , \"episodes\" ), process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . TRAIN_DATASET_DIR res [ \"loop_dataset\" ] = True res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) res [ \"allow_flipping\" ] = True return res def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) return res def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = \"10.0\" return res This is it! If we copy all of the code into a file we should be able to run our experiment!","title":"Config File Setup"},{"location":"tutorials/training-a-pointnav-model/#training-model-on-debug-dataset","text":"We can test if our installation worked properly by training our model on a small dataset of 4 episodes. This should take about 20 minutes on a computer with a NVIDIA GPU. First we need to change the dataset path to point to our small debug dataset. Modify these lines in your file # Dataset Parameters TRAIN_DATASET_DIR = \"datasets/robothor-pointnav/debug\" VAL_DATASET_DIR = \"datasets/robothor-pointnav/debug\" Note that we changed both the train and test dataset to debug, so the model will train and validate on the same 4 episodes. We can now train a model by running: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> If using the same configuration as we have set up, the following command should work: python main.py -o storage/robothor-pointnav-rgb-resnet-resnet -b projects/tutorials pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this:","title":"Training Model On Debug Dataset"},{"location":"tutorials/training-a-pointnav-model/#training-model-on-full-dataset","text":"We can also train the model on the full dataset by changing back our dataset path and running the same command as above. But be aware, training this takes nearly 2 days on a machine with 8 GPU.","title":"Training Model On Full Dataset"},{"location":"tutorials/training-a-pointnav-model/#testing-model","text":"To test the performance of a model please refer to this tutorial .","title":"Testing Model"},{"location":"tutorials/training-a-pointnav-model/#conclusion","text":"In this tutorial, we learned how to create a new PointNav experiment using AllenAct . There are many simple and obvious ways to modify the experiment from here - changing the model, the learning algorithm and the environment each requires very few lines of code changed in the above file, allowing us to explore our embodied ai research ideas across different frameworks with ease.","title":"Conclusion"},{"location":"tutorials/training-pipelines/","text":"Tutorial: IL to RL with a training pipeline #","title":"Tutorial: IL to RL with a training pipeline"},{"location":"tutorials/training-pipelines/#tutorial-il-to-rl-with-a-training-pipeline","text":"","title":"Tutorial: IL to RL with a training pipeline"},{"location":"tutorials/transfering-to-a-different-environment-framework/","text":"Tutorial: Swapping in a new environment # Introduction # This tutorial was designed as a continuation of the Robothor PointNav Tutorial and explains how to modify the experiment config created in that tutorial to work with the iTHOR and Habitat environments. Cross-platform support is one of the key design goals of allenact . This is achieved through a total decoupling of the environment code from the engine, model and algorithm code, so that swapping in a new environment is as plug and play as possible. Crucially we will be able to run a model on different environments without touching the model code at all, which will allow us to train neural networks in one environment and test them in another. RoboTHOR to iTHOR # Since both the RoboTHOR and the iTHOR environment stem from the same family and are developed by the same organization, switching between the two is incredibly easy. We only have to change the path parameter to point to an iTHOR dataset rather than the RoboTHOR one. # Dataset Parameters TRAIN_DATASET_DIR = \"datasets/ithor-pointnav/train\" VAL_DATASET_DIR = \"datasets/ithor-pointnav/val\" We also have to download the iTHOR-PointNav dataset, following these instructions . We might also want to modify the tag method to accurately reflect our config but this will not change the behavior at all and is merely a bookkeeping convenience. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" RoboTHOR to Habitat # To train experiments using the Habitat framework we need to install it following these instructions . Since the roboTHOR and Habitat simulators are sufficiently different and have different parameters to configure this transformation takes a bit more effort, but we only need to modify the environment config and TaskSampler (we have to change the former because the habitat simulator accepts a different format of configuration and the latter because the habitat dataset is formatted differently and thus needs to be parsed differently.) As part of our environment modification, we need to switch from using RoboTHOR sensors to using Habitat sensors. The implementation of sensors we provide offer an uniform interface across all the environments so we simply have to swap out our sensor classes: SENSORS = [ DepthSensorHabitat ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_normalization = True , ), TargetCoordinatesSensorHabitat ( coordinate_dims = 2 ), ] Next we need to define the simulator config: CONFIG = get_habitat_config ( \"configs/gibson.yaml\" ) CONFIG . defrost () CONFIG . NUM_PROCESSES = NUM_PROCESSES CONFIG . SIMULATOR_GPU_IDS = TRAIN_GPUS CONFIG . DATASET . SCENES_DIR = HABITAT_SCENE_DATASETS_DIR CONFIG . DATASET . POINTNAVV1 . CONTENT_SCENES = [ \"*\" ] CONFIG . DATASET . DATA_PATH = TRAIN_SCENES CONFIG . SIMULATOR . AGENT_0 . SENSORS = [ \"RGB_SENSOR\" ] CONFIG . SIMULATOR . RGB_SENSOR . WIDTH = CAMERA_WIDTH CONFIG . SIMULATOR . RGB_SENSOR . HEIGHT = CAMERA_HEIGHT CONFIG . SIMULATOR . TURN_ANGLE = 30 CONFIG . SIMULATOR . FORWARD_STEP_SIZE = 0.25 CONFIG . ENVIRONMENT . MAX_EPISODE_STEPS = MAX_STEPS CONFIG . TASK . TYPE = \"Nav-v0\" CONFIG . TASK . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SENSORS = [ \"POINTGOAL_WITH_GPS_COMPASS_SENSOR\" ] CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . GOAL_FORMAT = \"POLAR\" CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . DIMENSIONALITY = 2 CONFIG . TASK . GOAL_SENSOR_UUID = \"pointgoal_with_gps_compass\" CONFIG . TASK . MEASUREMENTS = [ \"DISTANCE_TO_GOAL\" , \"SUCCESS\" , \"SPL\" ] CONFIG . TASK . SPL . TYPE = \"SPL\" CONFIG . TASK . SPL . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SUCCESS . SUCCESS_DISTANCE = 0.2 CONFIG . MODE = \"train\" This CONFIG object holds very similar values to the ones ENV_ARGS held in the RoboTHOR example. We decided to leave this way of passing in configurations exposed to the user to offer maximum customization of the underlying environment. Finally we need to replace the task sampler and its argument generating functions: # Define Task Sampler from plugins.habitat_plugin.habitat_task_samplers import PointNavTaskSampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavTaskSampler ( ** kwargs ) def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TRAIN_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . CONFIG . clone () config . defrost () config . DATASET . DATA_PATH = self . VALID_SCENES config . MODE = \"validate\" config . freeze () return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TEST_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } As we can see this code looks very similar as well, we simply need to pass slightly different parameters. Conclusion # In this tutorial, we learned how to modify our experiment configurations to work with different environments. By providing a high level of modularity and out-of-the-box support for both Habitat and THOR , two of the most popular embodied frameworks out there AllenAct hopes to give researchers the ability to validate their results across many platforms and help guide them towards genuine progress. The source code for this tutorial can be found in /projects/framework_transfer_tutorial .","title":"Swapping environments"},{"location":"tutorials/transfering-to-a-different-environment-framework/#tutorial-swapping-in-a-new-environment","text":"","title":"Tutorial: Swapping in a new environment"},{"location":"tutorials/transfering-to-a-different-environment-framework/#introduction","text":"This tutorial was designed as a continuation of the Robothor PointNav Tutorial and explains how to modify the experiment config created in that tutorial to work with the iTHOR and Habitat environments. Cross-platform support is one of the key design goals of allenact . This is achieved through a total decoupling of the environment code from the engine, model and algorithm code, so that swapping in a new environment is as plug and play as possible. Crucially we will be able to run a model on different environments without touching the model code at all, which will allow us to train neural networks in one environment and test them in another.","title":"Introduction"},{"location":"tutorials/transfering-to-a-different-environment-framework/#robothor-to-ithor","text":"Since both the RoboTHOR and the iTHOR environment stem from the same family and are developed by the same organization, switching between the two is incredibly easy. We only have to change the path parameter to point to an iTHOR dataset rather than the RoboTHOR one. # Dataset Parameters TRAIN_DATASET_DIR = \"datasets/ithor-pointnav/train\" VAL_DATASET_DIR = \"datasets/ithor-pointnav/val\" We also have to download the iTHOR-PointNav dataset, following these instructions . We might also want to modify the tag method to accurately reflect our config but this will not change the behavior at all and is merely a bookkeeping convenience. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\"","title":"RoboTHOR to iTHOR"},{"location":"tutorials/transfering-to-a-different-environment-framework/#robothor-to-habitat","text":"To train experiments using the Habitat framework we need to install it following these instructions . Since the roboTHOR and Habitat simulators are sufficiently different and have different parameters to configure this transformation takes a bit more effort, but we only need to modify the environment config and TaskSampler (we have to change the former because the habitat simulator accepts a different format of configuration and the latter because the habitat dataset is formatted differently and thus needs to be parsed differently.) As part of our environment modification, we need to switch from using RoboTHOR sensors to using Habitat sensors. The implementation of sensors we provide offer an uniform interface across all the environments so we simply have to swap out our sensor classes: SENSORS = [ DepthSensorHabitat ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_normalization = True , ), TargetCoordinatesSensorHabitat ( coordinate_dims = 2 ), ] Next we need to define the simulator config: CONFIG = get_habitat_config ( \"configs/gibson.yaml\" ) CONFIG . defrost () CONFIG . NUM_PROCESSES = NUM_PROCESSES CONFIG . SIMULATOR_GPU_IDS = TRAIN_GPUS CONFIG . DATASET . SCENES_DIR = HABITAT_SCENE_DATASETS_DIR CONFIG . DATASET . POINTNAVV1 . CONTENT_SCENES = [ \"*\" ] CONFIG . DATASET . DATA_PATH = TRAIN_SCENES CONFIG . SIMULATOR . AGENT_0 . SENSORS = [ \"RGB_SENSOR\" ] CONFIG . SIMULATOR . RGB_SENSOR . WIDTH = CAMERA_WIDTH CONFIG . SIMULATOR . RGB_SENSOR . HEIGHT = CAMERA_HEIGHT CONFIG . SIMULATOR . TURN_ANGLE = 30 CONFIG . SIMULATOR . FORWARD_STEP_SIZE = 0.25 CONFIG . ENVIRONMENT . MAX_EPISODE_STEPS = MAX_STEPS CONFIG . TASK . TYPE = \"Nav-v0\" CONFIG . TASK . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SENSORS = [ \"POINTGOAL_WITH_GPS_COMPASS_SENSOR\" ] CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . GOAL_FORMAT = \"POLAR\" CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . DIMENSIONALITY = 2 CONFIG . TASK . GOAL_SENSOR_UUID = \"pointgoal_with_gps_compass\" CONFIG . TASK . MEASUREMENTS = [ \"DISTANCE_TO_GOAL\" , \"SUCCESS\" , \"SPL\" ] CONFIG . TASK . SPL . TYPE = \"SPL\" CONFIG . TASK . SPL . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SUCCESS . SUCCESS_DISTANCE = 0.2 CONFIG . MODE = \"train\" This CONFIG object holds very similar values to the ones ENV_ARGS held in the RoboTHOR example. We decided to leave this way of passing in configurations exposed to the user to offer maximum customization of the underlying environment. Finally we need to replace the task sampler and its argument generating functions: # Define Task Sampler from plugins.habitat_plugin.habitat_task_samplers import PointNavTaskSampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavTaskSampler ( ** kwargs ) def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TRAIN_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . CONFIG . clone () config . defrost () config . DATASET . DATA_PATH = self . VALID_SCENES config . MODE = \"validate\" config . freeze () return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TEST_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } As we can see this code looks very similar as well, we simply need to pass slightly different parameters.","title":"RoboTHOR to Habitat"},{"location":"tutorials/transfering-to-a-different-environment-framework/#conclusion","text":"In this tutorial, we learned how to modify our experiment configurations to work with different environments. By providing a high level of modularity and out-of-the-box support for both Habitat and THOR , two of the most popular embodied frameworks out there AllenAct hopes to give researchers the ability to validate their results across many platforms and help guide them towards genuine progress. The source code for this tutorial can be found in /projects/framework_transfer_tutorial .","title":"Conclusion"}]}